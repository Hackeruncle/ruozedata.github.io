<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hive自定义函数(UDF)的编程开发，你会吗？]]></title>
    <url>%2F2018%2F04%2F25%2FHive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本地开发环境：IntelliJ IDEA+Maven3.3.9 1. 创建工程 打开IntelliJ IDEA File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart 2. 配置 在工程中找到pom.xml文件，添加hadoop、hive依赖 3. 创建类、并编写一个HelloUDF.java，代码如下： 首先一个UDF必须满足下面两个条件: 一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类） 一个UDF必须至少实现了evaluate()方法 4. 测试，右击运行run ‘HelloUDF.main()’5. 打包在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包执行成功后在日志中找： [INFO] Building jar: (路径)/hive-1.0.jar]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DDL，你真的了解吗？]]></title>
    <url>%2F2018%2F04%2F24%2FHive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你全面剖析Hive DDL！ 概念DatabaseHive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置） TableHive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table Partition分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：/user/hadoop/hive/warehouse/[databasename.db]/table DDL(Data Definition Language)Create Database1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。COMMENT：数据库的描述LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下WITH DBPROPERTIES：数据库的属性 Drop Database12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; RESTRICT：默认是restrict，如果该数据库还有表存在则报错；CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。 Alter Database12345ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later) Use Database12USE database_name;USE DEFAULT; Show Databases123456SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;“ | ”：可以选择其中一种“[ ]”：可选项LIKE ‘identifier_with_wildcards’：模糊查询数据库 Describe Database12345678910111213DESCRIBE DATABASE [EXTENDED] db_name;DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；EXTENDED：加上数据库键值对的属性信息。hive&gt; describe database default;OKdefault Default Hive database hdfs://hadoop1:9000/user/hive/warehouse public ROLE Time taken: 0.065 seconds, Fetched: 1 row(s)hive&gt; hive&gt; describe database extended hive2;OKhive2 it is my database hdfs://hadoop1:9000/user/hive/warehouse/hive2.db hadoop USER &#123;date=2018-08-08, creator=zhangsan&#125;Time taken: 0.135 seconds, Fetched: 1 row(s) Create Table1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; data_type12345: primitive_type| array_type| map_type| struct_type| union_type -- (Note: Available in Hive 0.7.0 and later) primitive_type12345678910111213141516 : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN| FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type1: ARRAY &lt; data_type &gt; map_type1: MAP &lt; primitive_type, data_type &gt; struct_type1: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type1: UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later) row_format1234 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] file_format:1234567: SEQUENCEFILE| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)| RCFILE -- (Note: Available in Hive 0.6.0 and later)| ORC -- (Note: Available in Hive 0.11.0 and later)| PARQUET -- (Note: Available in Hive 0.13.0 and later)| AVRO -- (Note: Available in Hive 0.14.0 and later)| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname constraint_specification:12345 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE TEMPORARY（临时表）Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。语法：CREATE TEMPORARY TABLE … 注意： 如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表 临时表限制：不支持分区字段和创建索引 EXTERNAL（外部表）Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。1234hive&gt; create external table external_table( &gt; id int, &gt; name string &gt; ); PARTITIONED BY（分区表）产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。 可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下； 分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。 分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。 单分区：123456hive&gt; CREATE TABLE order_partition (&gt; order_number string, &gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string);OK 多分区：123456hive&gt; CREATE TABLE order_partition2 (&gt; order_number string,&gt; event_time string&gt; )&gt; PARTITIONED BY (event_month string,every_day string);OK 1234567[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partitiondrwxr-xr-x - hadoop supergroup 0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2[hadoop@hadoop000 ~]$ROW FORMAT 官网解释：1234567: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char][NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] DELIMITED：分隔符（可以自定义分隔符）； FIELDS TERMINATED BY char:每个字段之间使用的分割； 例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n; COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）； MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符； LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]） 一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。 创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：123456hive&gt; create table demo1(&gt; id int,&gt; name string&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;OK 创建demo2表，并指定其他字段：123456789101112hive&gt; create table demo2 (&gt; id int,&gt; name string,&gt; hobbies ARRAY &lt;string&gt;,&gt; address MAP &lt;string, string&gt;&gt; )&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;&gt; MAP KEYS TERMINATED BY &apos;:&apos;;OKSTORED AS（存储格式）Create Table As Select 创建表（拷贝表结构及数据，并且会运行MapReduce作业）12345678910CREATE TABLE emp (empno int,ename string,job string,mgr int,hiredate string,salary double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;; 加载数据1LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp; 复制整张表12345678910111213141516171819202122232425262728293031hive&gt; create table emp2 as select * from emp;Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/Kill Command = /opt/software/hadoop/bin/hadoop job -kill job_1514116522188_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-01-08 05:21:07,707 Stage-1 map = 0%, reduce = 0%2018-01-08 05:21:19,605 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.81 secMapReduce Total cumulative CPU time: 1 seconds 810 msecEnded Job = job_1514116522188_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 1.81 sec HDFS Read: 3927 HDFS Write: 730 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 810 msecOKTime taken: 33.322 secondshive&gt; show tables;OKempemp2order_partitionorder_partition2Time taken: 0.071 seconds, Fetched: 4 row(s)hive&gt; 复制表中的一些字段1create table emp3 as select empno,ename from emp; LIKE使用like创建表时，只会复制表的结构，不会复制表的数据1234567hive&gt; create table emp4 like emp;OKTime taken: 0.149 secondshive&gt; select * from emp4;OKTime taken: 0.151 secondshive&gt; 并没有查询到数据 desc formatted table_name查询表的详细信息12hive&gt; desc formatted emp;OK col_name data_type comment12345678empno int ename string job string mgr int hiredate string salary double comm double deptno int Detailed Table Information123456789101112131415Database: hive Owner: hadoop CreateTime: Mon Jan 08 05:17:54 CST 2018 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 0 rawDataSize 0 totalSize 668 transient_lastDdlTime 1515359982 Storage Information123456789101112SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t serialization.format \t Time taken: 0.228 seconds, Fetched: 39 row(s)hive&gt; 通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params; 查询数据库下的所有表1234567891011hive&gt; show tables;OKempemp1emp2emp3emp4order_partitionorder_partition2Time taken: 0.047 seconds, Fetched: 7 row(s)hive&gt; 查询创建表的语法123456789101112131415161718192021222324252627282930hive&gt; show create table emp;OKCREATE TABLE `emp`( `empno` int, `ename` string, `job` string, `mgr` int, `hiredate` string, `salary` double, `comm` double, `deptno` int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;668&apos;, &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)Time taken: 0.192 seconds, Fetched: 24 row(s)hive&gt; Drop TableDROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) 指定PURGE后，数据不会放到回收箱，会直接删除 DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失 删除EXTERNAL表时，表中的数据不会从文件系统中删除Alter Table 重命名1234567891011121314151617181920212223242526272829hive&gt; alter table demo2 rename to new_demo2;OKAdd PartitionsALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...)用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。hive&gt; create table dept(&gt; deptno int,&gt; dname string,&gt; loc string&gt; )&gt; PARTITIONED BY (dt string)&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;OKTime taken: 0.953 seconds hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);Loading data to table default.dept partition (dt=2018-08-08)Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]OKTime taken: 5.147 seconds 查询结果123456789101112131415hive&gt; select * from dept;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 0.481 seconds, Fetched: 4 row(s)hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);OKDrop PartitionsALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;); 查看分区语句12345hive&gt; show partitions dept;OKdt=2018-08-08dt=2018-09-09Time taken: 0.385 seconds, Fetched: 2 row(s) 按分区查询1234567hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;OK10 ACCOUNTING NEW YORK 2018-08-0820 RESEARCH DALLAS 2018-08-0830 SALES CHICAGO 2018-08-0840 OPERATIONS BOSTON 2018-08-08Time taken: 2.323 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive生产上，压缩和存储结合使用案例]]></title>
    <url>%2F2018%2F04%2F23%2FHive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[你们Hive生产上，压缩和存储，结合使用了吗？ 案例：原文件大小：19M 1. ORC+Zlip结合12345 create table page_views_orc_zlibROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)as select * from page_views; 用ORC+Zlip之后的文件为2.8M 用ORC+Zlip之后的文件为2.8M###### 2. Parquet+gzip结合 12345 set parquet.compression=gzip;create table page_views_parquet_gzipROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;用Parquet+gzip之后的文件为3.9M 3. Parquet+Lzo结合3.1 安装Lzo1234567891011 wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gztar -zxvf lzo-2.06.tar.gzcd lzo-2.06./configure -enable-shared -prefix=/usr/local/hadoop/lzo/make &amp;&amp; make installcp /usr/local/hadoop/lzo/lib/* /usr/lib/cp /usr/local/hadoop/lzo/lib/* /usr/lib64/vi /etc/profileexport PATH=/usr/local//hadoop/lzo/:$PATHexport C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/source /etc/profile 3.2 安装Lzop12345678 wget http://www.lzop.org/download/lzop-1.03.tar.gztar -zxvf lzop-1.03.tar.gzcd lzop-1.03./configure -enable-shared -prefix=/usr/local/hadoop/lzopmake &amp;&amp; make installvi /etc/profileexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64source /etc/profile 3.3 软连接1ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop 3.4 测试lzop lzop xxx.log 若生成xxx.log.lzo文件，则说明成功3.5 安装Hadoop-LZO12345 git或svn 下载https://github.com/twitter/hadoop-lzocd hadoop-lzomvn clean package -Dmaven.test.skip=true tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/ 3.6 配置 在core-site.xml配置1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;在mapred-site.xml中配置 &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt;在hadoop-env.sh中配置export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib 3.7 测试12345678SET hive.exec.compress.output=true; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUETTBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)as select * from page_views; 用Parquet+Lzo(未建立索引)之后的文件为5.9M]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive存储格式的生产应用]]></title>
    <url>%2F2018%2F04%2F20%2FHive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。 原始大小: 19M 1. TextFile(默认) 文件大小为18.1M 2. SequenceFile123456789101112 create table page_views_seq( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string )ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” STORED AS SEQUENCEFILE;insert into table page_views_seq select * from page_views; 用SequenceFile存储后的文件为19.6M 3. RcFile123456789101112 create table page_views_rcfile(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS RCFILE; insert into table page_views_rcfile select * from page_views; 用RcFile存储后的文件为17.9M 4. ORCFile12345 create table page_views_orcROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS ORC TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from page_views; 用ORCFile存储后的文件为7.7M 5. Parquetcreate table page_views_parquet ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot; STORED AS PARQUET as select * from page_views; 用ORCFile存储后的文件为13.1M 总结：磁盘空间占用大小比较 ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据压缩，你们真的了解吗？]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，带你们剖析大数据之压缩！ 1. 压缩的好处和坏处 好处 减少存储磁盘空间 降低IO(网络的IO和磁盘的IO) 加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度 坏处 由于使用数据时，需要先将数据解压，加重CPU负荷 2. 压缩格式压缩比压缩时间 可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2 压缩格式 优点 缺点 gzip 压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便 不支持split lzo 压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便 压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式） snappy 压缩速度快；支持hadoop native库 不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2 bzip2 支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便 压缩/解压速度慢；不支持native 总结：不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。 应用场景：一般在HDFS 、Hive、HBase中会使用；当然一般较多的是结合Spark 来一起使用。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop常用命令大全]]></title>
    <url>%2F2018%2F04%2F14%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop常用命令大全 1. 单独启动和关闭hadoop服务 功能 命令 启动名称节点 hadoop-daemon.sh start namenode 启动数据节点 hadoop-daemons.sh start datanode slave 启动secondarynamenode hadoop-daemon.sh start secondarynamenode 启动resourcemanager yarn-daemon.sh start resourcemanager 启动nodemanager bin/yarn-daemons.sh start nodemanager 停止数据节点 hadoop-daemons.sh stop datanode 2. 常用的命令 功能 命令 创建目录 hdfs dfs -mkdir /input 查看 hdfs dfs -ls 递归查看 hdfs dfs ls -R 上传 hdfs dfs -put 下载 hdfs dfs -get 删除 hdfs dfs -rm 从本地剪切粘贴到hdfs hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt 从hdfs剪切粘贴到本地 hdfs fs -moveToLocal /input/xx.txt /input/xx.txt 追加一个文件到另一个文件到末尾 hdfs fs -appedToFile ./hello.txt /input/hello.txt 查看文件内容 hdfs fs -cat /input/hello.txt 显示一个文件到末尾 hdfs fs -tail /input/hello.txt 以字符串的形式打印文件的内容 hdfs fs -text /input/hello.txt 修改文件权限 hdfs fs -chmod 666 /input/hello.txt 修改文件所属 hdfs fs -chown ruoze.ruoze /input/hello.txt 从本地文件系统拷贝到hdfs里 hdfs fs -copyFromLocal /input/hello.txt /input/ 从hdfs拷贝到本地 hdfs fs -copyToLocal /input/hello.txt /input/ 从hdfs到一个路径拷贝到另一个路径 hdfs fs -cp /input/xx.txt /output/xx.txt 从hdfs到一个路径移动到另一个路径 hdfs fs -mv /input/xx.txt /output/xx.txt 统计文件系统的可用空间信息 hdfs fs -df -h / 统计文件夹的大小信息 hdfs fs -du -s -h / 统计一个指定目录下的文件节点数量 hadoop fs -count /aaa 设置hdfs的文件副本数量 hadoop fs -setrep 3 /input/xx.txt 总结：一定要学会查看命令帮助1.hadoop命令直接回车查看命令帮助2.hdfs命令、hdfs dfs命令直接回车查看命令帮助3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。]]></content>
      <categories>
        <category>其余组件</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.2.0 全网最详细的源码编译]]></title>
    <url>%2F2018%2F04%2F14%2FSpark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[若泽大数据，Spark2.2.0 全网最详细的源码编译 环境准备 JDK： Spark 2.2.0及以上版本只支持JDK1.8 Maven：3.3.9设置maven环境变量时，需设置maven内存：export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m” Scala：2.11.8 Git 编译下载spark的tar包，并解压12[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz 编辑dev/make-distribution.sh123456789101112131415[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh注释以下内容：#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | tail -n 1)#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v &quot;INFO&quot;\# | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use &quot;set -o pipefail&quot;# echo -n) 添加以下内容：1234VERSION=2.2.0SCALA_VERSION=2.11SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1 编辑pom.xml1234567[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml添加在repositorys内&lt;repository&gt; &lt;id&gt;clouders&lt;/id&gt; &lt;name&gt;clouders Repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt; 安装1[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn 稍微等待几小时，网络较好的话，非常快。也可以参考J哥博客：基于CentOS6.4环境编译Spark-2.1.0源码 http://blog.itpub.net/30089851/viewspace-2140779/]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>基础</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我们生产上要选择Spark On Yarn模式？]]></title>
    <url>%2F2018%2F04%2F13%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[若泽大数据，为什么我们生产上要选择Spark On Yarn？开发上我们选择local[2]模式生产上跑任务Job，我们选择Spark On Yarn模式 ， 将Spark Application部署到yarn中，有如下优点： 1.部署Application和服务更加方便 只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。 2.资源隔离机制 yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。 3.资源弹性管理 Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。 Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。 运行client模式： “./spark-shell –master yarn” “./spark-shell –master yarn-client” “./spark-shell –master yarn –deploy-mode client” 运行的是cluster模式 “./spark-shell –master yarn-cluster” “./spark-shell –master yarn –deploy-mode cluster” client和cluster模式的主要区别：a. client的driver是运行在客户端进程中b. cluster的driver是运行在Application Master之中]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive全网最详细的编译及部署]]></title>
    <url>%2F2018%2F04%2F11%2FHive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hive全网最详细的编译及部署 一、需要安装的软件 相关环境： jdk-7u80 hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7 apache-maven-3.3.9 mysql5.1 hadoop伪分布集群已启动 二、安装jdk123456789mkdir /usr/java &amp;&amp; cd /usr/java/ tar -zxvf /tmp/server-jre-7u80-linux-x64.tar.gzchown -R root:root /usr/java/jdk1.7.0_80/ echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profilesource /etc/profile 三、安装maven12345678910111213cd /usr/local/unzip /tmp/apache-maven-3.3.9-bin.zipchown root: /usr/local/apache-maven-3.3.9 -Recho &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profileecho &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profilesource /etc/profile 四、安装mysql1234567891011121314151617181920212223242526272829yum -y install mysql-server mysql/etc/init.d/mysqld startchkconfig mysqld onmysqladmin -u root password 123456mysql -uroot -p123456use mysql;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;delete from user where not (user=&apos;root&apos;) ;delete from user where user=&apos;root&apos; and password=&apos;&apos;; drop database test;DROP USER &apos;&apos;@&apos;%&apos;;flush privileges; 五、下载hive源码包：输入：http://archive.cloudera.com/cdh5/cdh/5/根据cdh版本选择对应hive软件包：hive-1.1.0-cdh5.7.1-src.tar.gz解压后使用maven命令编译成安装包 六、编译:1234567891011cd /tmp/tar -xf hive-1.1.0-cdh5.7.1-src.tar.gzcd /tmp/hive-1.1.0-cdh5.7.1mvn clean package -DskipTests -Phadoop-2 -Pdist# 编译生成的包在以下位置：# packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz 七、安装编译生成的Hive包，然后测试12345678910111213cd /usr/local/tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gzln -s apache-hive-1.1.0-cdh5.7.1-bin hivechown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin chown -R hadoop:hadoop hive echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profileecho &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile 八、更改环境变量12345su - hadoopcd /usr/local/hivecd conf 1、hive-env.sh123cp hive-env.sh.template hive-env.sh&amp;&amp;vi hive-env.shHADOOP_HOME=/usr/local/hadoop 2、hive-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243vi hive-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;vincent&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 九、拷贝mysql驱动包到$HIVE_HOME/lib上方的hive-site.xml使用了java的mysql驱动包需要将这个包上传到hive的lib目录之下解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可1234567cd /tmpunzip mysql-connector-java-5.1.45.zipcd mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/ 未拷贝有相关报错： The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>基础</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)]]></title>
    <url>%2F2018%2F04%2F10%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce%2BYarn)%2F</url>
    <content type="text"><![CDATA[若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn) 修改mapred-site.xml 1234567891011121314151617[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop000 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 12345678910111213[hadoop@hadoop000 hadoop]# vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动 123[hadoop@hadoop000 hadoop]# cd ../../[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh 关闭 1[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS)]]></title>
    <url>%2F2018%2F04%2F08%2FHadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)%2F</url>
    <content type="text"><![CDATA[Hadoop全网最详细的伪分布式部署(HDFS) 1.添加hadoop用户123456[root@hadoop-01 ~]# useradd hadoop[root@hadoop-01 ~]# vi /etc/sudoers# 找到root ALL=(ALL) ALL，添加hadoop ALL=(ALL) NOPASSWD:ALL 2.上传并解压123[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz 3.软连接1[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop 4.设置环境变量1234567[root@hadoop-01 software]# vi /etc/profileexport HADOOP_HOME=/opt/software/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[root@hadoop-01 software]# source /etc/profile 5.设置用户、用户组123456789[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1 [root@hadoop-01 software]# cd hadoop[root@hadoop-01 hadoop]# rm -f *.txt 6.切换hadoop用户1234567891011121314151617181920212223242526272829[root@hadoop-01 software]# su - hadoop[root@hadoop-01 hadoop]# lltotal 32drwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 bindrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 etcdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 includedrwxrwxr-x. 3 hadoop hadoop 4096 Jun 2 14:24 libdrwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexecdrwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logsdrwxrwxr-x. 2 hadoop hadoop 4096 Jun 2 14:24 sbindrwxrwxr-x. 4 hadoop hadoop 4096 Jun 2 14:24 share # bin: 可执行文件# etc: 配置文件# sbin: shell脚本，启动关闭hdfs,yarn等 7.配置文件12345678910111213141516171819202122232425262728293031[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt; # 配置自己机器的IP &lt;/property&gt;&lt;/configuration&gt; [hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 8.配置hadoop用户的ssh信任关系8.1公钥/密钥 配置无密码登录12345[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys 8.2 查看日期，看是否配置成功1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop-01 ~]# ssh hadoop-01 dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017 [hadoop@hadoop-01 ~]# ssh hadoop-01 date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 2017 [hadoop@hadoop-01 ~]# ssh localhost dateThe authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.Are you sure you want to continue connecting (yes/no)? yes # 第一次回车输入yesWarning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.Sun Aug 20 14:22:28 CST 2017[hadoop@hadoop-01 ~]# ssh localhost date #不需要回车输入yes,即OKSun Aug 20 14:22:29 CST 2017 9.格式化和启动123456789[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: hadoop-01: Error: JAVA_HOME is not set and could not be found. localhost: Error: JAVA_HOME is not set and could not be found. 9.1解决方法:添加环境变量12345[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hadoop-env.sh# 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为export JAVA_HOME=/usr/java/jdk1.8.0_45 12345[hadoop@hadoop-01 hadoop]# sbin/start-dfs.shERROR: mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied 9.2解决方法:添加权限123456789[hadoop@hadoop-01 hadoop]# exit[root@hadoop-01 hadoop]# cd ../[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1[root@hadoop-01 software]# su - hadoop[root@hadoop-01 ~]# cd /opt/software/hadoop 9.3 继续启动1[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh 9.4检查是否成功123456789[hadoop@hadoop-01 hadoop]# jps19536 DataNode19440 NameNode19876 Jps19740 SecondaryNameNode 9.5访问： http://192.168.137.130:50070 9.6修改dfs启动的进程，以hadoop-01启动 启动的三个进程： namenode: hadoop-01 bin/hdfs getconf -namenodes datanode: localhost datanodes (using default slaves file) etc/hadoop/slaves secondarynamenode: 0.0.0.01234567891011121314151617181920212223242526272829[hadoop@hadoop-01 ~]# cd /opt/software/hadoop[hadoop@hadoop-01 hadoop]# echo &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves [hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves hadoop-01 [hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-01:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop-01:50091&lt;/value&gt;&lt;/property&gt; 9.7重启123[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>环境搭建</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（一）]]></title>
    <url>%2F2018%2F04%2F01%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（一） 查看当前目录 pwd 查看IP ifconfig 查看虚拟机ip hostname 主机名字 i 查看主机名映射的IP 切换目录 cd cd ~ 切换家目录（root为/root，普通用户为/home/用户名） cd /filename 以绝对路径切换目录 cd - 返回上一次操作路径，并输出路径 cd ../ 返回上一层目录 清理桌面 clear 显示当前目录文件和文件夹 ls ls -l(ll) 显示详细信息 ls -la 显示详细信息+隐藏文件（以 . 开头，例：.ssh） ls -lh 显示详细信息+文件大小 ls -lrt 显示详细信息+按时间排序 查看文件夹大小 du -sh 命令帮助 man 命令 命令 –help 创建文件夹 mkdir mkdir -p filename1/filename2 递归创建文件夹 创建文件 touch/vi/echo xx&gt;filename 查看文件内容 cat filename 直接打印所有内容 more filename 根据窗口大小进行分页显示 文件编辑 vi vi分为命令行模式，插入模式，尾行模式 命令行模式—&gt;插入模式：按i或a键 插入模式—&gt;命令行模式：按Esc键 命令行模式—&gt;尾行模式：按Shift和:键 插入模式 dd 删除光标所在行 n+dd 删除光标以下的n行 dG 删除光标以下行 gg 第一行第一个字母 G 最后一行第一个字母 shift+$ 该行最后一个字母 尾行模式 q! 强制退出 qw 写入并退出 qw! 强制写入退出 x 退出，如果存在改动，则保存再退出]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令（三）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（三） 用户、用户组 用户 useradd 用户名 添加用户 userdel 用户名 删除用户 id 用户名 查看用户信息 passwd 用户名 修改用户密码 su - 用户名 切换用户 ll /home/ 查看已有的用户 用户组 groupadd 用户组 添加用户组 cat /etc/group 用户组的文件 usermod -a -G 用户组 用户 将用户添加到用户组中 给一个普通用户添加sudo权限 123vi /etc/sudoers #在root ALL=(ALL) ALL 下面添加一行 用户 ALL=(ALL) NOPASSWD:ALL 修改文件权限 chown 修改文件或文件夹的所属用户和用户组 chown -R 用户:用户组 文件夹名 -R 为递归参数，指针对文件夹 chown 用户:用户组 文件名 chmod: 修改文件夹或者文件的权限 chmod -R 700 文件夹名 chmod 700 文件夹名 r =&gt; 4 w =&gt; 2 x =&gt; 1 后台执行命令 &amp; nohup screen 多人合作 screen screen -list 查看会话 screen -S 建立一个后台的会话 screen -r 进入会话 ctrl+a+d 退出会话]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（二）]]></title>
    <url>%2F2018%2F04%2F01%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux最常用实战命令（二） 实时查看文件内容 tail filename tail -f filename 当文件(名)被修改后，不能监视文件内容 tail -F filename 当文件(名)被修改后，依然可以监视文件内容 复制、移动文件 cp oldfilename newfilename 复制 mv oldfilename newfilename 移动/重命名 echo echo “xxx” 输出 echo “xxx” &gt; filename 覆盖 echo “xxx” &gt;&gt; filename 追加 删除 rm rm -f 强制删除 rm -rf 强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件 别名 alias alias x=”xxxxxx” 临时引用别名 alias x=”xxxxxx” 配置到环境变量中即为永久生效 查看历史命令 history history 显示出所有历史记录 history n 显示出n条记录 !n 执行第n条记录 管道命令 （ | ） 管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入 查看进程、查看id、端口 ps -ef ｜grep 进程名 查看进程基本信息 netstat -npl｜grep 进程名或进程id 查看服务id和端口 杀死进程 kill kill -9 进程名/pid 强制删除 kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程 rpm 搜索、卸载 rpm -qa | grep xxx 搜索xxx rpm –nodeps -e xxx 删除xxx –nodeps 不验证包的依赖性 查询 find 路径 -name xxx (推荐) which xxx local xxx 查看磁盘、内存、系统的情况 df -h 查看磁盘大小及其使用情况 free -m 查看内存大小及其使用情况 top 查看系统情况 软连接 ln -s 原始目录 目标目录 压缩、解压 tar -czf 压缩 tar -xzvf 解压 zip 压缩 unzip 解压]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构设计及副本放置策略]]></title>
    <url>%2F2018%2F03%2F30%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[HDFS架构设计及副本放置策略HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。 NameNode和DataNode架构图NameNode(名称节点)存储：元信息的种类，包含: 文件名称 文件目录结构 文件的属性[权限,创建时间,副本数] 文件对应哪些数据块–&gt;数据块对应哪些datanode节点 作用： 管理着文件系统命名空间 维护这文件系统树及树中的所有文件和目录 维护所有这些文件或目录的打开、关闭、移动、重命名等操作 DataNode(数据节点) 存储：数据块、数据块校验、与NameNode通信 作用： 读写文件的数据块 NameNode的指示来进行创建、删除、和复制等操作 通过心跳定期向NameNode发送所存储文件块列表信息 Scondary NameNode(第二名称节点) 存储: 命名空间镜像文件fsimage+编辑日志editlog 作用: 定期合并fsimage+editlog文件为新的fsimage推送给NamenNode副本放置策略第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上第二副本：放置在与第一个副本不同的机架的节点上第三副本：与第二个副本相同机架的不同节点上如果还有更多的副本：随机放在节点中]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置多台虚拟机之间的SSH信任]]></title>
    <url>%2F2018%2F03%2F28%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB%2F</url>
    <content type="text"><![CDATA[本机环境 3台机器执行命令ssh-keygen 选取第一台,生成authorized_keys文件 hadoop002 hadoop003传输id_rsa.pub文件到hadoop001 hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys 设置每台机器的权限12chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keys 将authorized_keys分发到hadoop002、hadoop003机器 验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)123[root@hadoop001 ~]# ssh root@hadoop002 date[root@hadoop002 ~]# ssh root@hadoop001 date[root@hadoop003 ~]# ssh root@hadoop001 date]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>基础</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
