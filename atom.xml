<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-16T12:35:29.773Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive自定义函数(UDF)的部署使用，你会吗？</title>
    <link href="http://yoursite.com/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/27/Hive自定义函数(UDF)的部署使用，你会吗？/</id>
    <published>2018-04-26T16:00:00.000Z</published>
    <updated>2019-04-16T12:35:29.773Z</updated>
    
    <content type="html"><![CDATA[<p>Hive自定义函数(UDF)的部署使用，你会吗，三种方式！<br><a id="more"></a> </p><font color="#FF4500"><br></font><h6 id="一-临时函数"><a href="#一-临时函数" class="headerlink" title="一.临时函数"></a>一.临时函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>添加jar包<br>hive&gt;add xxx.jar jar_filepath;</li><li>查看jar包<br>hive&gt;list jars;</li><li>创建临时函数<br>hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;</li></ol><h6 id="二-持久函数"><a href="#二-持久函数" class="headerlink" title="二.持久函数"></a>二.持久函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>上传到HDFS<br>$ hdfs dfs -put xxx.jar  hdfs:///path/to/xxx.jar</li><li>创建持久函数<br>hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;</li></ol><p><strong>注意点：</strong></p><ul><li><ol><li>此方法在show functions时是看不到的，但是可以使用</li></ol></li><li><ol start="2"><li>需要上传至hdfs</li></ol></li></ul><h6 id="三-持久函数，并注册"><a href="#三-持久函数，并注册" class="headerlink" title="三.持久函数，并注册"></a>三.持久函数，并注册</h6><p>环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>下载源码<br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a> </p></li><li><p>解压源码<br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>将HelloUDF.java文件增加到HIVE源码中<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>修改FunctionRegistry.java 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>重新编译<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>编译结果全部为：BUILD SUCCESS<br>文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>配置hive环境<br>配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：<br>7.1. 全部配置：参照之前文档  <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hive全网最详细的编译及部署</a></p><p>7.2. 将编译后带UDF函数的包复制到旧hive环境<br>   到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉<br>   命令：</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>最终启动hive</p></li><li><p>测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- 能查看到有 helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudf函数生效</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive自定义函数(UDF)的部署使用，你会吗，三种方式！&lt;br&gt;
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数(UDF)的编程开发，你会吗？</title>
    <link href="http://yoursite.com/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/25/Hive自定义函数(UDF)的编程开发，你会吗？/</id>
    <published>2018-04-24T16:00:00.000Z</published>
    <updated>2019-04-16T12:28:35.658Z</updated>
    
    <content type="html"><![CDATA[<p>本地开发环境：IntelliJ IDEA+Maven3.3.9<br><a id="more"></a> </p><h6 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h6><p>   打开IntelliJ IDEA<br>   File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h6><p> 在工程中找到pom.xml文件，添加hadoop、hive依赖<br> <img src="/assets/blogImg/425hive1.png" alt="Hive图1"></p><h6 id="3-创建类、并编写一个HelloUDF-java，代码如下："><a href="#3-创建类、并编写一个HelloUDF-java，代码如下：" class="headerlink" title="3. 创建类、并编写一个HelloUDF.java，代码如下："></a>3. 创建类、并编写一个HelloUDF.java，代码如下：</h6><p>  <img src="/assets/blogImg/425hive2.png" alt="Hive图2"></p><p><strong>首先一个UDF必须满足下面两个条件:</strong></p><ul><li><ol><li>一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）</li></ol></li><li><ol start="2"><li>一个UDF必须至少实现了evaluate()方法</li></ol></li></ul><h6 id="4-测试，右击运行run-‘HelloUDF-main-’"><a href="#4-测试，右击运行run-‘HelloUDF-main-’" class="headerlink" title="4. 测试，右击运行run ‘HelloUDF.main()’"></a>4. 测试，右击运行run ‘HelloUDF.main()’</h6><h6 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h6><p>在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包<br>执行成功后在日志中找：<br>     <font color="#FF4500">     [INFO] Building jar: (路径)/hive-1.0.jar  </font></p><p></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本地开发环境：IntelliJ IDEA+Maven3.3.9&lt;br&gt;
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive DDL，你真的了解吗？</title>
    <link href="http://yoursite.com/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/24/Hive DDL，你真的了解吗？/</id>
    <published>2018-04-23T16:00:00.000Z</published>
    <updated>2019-04-16T12:29:01.995Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，带你全面剖析Hive DDL！</p><font color="#FF4500"><br></font><p><img src="/assets/blogImg/hive424.png" alt="Hive架构图"><br><a id="more"></a> </p><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p><strong>Database</strong><br>Hive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）</p><p><strong>Table</strong><br>Hive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table</p><p><strong>Partition</strong><br>分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：<br>/user/hadoop/hive/warehouse/[databasename.db]/table</p><h5 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h5><p><strong>Create Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure></p><p>IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。<br>COMMENT：数据库的描述<br>LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下<br>WITH DBPROPERTIES：数据库的属性</p><p><strong>Drop Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name </span><br><span class="line">[RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure></p><p>RESTRICT：默认是restrict，如果该数据库还有表存在则报错；<br>CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。</p><p><strong>Alter Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>Use Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br></pre></td></tr></table></figure></p><p><strong>Show Databases</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;</span><br><span class="line">“ | ”：可以选择其中一种</span><br><span class="line"></span><br><span class="line">“[ ]”：可选项</span><br><span class="line"></span><br><span class="line">LIKE ‘identifier_with_wildcards’：模糊查询数据库</span><br></pre></td></tr></table></figure></p><p><strong>Describe Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE DATABASE [EXTENDED] db_name;</span><br><span class="line">DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；</span><br><span class="line">EXTENDED：加上数据库键值对的属性信息。</span><br><span class="line">hive&gt; describe database default;</span><br><span class="line">OK</span><br><span class="line">default    Default Hive database    hdfs://hadoop1:9000/user/hive/warehouse    public    ROLE    </span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line"></span><br><span class="line">hive&gt; describe database extended hive2;</span><br><span class="line">OK</span><br><span class="line">hive2   it is my database       hdfs://hadoop1:9000/user/hive/warehouse/hive2.db        hadoop      USER    &#123;date=2018-08-08, creator=zhangsan&#125;</span><br><span class="line">Time taken: 0.135 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></p><p><strong>Create Table</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line"> ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line"> [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line"> | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"></span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br></pre></td></tr></table></figure></p><p><strong>data_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">: primitive_type</span><br><span class="line">| array_type</span><br><span class="line">| map_type</span><br><span class="line">| struct_type</span><br><span class="line">| union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>primitive_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> : TINYINT</span><br><span class="line"> | SMALLINT</span><br><span class="line"> | INT</span><br><span class="line"> | BIGINT</span><br><span class="line"> | BOOLEAN</span><br><span class="line">| FLOAT</span><br><span class="line"> | DOUBLE</span><br><span class="line"> | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line"> | STRING</span><br><span class="line"> | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line"> | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>array_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: ARRAY &lt; data_type &gt;</span><br></pre></td></tr></table></figure></p><p><strong>map_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: MAP &lt; primitive_type, data_type &gt;</span><br></pre></td></tr></table></figure></p><p><strong>struct_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br></pre></td></tr></table></figure></p><p><strong>union_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note:     Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>row_format</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure></p><p><strong>file_format:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure></p><p><strong>constraint_specification:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">TEMPORARY（临时表）</span><br><span class="line">Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。</span><br><span class="line">语法：CREATE TEMPORARY TABLE …</span><br></pre></td></tr></table></figure></p><h6 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h6><ol><li>如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表</li><li>临时表限制：不支持分区字段和创建索引</li></ol><p>EXTERNAL（外部表）<br>Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table external_table(</span><br><span class="line">  &gt; id int,</span><br><span class="line">&gt;  name string </span><br><span class="line">&gt; );</span><br></pre></td></tr></table></figure></p><p>PARTITIONED BY（分区表）<br>产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。</p><p>可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；</p><p>分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。</p><p>分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。</p><p>单分区：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE order_partition (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">    &gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (event_month string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>多分区：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE TABLE order_partition2 (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">&gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt;  PARTITIONED BY (event_month string,every_day string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db</span><br><span class="line">18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2</span><br><span class="line">[hadoop@hadoop000 ~]$</span><br><span class="line">ROW FORMAT</span><br></pre></td></tr></table></figure><p>官网解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char [ESCAPED BY char]]       [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   </span><br><span class="line">-- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure></p><p>DELIMITED：分隔符（可以自定义分隔符）；</p><p>FIELDS TERMINATED BY char:每个字段之间使用的分割；</p><p>例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;</p><p>COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；</p><p>MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；</p><p>LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）</p><p>一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。</p><p>创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo1(</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>创建demo2表，并指定其他字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo2 (</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string,</span><br><span class="line">&gt; hobbies ARRAY &lt;string&gt;,</span><br><span class="line">&gt; address MAP &lt;string, string&gt;</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;</span><br><span class="line">&gt; MAP KEYS TERMINATED BY &apos;:&apos;;</span><br><span class="line">OK</span><br><span class="line">STORED AS（存储格式）</span><br><span class="line">Create Table As Select</span><br></pre></td></tr></table></figure></p><p>创建表（拷贝表结构及数据，并且会运行MapReduce作业）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp (</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">salary double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure></p><p>加载数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;</span><br></pre></td></tr></table></figure></p><p>复制整张表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp2 as select * from emp;</span><br><span class="line">Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/</span><br><span class="line">Kill Command = /opt/software/hadoop/bin/hadoop job  -kill job_1514116522188_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2018-01-08 05:21:07,707 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2018-01-08 05:21:19,605 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 810 msec</span><br><span class="line">Ended Job = job_1514116522188_0003</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2</span><br><span class="line">Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 3927 HDFS Write: 730 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 810 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 33.322 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp2</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.071 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>复制表中的一些字段<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table emp3 as select empno,ename from emp;</span><br></pre></td></tr></table></figure></p><p>LIKE<br>使用like创建表时，只会复制表的结构，不会复制表的数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp4 like emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.149 seconds</span><br><span class="line">hive&gt; select * from emp4;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.151 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>并没有查询到数据</p><p>desc formatted table_name<br>查询表的详细信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted emp;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>col_name                data_type               comment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">empno                   int                                         </span><br><span class="line">ename                   string                                      </span><br><span class="line">job                     string                                      </span><br><span class="line">mgr                     int                                         </span><br><span class="line">hiredate                string                                      </span><br><span class="line">salary                  double                                      </span><br><span class="line">comm                    double                                      </span><br><span class="line">deptno                  int</span><br></pre></td></tr></table></figure></p><p>Detailed Table Information<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Database:               hive                     </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Mon Jan 08 05:17:54 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Protect Mode:           None                     </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">COLUMN_STATS_ACCURATE    true                </span><br><span class="line">numFiles                1                   </span><br><span class="line">numRows                 0                   </span><br><span class="line">rawDataSize             0                   </span><br><span class="line">totalSize               668                 </span><br><span class="line">transient_lastDdlTime    1515359982</span><br></pre></td></tr></table></figure></p><p>Storage Information<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">field.delim             \t                  </span><br><span class="line">serialization.format    \t                  </span><br><span class="line">Time taken: 0.228 seconds, Fetched: 39 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;</p><p>查询数据库下的所有表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp1</span><br><span class="line">emp2</span><br><span class="line">emp3</span><br><span class="line">emp4</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.047 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>查询创建表的语法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show create table emp;</span><br><span class="line">OK</span><br><span class="line">CREATE TABLE `emp`(</span><br><span class="line">  `empno` int, </span><br><span class="line">  `ename` string, </span><br><span class="line">  `job` string, </span><br><span class="line">  `mgr` int, </span><br><span class="line">  `hiredate` string, </span><br><span class="line">  `salary` double, </span><br><span class="line">  `comm` double, </span><br><span class="line">  `deptno` int)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;\t&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;numRows&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;rawDataSize&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;668&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)</span><br><span class="line">Time taken: 0.192 seconds, Fetched: 24 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">Drop Table</span><br><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];     -- (Note: PURGE available in Hive 0.14.0 and later)</span><br></pre></td></tr></table></figure></p><p>指定PURGE后，数据不会放到回收箱，会直接删除</p><p>DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失</p><p>删除EXTERNAL表时，表中的数据不会从文件系统中删除<br>Alter Table</p><p>重命名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table demo2 rename to new_demo2;</span><br><span class="line">OK</span><br><span class="line">Add Partitions</span><br><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line">  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)</span><br><span class="line">用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。</span><br><span class="line">原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。</span><br><span class="line"></span><br><span class="line">hive&gt;  create table dept(</span><br><span class="line">&gt;  deptno int,</span><br><span class="line">&gt; dname string,</span><br><span class="line">&gt; loc string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (dt string)</span><br><span class="line">&gt;  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.953 seconds </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);</span><br><span class="line">Loading data to table default.dept partition (dt=2018-08-08)</span><br><span class="line">Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.147 seconds</span><br></pre></td></tr></table></figure></p><p>查询结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 0.481 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);</span><br><span class="line">OK</span><br><span class="line">Drop Partitions</span><br><span class="line">ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);</span><br></pre></td></tr></table></figure></p><p>查看分区语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept;</span><br><span class="line">OK</span><br><span class="line">dt=2018-08-08</span><br><span class="line">dt=2018-09-09</span><br><span class="line">Time taken: 0.385 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure></p><p>按分区查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 2.323 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，带你全面剖析Hive DDL！&lt;/p&gt;
&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;/assets/blogImg/hive424.png&quot; alt=&quot;Hive架构图&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive生产上，压缩和存储结合使用案例</title>
    <link href="http://yoursite.com/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2018/04/23/Hive生产上，压缩和存储结合使用案例/</id>
    <published>2018-04-22T16:00:00.000Z</published>
    <updated>2019-04-15T08:01:22.572Z</updated>
    
    <content type="html"><![CDATA[<p>你们Hive生产上，压缩和存储，结合使用了吗？</p><p>案例：<br>原文件大小：19M<br><img src="/assets/blogImg/423_1.png" alt="enter description here"><br><a id="more"></a> </p><h6 id="1-ORC-Zlip结合"><a href="#1-ORC-Zlip结合" class="headerlink" title="1. ORC+Zlip结合"></a>1. ORC+Zlip结合</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc_zlib</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><font color="#FF4500"> 用ORC+Zlip之后的文件为2.8M<br><br></font><br> 用ORC+Zlip之后的文件为2.8M<br><img src="/assets/blogImg/423_2.png" alt="enter description here"><br><br><br>######  2. Parquet+gzip结合<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       set parquet.compression=gzip;</span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><br><br><font color="#FF4500"><br>用Parquet+gzip之后的文件为3.9M<br></font><p><img src="/assets/blogImg/423_3.png" alt="enter description here"></p><h6 id="3-Parquet-Lzo结合"><a href="#3-Parquet-Lzo结合" class="headerlink" title="3. Parquet+Lzo结合"></a>3. Parquet+Lzo结合</h6><p><strong>3.1 安装Lzo</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gz</span><br><span class="line">tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">cd lzo-2.06</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzo/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib/</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib64/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export PATH=/usr/local//hadoop/lzo/:$PATH</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>3.2 安装Lzop</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.lzop.org/download/lzop-1.03.tar.gz</span><br><span class="line">tar -zxvf lzop-1.03.tar.gz</span><br><span class="line">cd lzop-1.03</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzop</span><br><span class="line">make  &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>3.3 软连接</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop</span><br></pre></td></tr></table></figure></p><p><strong>3.4 测试lzop</strong><br>        lzop xxx.log<br>    若生成xxx.log.lzo文件，则说明成功<br><strong>3.5 安装Hadoop-LZO</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   git或svn 下载https://github.com/twitter/hadoop-lzo</span><br><span class="line">cd hadoop-lzo</span><br><span class="line">mvn clean package -Dmaven.test.skip=true </span><br><span class="line">tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/</span><br></pre></td></tr></table></figure></p><p><strong>3.6 配置</strong><br>    在core-site.xml配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">     org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">           &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在mapred-site.xml中配置</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">           &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在hadoop-env.sh中配置</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure></p><p><strong>3.7 测试</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.compress.output=true;  </span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line">SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line">create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">TBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure></p><p><font color="#FF4500">用Parquet+Lzo(未建立索引)之后的文件为5.9M<br></font><br><img src="/assets/blogImg/423_4.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;你们Hive生产上，压缩和存储，结合使用了吗？&lt;/p&gt;
&lt;p&gt;案例：&lt;br&gt;原文件大小：19M&lt;br&gt;&lt;img src=&quot;/assets/blogImg/423_1.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
      <category term="案例" scheme="http://yoursite.com/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hive存储格式的生产应用</title>
    <link href="http://yoursite.com/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2018/04/20/Hive存储格式的生产应用/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2019-04-16T12:28:30.178Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。</strong></p><p>原始大小: 19M</p><p><img src="/assets/blogImg/420_1.png" alt="enter description here"><br><a id="more"></a> </p><h5 id="1-TextFile-默认-文件大小为18-1M"><a href="#1-TextFile-默认-文件大小为18-1M" class="headerlink" title="1. TextFile(默认) 文件大小为18.1M"></a>1. TextFile(默认) 文件大小为18.1M</h5><p><img src="/assets/blogImg/420_2.png" alt="enter description here"></p><h5 id="2-SequenceFile"><a href="#2-SequenceFile" class="headerlink" title="2. SequenceFile"></a>2. SequenceFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_seq( </span><br><span class="line">track_time string, </span><br><span class="line">url string, </span><br><span class="line">session_id string, </span><br><span class="line">referer string, </span><br><span class="line">ip string, </span><br><span class="line">end_user_id string, </span><br><span class="line">city_id string </span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” </span><br><span class="line">STORED AS SEQUENCEFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_seq select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用SequenceFile存储后的文件为19.6M</strong><br><img src="/assets/blogImg/420_3.png" alt="enter description here"></p><h5 id="3-RcFile"><a href="#3-RcFile" class="headerlink" title="3. RcFile"></a>3. RcFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_rcfile(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS RCFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_rcfile select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用RcFile存储后的文件为17.9M</strong><br><img src="/assets/blogImg/420_4.png" alt="enter description here"></p><h5 id="4-ORCFile"><a href="#4-ORCFile" class="headerlink" title="4. ORCFile"></a>4. ORCFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用ORCFile存储后的文件为7.7M</strong><br><img src="/assets/blogImg/420_5.png" alt="enter description here"></p><h5 id="5-Parquet"><a href="#5-Parquet" class="headerlink" title="5. Parquet"></a>5. Parquet</h5><pre><code>create table page_views_parquetROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;</code></pre><p><strong>用ORCFile存储后的文件为13.1M</strong><br><img src="/assets/blogImg/420_6.png" alt="enter description here"></p><p><strong>总结：磁盘空间占用大小比较</strong></p><font color="#FF4500">ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)</font>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始大小: 19M&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blogImg/420_1.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>大数据压缩，你们真的了解吗？</title>
    <link href="http://yoursite.com/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/18/大数据压缩，你们真的了解吗？/</id>
    <published>2018-04-17T16:00:00.000Z</published>
    <updated>2019-04-15T08:00:44.350Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，带你们剖析大数据之压缩！<br><a id="more"></a> </p><h6 id="1-压缩的好处和坏处"><a href="#1-压缩的好处和坏处" class="headerlink" title="1. 压缩的好处和坏处"></a>1. 压缩的好处和坏处</h6><p> <strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h6 id="2-压缩格式"><a href="#2-压缩格式" class="headerlink" title="2. 压缩格式"></a>2. 压缩格式</h6><p><img src="/assets/blogImg/压缩1.png" alt="enter description here"><br>压缩比<br><img src="/assets/blogImg/压缩2.png" alt="enter description here"><br>压缩时间<br><img src="/assets/blogImg/yasuo3.png" alt="enter description here"></p><font color="#FF0000">可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</font><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>gzip</strong></td><td>压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td>不支持split </td></tr><tr><td><strong>lzo</strong></td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td><strong>snappy</strong></td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td><strong>bzip2</strong></td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h5><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h5 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a><strong>应用场景：</strong></h5><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，带你们剖析大数据之压缩！&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop常用命令大全</title>
    <link href="http://yoursite.com/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
    <id>http://yoursite.com/2018/04/14/Hadoop常用命令大全/</id>
    <published>2018-04-13T16:00:00.000Z</published>
    <updated>2019-04-15T07:53:37.751Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Hadoop常用命令大全<br><a id="more"></a> </p><h6 id="1-单独启动和关闭hadoop服务"><a href="#1-单独启动和关闭hadoop服务" class="headerlink" title="1. 单独启动和关闭hadoop服务"></a><strong>1. 单独启动和关闭hadoop服务</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>启动名称节点</strong></td><td style="text-align:center">hadoop-daemon.sh start namenode</td></tr><tr><td><strong>启动数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh start datanode slave</td></tr><tr><td><strong>启动secondarynamenode</strong></td><td style="text-align:center">hadoop-daemon.sh start secondarynamenode</td></tr><tr><td><strong>启动resourcemanager</strong></td><td style="text-align:center">yarn-daemon.sh start resourcemanager</td></tr><tr><td><strong>启动nodemanager</strong></td><td style="text-align:center">bin/yarn-daemons.sh start nodemanager</td></tr><tr><td><strong>停止数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh stop datanode</td></tr></tbody></table><h6 id="2-常用的命令"><a href="#2-常用的命令" class="headerlink" title="2. 常用的命令"></a><strong>2. 常用的命令</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>创建目录</strong></td><td style="text-align:center">hdfs dfs -mkdir /input</td></tr><tr><td><strong>查看</strong></td><td style="text-align:center">hdfs dfs  -ls</td></tr><tr><td><strong>递归查看</strong></td><td style="text-align:center">hdfs dfs ls -R</td></tr><tr><td><strong>上传</strong></td><td style="text-align:center">hdfs dfs -put </td></tr><tr><td><strong>下载</strong></td><td style="text-align:center">hdfs dfs -get </td></tr><tr><td><strong>删除</strong></td><td style="text-align:center">hdfs dfs -rm</td></tr><tr><td><strong>从本地剪切粘贴到hdfs</strong></td><td style="text-align:center">hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>从hdfs剪切粘贴到本地</strong></td><td style="text-align:center">hdfs fs -moveToLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>追加一个文件到另一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -appedToFile ./hello.txt /input/hello.txt</td></tr><tr><td><strong>查看文件内容</strong></td><td style="text-align:center">hdfs fs -cat /input/hello.txt</td></tr><tr><td><strong>显示一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -tail /input/hello.txt</td></tr><tr><td><strong>以字符串的形式打印文件的内容</strong></td><td style="text-align:center">hdfs fs -text /input/hello.txt</td></tr><tr><td><strong>修改文件权限</strong></td><td style="text-align:center">hdfs fs -chmod 666 /input/hello.txt</td></tr><tr><td><strong>修改文件所属</strong></td><td style="text-align:center">hdfs fs -chown ruoze.ruoze  /input/hello.txt</td></tr><tr><td><strong>从本地文件系统拷贝到hdfs里</strong></td><td style="text-align:center">hdfs fs -copyFromLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs拷贝到本地</strong></td><td style="text-align:center">hdfs fs -copyToLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs到一个路径拷贝到另一个路径</strong></td><td style="text-align:center">hdfs fs -cp /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>从hdfs到一个路径移动到另一个路径</strong></td><td style="text-align:center">hdfs fs -mv /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>统计文件系统的可用空间信息</strong></td><td style="text-align:center">hdfs fs -df -h /</td></tr><tr><td><strong>统计文件夹的大小信息</strong></td><td style="text-align:center">hdfs fs -du -s -h /</td></tr><tr><td><strong>统计一个指定目录下的文件节点数量</strong></td><td style="text-align:center">hadoop  fs -count /aaa</td></tr><tr><td><strong>设置hdfs的文件副本数量</strong></td><td style="text-align:center">hadoop fs -setrep 3 /input/xx.txt</td></tr></tbody></table><h5 id="总结：一定要学会查看命令帮助"><a href="#总结：一定要学会查看命令帮助" class="headerlink" title="总结：一定要学会查看命令帮助"></a>总结：一定要学会查看命令帮助</h5><p><strong>1.hadoop命令直接回车查看命令帮助<br>2.hdfs命令、hdfs dfs命令直接回车查看命令帮助<br>3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Hadoop常用命令大全&lt;br&gt;
    
    </summary>
    
      <category term="其余组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BD%99%E7%BB%84%E4%BB%B6/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.2.0 全网最详细的源码编译</title>
    <link href="http://yoursite.com/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2018/04/14/Spark2.2.0 全网最详细的源码编译/</id>
    <published>2018-04-13T16:00:00.000Z</published>
    <updated>2019-04-15T07:50:33.639Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Spark2.2.0 全网最详细的源码编译<br><a id="more"></a> </p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><hr><p>JDK： Spark 2.2.0及以上版本只支持JDK1.8 </p><hr><p>Maven：3.3.9<br>设置maven环境变量时，需设置maven内存：<br>export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”</p><hr><p>Scala：2.11.8</p><hr><p>Git</p><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>下载spark的tar包，并解压<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</span><br><span class="line">[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz</span><br></pre></td></tr></table></figure></p><p>编辑dev/make-distribution.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh</span><br><span class="line">注释以下内容：</span><br><span class="line">#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use &quot;set -o pipefail&quot;</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure></p><p>添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure></p><p>编辑pom.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml</span><br><span class="line">添加在repositorys内</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">      &lt;id&gt;clouders&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;clouders Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br></pre></td></tr></table></figure></p><p>安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn</span><br></pre></td></tr></table></figure></p><p>稍微等待几小时，网络较好的话，非常快。<br>也可以参考J哥博客：<br>基于CentOS6.4环境编译Spark-2.1.0源码  <a href="http://blog.itpub.net/30089851/viewspace-2140779/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2140779/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Spark2.2.0 全网最详细的源码编译&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>为什么我们生产上要选择Spark On Yarn模式？</title>
    <link href="http://yoursite.com/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/13/为什么我们生产上要选择Spark On Yarn？/</id>
    <published>2018-04-12T16:00:00.000Z</published>
    <updated>2019-04-15T07:50:46.149Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，为什么我们生产上要选择Spark On Yarn？<br><a id="more"></a><br>开发上我们选择local[2]模式<br>生产上跑任务Job，我们选择Spark On Yarn模式 ，</p><p>将Spark Application部署到yarn中，有如下优点：</p><p>1.部署Application和服务更加方便</p><ul><li>只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。</li></ul><p>2.资源隔离机制</p><ul><li>yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。</li></ul><p>3.资源弹性管理</p><ul><li>Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。</li></ul><p>Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。</p><p><strong>运行client模式：</strong></p><ul><li><p>“./spark-shell –master yarn”</p></li><li><p>“./spark-shell –master yarn-client”</p></li><li><p>“./spark-shell –master yarn –deploy-mode client”</p></li></ul><p><strong>运行的是cluster模式</strong></p><ul><li><p>“./spark-shell –master yarn-cluster”</p></li><li><p>“./spark-shell –master yarn –deploy-mode cluster”</p></li></ul><p><strong>client和cluster模式的主要区别：<br>a. client的driver是运行在客户端进程中<br>b. cluster的driver是运行在Application Master之中</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，为什么我们生产上要选择Spark On Yarn？&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive全网最详细的编译及部署</title>
    <link href="http://yoursite.com/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2018/04/11/Hive全网最详细的编译及部署/</id>
    <published>2018-04-10T16:00:00.000Z</published>
    <updated>2019-04-16T12:28:25.419Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Hive全网最详细的编译及部署<br><a id="more"></a> </p><h6 id="一、需要安装的软件"><a href="#一、需要安装的软件" class="headerlink" title="一、需要安装的软件"></a>一、需要安装的软件</h6><ul><li><p>相关环境：</p><ul><li><p>jdk-7u80 </p><ul><li>hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7</li></ul></li><li><p>apache-maven-3.3.9</p></li><li><p>mysql5.1</p></li><li><p>hadoop伪分布集群已启动</p></li></ul></li></ul><h6 id="二、安装jdk"><a href="#二、安装jdk" class="headerlink" title="二、安装jdk"></a>二、安装jdk</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /usr/java &amp;&amp; cd  /usr/java/    </span><br><span class="line"></span><br><span class="line">tar -zxvf  /tmp/server-jre-7u80-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line">chown -R root:root  /usr/java/jdk1.7.0_80/ </span><br><span class="line"></span><br><span class="line">echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="三、安装maven"><a href="#三、安装maven" class="headerlink" title="三、安装maven"></a>三、安装maven</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">unzip /tmp/apache-maven-3.3.9-bin.zip</span><br><span class="line"></span><br><span class="line">chown root: /usr/local/apache-maven-3.3.9 -R</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="四、安装mysql"><a href="#四、安装mysql" class="headerlink" title="四、安装mysql"></a>四、安装mysql</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server mysql</span><br><span class="line"></span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">chkconfig mysqld on</span><br><span class="line"></span><br><span class="line">mysqladmin -u root password 123456</span><br><span class="line"></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;</span><br><span class="line"></span><br><span class="line">delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line"></span><br><span class="line">delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line"></span><br><span class="line">drop database test;</span><br><span class="line"></span><br><span class="line">DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h6 id="五、下载hive源码包："><a href="#五、下载hive源码包：" class="headerlink" title="五、下载hive源码包："></a>五、下载hive源码包：</h6><p>输入：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>根据cdh版本选择对应hive软件包：<br>hive-1.1.0-cdh5.7.1-src.tar.gz<br>解压后使用maven命令编译成安装包</p><h6 id="六、编译"><a href="#六、编译" class="headerlink" title="六、编译:"></a>六、编译:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line"></span><br><span class="line">tar -xf hive-1.1.0-cdh5.7.1-src.tar.gz</span><br><span class="line"></span><br><span class="line">cd /tmp/hive-1.1.0-cdh5.7.1</span><br><span class="line"></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br><span class="line"></span><br><span class="line"># 编译生成的包在以下位置：</span><br><span class="line"></span><br><span class="line"># packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br></pre></td></tr></table></figure><h6 id="七、安装编译生成的Hive包，然后测试"><a href="#七、安装编译生成的Hive包，然后测试" class="headerlink" title="七、安装编译生成的Hive包，然后测试"></a>七、安装编译生成的Hive包，然后测试</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-1.1.0-cdh5.7.1-bin hive</span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin </span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop hive </span><br><span class="line"></span><br><span class="line">echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h6 id="八、更改环境变量"><a href="#八、更改环境变量" class="headerlink" title="八、更改环境变量"></a>八、更改环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">cd /usr/local/hive</span><br><span class="line"></span><br><span class="line">cd conf</span><br></pre></td></tr></table></figure><p>1、hive-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh&amp;&amp;vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure></p><p>2、hive-site.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt; </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;vincent&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><h6 id="九、拷贝mysql驱动包到-HIVE-HOME-lib"><a href="#九、拷贝mysql驱动包到-HIVE-HOME-lib" class="headerlink" title="九、拷贝mysql驱动包到$HIVE_HOME/lib"></a>九、拷贝mysql驱动包到$HIVE_HOME/lib</h6><p>上方的hive-site.xml使用了java的mysql驱动包<br>需要将这个包上传到hive的lib目录之下<br>解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">unzip mysql-connector-java-5.1.45.zip</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure></p><p>未拷贝有相关报错：</p><p>The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. </p><p>Please check your CLASSPATH specification, </p><p>and the name of the driver.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Hive全网最详细的编译及部署&lt;br&gt;
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)</title>
    <link href="http://yoursite.com/2018/04/10/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(MapReduce+Yarn)/"/>
    <id>http://yoursite.com/2018/04/10/Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)/</id>
    <published>2018-04-09T16:00:00.000Z</published>
    <updated>2019-04-15T07:49:46.211Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)<br><a id="more"></a> </p><ol><li><p>修改mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]# cd /opt/software/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# cp mapred-site.xml.template  mapred-site.xml</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# cd ../../</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 hadoop]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 hadoop]# sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop全网最详细的伪分布式部署(HDFS)</title>
    <link href="http://yoursite.com/2018/04/08/Hadoop%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2(HDFS)/"/>
    <id>http://yoursite.com/2018/04/08/Hadoop全网最详细的伪分布式部署(HDFS)/</id>
    <published>2018-04-07T16:00:00.000Z</published>
    <updated>2019-04-15T07:49:38.673Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop全网最详细的伪分布式部署(HDFS)<br><a id="more"></a> </p><h6 id="1-添加hadoop用户"><a href="#1-添加hadoop用户" class="headerlink" title="1.添加hadoop用户"></a>1.添加hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 ~]# useradd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# vi /etc/sudoers</span><br><span class="line"># 找到root ALL=(ALL) ALL，添加</span><br><span class="line"></span><br><span class="line">hadoop ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure><h6 id="2-上传并解压"><a href="#2-上传并解压" class="headerlink" title="2.上传并解压"></a>2.上传并解压</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# rz #上传hadoop-2.8.1.tar.gz</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# tar -xzvf hadoop-2.8.1.tar.gz</span><br></pre></td></tr></table></figure><h6 id="3-软连接"><a href="#3-软连接" class="headerlink" title="3.软连接"></a>3.软连接</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# ln -s /opt/software/hadoop-2.8.1 /opt/software/hadoop</span><br></pre></td></tr></table></figure><h6 id="4-设置环境变量"><a href="#4-设置环境变量" class="headerlink" title="4.设置环境变量"></a>4.设置环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/software/hadoop</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="5-设置用户、用户组"><a href="#5-设置用户、用户组" class="headerlink" title="5.设置用户、用户组"></a>5.设置用户、用户组</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop/*</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# cd hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# rm -f *.txt</span><br></pre></td></tr></table></figure><h6 id="6-切换hadoop用户"><a href="#6-切换hadoop用户" class="headerlink" title="6.切换hadoop用户"></a>6.切换hadoop用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# ll</span><br><span class="line"></span><br><span class="line">total 32</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 bin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 etc</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 include</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 Jun  2 14:24 lib</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Aug 20 13:59 libexec</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop 4096 Aug 20 13:59 logs</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4096 Jun  2 14:24 sbin</span><br><span class="line"></span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop 4096 Jun  2 14:24 share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># bin:可执行文件</span><br><span class="line"></span><br><span class="line"># etc: 配置文件</span><br><span class="line"></span><br><span class="line"># sbin:shell脚本，启动关闭hdfs,yarn等</span><br></pre></td></tr></table></figure><h6 id="7-配置文件"><a href="#7-配置文件" class="headerlink" title="7.配置文件"></a>7.配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;hdfs://192.168.137.130:9000&lt;/value&gt;    # 配置自己机器的IP</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="8-配置hadoop用户的ssh信任关系"><a href="#8-配置hadoop用户的ssh信任关系" class="headerlink" title="8.配置hadoop用户的ssh信任关系"></a>8.配置hadoop用户的ssh信任关系</h6><p>8.1公钥/密钥   配置无密码登录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>8.2 查看日期，看是否配置成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh hadoop-01 date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date</span><br><span class="line"></span><br><span class="line">The authenticity of host &apos;hadoop-01 (192.168.137.130)&apos; can&apos;t be established.</span><br><span class="line"></span><br><span class="line">RSA key fingerprint is 09:f6:4a:f1:a0:bd:79:fd:34:e7:75:94:0b:3c:83:5a.</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes   # 第一次回车输入yes</span><br><span class="line"></span><br><span class="line">Warning: Permanently added &apos;hadoop-01,192.168.137.130&apos; (RSA) to the list of known hosts.</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:28 CST 2017</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# ssh localhost date   #不需要回车输入yes,即OK</span><br><span class="line"></span><br><span class="line">Sun Aug 20 14:22:29 CST 2017</span><br></pre></td></tr></table></figure></p><h6 id="9-格式化和启动"><a href="#9-格式化和启动" class="headerlink" title="9.格式化和启动"></a>9.格式化和启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">hadoop-01: Error: JAVA_HOME is not set and could not be found.</span><br><span class="line"></span><br><span class="line">localhost: Error: JAVA_HOME is not set and could not be found.</span><br></pre></td></tr></table></figure><p>9.1解决方法:添加环境变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]#  vi etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line"># 将export JAVA_HOME=$&#123;JAVA_HOME&#125;改为</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line"></span><br><span class="line">mkdir: cannot create directory `/opt/software/hadoop-2.8.1/logs&apos;: Permission denied</span><br></pre></td></tr></table></figure><p>9.2解决方法:添加权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# exit</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 hadoop]# cd ../</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# chown -R hadoop:hadoop hadoop-2.8.1</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 software]# su - hadoop</span><br><span class="line"></span><br><span class="line">[root@hadoop-01 ~]# cd /opt/software/hadoop</span><br></pre></td></tr></table></figure></p><p>9.3 继续启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p><p>9.4检查是否成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# jps</span><br><span class="line"></span><br><span class="line">19536 DataNode</span><br><span class="line"></span><br><span class="line">19440 NameNode</span><br><span class="line"></span><br><span class="line">19876 Jps</span><br><span class="line"></span><br><span class="line">19740 SecondaryNameNode</span><br></pre></td></tr></table></figure></p><p>9.5访问： <a href="http://192.168.137.130:50070" target="_blank" rel="noopener">http://192.168.137.130:50070</a></p><p>9.6修改dfs启动的进程，以hadoop-01启动</p><p>启动的三个进程：</p><p>namenode: hadoop-01    bin/hdfs getconf -namenodes</p><p>datanode: localhost    datanodes (using default slaves file)   etc/hadoop/slaves</p><p>secondarynamenode: 0.0.0.0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 ~]# cd /opt/software/hadoop</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# echo  &quot;hadoop-01&quot; &gt; ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# cat ./etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">hadoop-01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50090&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;hadoop-01:50091&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>9.7重启<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop]# sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop-01 hadoop]# sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop全网最详细的伪分布式部署(HDFS)&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令（三）</title>
    <link href="http://yoursite.com/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89/"/>
    <id>http://yoursite.com/2018/04/01/linux常用命令（3）/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2019-04-15T08:01:06.247Z</updated>
    
    <content type="html"><![CDATA[<p>Linux最常用实战命令（三）<br><a id="more"></a> </p><ol><li><p>用户、用户组</p><p> 用户</p><ul><li><p>useradd 用户名    添加用户</p></li><li><p>userdel 用户名    删除用户</p></li><li><p>id 用户名    查看用户信息</p></li><li><p>passwd 用户名    修改用户密码</p></li><li><p>su - 用户名    切换用户</p></li><li><p>ll /home/    查看已有的用户</p><p>用户组</p></li><li><p>groupadd 用户组    添加用户组</p></li><li><p>cat /etc/group    用户组的文件</p></li><li><p>usermod -a -G 用户组 用户    将用户添加到用户组中</p><p>给一个普通用户添加sudo权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span><br><span class="line">    #在root     ALL=(ALL)    ALL    下面添加一行</span><br><span class="line">    用户    ALL=(ALL)    NOPASSWD:ALL</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改文件权限</p><p> chown    修改文件或文件夹的所属用户和用户组</p><ul><li><p>chown -R 用户:用户组 文件夹名    -R 为递归参数，指针对文件夹</p></li><li><p>chown 用户:用户组 文件名</p><p>chmod: 修改文件夹或者文件的权限 </p></li><li><p>chmod -R 700 文件夹名 </p></li><li><p>chmod 700 文件夹名</p></li></ul></li></ol><pre><code>r  =&gt;    4w  =&gt;    2x  =&gt;    1</code></pre><ol start="3"><li>后台执行命令</li></ol><ul><li><p>&amp; </p></li><li><p>nohup </p></li><li><p>screen</p></li></ul><ol start="4"><li>多人合作    screen</li></ol><ul><li><p>screen -list    查看会话 </p></li><li><p>screen -S    建立一个后台的会话 </p></li><li><p>screen -r    进入会话 </p></li><li><p>ctrl+a+d    退出会话</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux最常用实战命令（三）&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux常用命令（二）</title>
    <link href="http://yoursite.com/2018/04/01/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/04/01/linux常用命令（二）/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2019-04-15T07:50:11.632Z</updated>
    
    <content type="html"><![CDATA[<p>Linux最常用实战命令（二）<br><a id="more"></a> </p><ol><li>实时查看文件内容    tail filename</li></ol><ul><li><p>tail -f filename    当文件(名)被修改后，不能监视文件内容</p></li><li><p>tail -F filename    当文件(名)被修改后，依然可以监视文件内容</p></li></ul><ol start="2"><li>复制、移动文件</li></ol><ul><li><p>cp oldfilename newfilename    复制</p></li><li><p>mv oldfilename newfilename    移动/重命名</p></li></ul><ol start="3"><li>echo</li></ol><ul><li><p>echo “xxx”     输出</p></li><li><p>echo “xxx” &gt; filename    覆盖</p></li><li><p>echo “xxx” &gt;&gt; filename    追加</p></li></ul><ol start="4"><li>删除    rm</li></ol><ul><li><p>rm -f    强制删除</p></li><li><p>rm -rf    强制删除文件夹，r 表示递归参数，指针对文件夹及文件夹里面文件 </p></li></ul><ol start="5"><li>别名 alias</li></ol><ul><li><p>alias x=”xxxxxx”    临时引用别名</p></li><li><p>alias x=”xxxxxx” 配置到环境变量中即为永久生效</p></li></ul><ol start="6"><li>查看历史命令    history</li></ol><ul><li><p>history    显示出所有历史记录 </p></li><li><p>history n    显示出n条记录 </p></li><li><p>!n    执行第n条记录</p></li></ul><ol start="7"><li>管道命令    （ | ）</li></ol><ul><li>管道的两边都是命令，左边的命令先执行，执行的结果作为右边命令的输入</li></ul><ol start="8"><li>查看进程、查看id、端口</li></ol><ul><li><p>ps -ef ｜grep 进程名    查看进程基本信息</p></li><li><p>netstat -npl｜grep 进程名或进程id    查看服务id和端口</p></li></ul><ol start="9"><li>杀死进程     kill</li></ol><ul><li><p>kill -9 进程名/pid    强制删除</p></li><li><p>kill -9 $(pgrep 进程名)：杀死与该进程相关的所有进程</p></li></ul><ol start="10"><li>rpm 搜索、卸载</li></ol><ul><li><p>rpm -qa | grep xxx     搜索xxx</p></li><li><p>rpm –nodeps -e xxx    删除xxx</p></li><li><p>–nodeps    不验证包的依赖性</p></li></ul><ol start="11"><li>查询</li></ol><ul><li><p>find 路径 -name xxx    (推荐)</p></li><li><p>which xxx</p></li><li><p>local xxx</p></li></ul><ol start="12"><li>查看磁盘、内存、系统的情况</li></ol><ul><li><p>df -h    查看磁盘大小及其使用情况</p></li><li><p>free -m    查看内存大小及其使用情况</p></li><li><p>top    查看系统情况</p></li></ul><ol start="13"><li>软连接</li></ol><ul><li>ln -s 原始目录 目标目录</li></ul><ol start="14"><li>压缩、解压</li></ol><ul><li><p>tar -czf    压缩     tar -xzvf    解压</p></li><li><p>zip    压缩    unzip    解压</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux最常用实战命令（二）&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux常用命令（一）</title>
    <link href="http://yoursite.com/2018/04/01/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4(%E4%B8%80)/"/>
    <id>http://yoursite.com/2018/04/01/Linux常用命令(一)/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2019-04-15T07:49:59.571Z</updated>
    
    <content type="html"><![CDATA[<p>Linux最常用实战命令（一）<br><a id="more"></a> </p><ol><li><p>查看当前目录    pwd</p></li><li><p>查看IP</p></li></ol><ul><li><p>ifconfig    查看虚拟机ip </p></li><li><p>hostname    主机名字</p><ul><li>i    查看主机名映射的IP</li></ul></li></ul><ol start="3"><li>切换目录    cd</li></ol><ul><li><p>cd ~    切换家目录（root为/root，普通用户为/home/用户名）</p></li><li><p>cd /filename    以绝对路径切换目录</p></li><li><p>cd -    返回上一次操作路径，并输出路径</p></li><li><p>cd ../    返回上一层目录</p></li></ul><ol start="4"><li><p>清理桌面    clear</p></li><li><p>显示当前目录文件和文件夹  ls</p></li></ol><ul><li><p>ls -l(ll)   显示详细信息</p></li><li><p>ls -la    显示详细信息+隐藏文件（以 . 开头，例：.ssh）</p></li><li><p>ls -lh    显示详细信息+文件大小</p></li><li><p>ls -lrt    显示详细信息+按时间排序</p></li></ul><ol start="6"><li><p>查看文件夹大小    du -sh</p></li><li><p>命令帮助</p></li></ol><ul><li><p>man 命令</p></li><li><p>命令 –help</p></li></ul><ol start="8"><li>创建文件夹    mkdir</li></ol><ul><li>mkdir -p filename1/filename2    递归创建文件夹</li></ul><ol start="9"><li><p>创建文件    touch/vi/echo xx&gt;filename</p></li><li><p>查看文件内容</p></li></ol><ul><li><p>cat filename    直接打印所有内容</p></li><li><p>more filename    根据窗口大小进行分页显示</p></li></ul><ol start="11"><li>文件编辑 vi</li></ol><ul><li><p>vi分为命令行模式，插入模式，尾行模式</p></li><li><p>命令行模式—&gt;插入模式：按i或a键</p></li><li><p>插入模式—&gt;命令行模式：按Esc键</p></li><li><p>命令行模式—&gt;尾行模式：按Shift和:键</p><p>  插入模式</p><ul><li><p>dd    删除光标所在行</p></li><li><p>n+dd    删除光标以下的n行</p></li><li><p>dG    删除光标以下行</p></li><li><p>gg    第一行第一个字母</p></li><li><p>G    最后一行第一个字母</p></li><li><p>shift+$    该行最后一个字母</p><p>尾行模式</p></li><li><p>q!    强制退出</p></li><li><p>qw    写入并退出</p></li><li><p>qw!    强制写入退出</p></li><li><p>x    退出，如果存在改动，则保存再退出</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux最常用实战命令（一）&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>HDFS架构设计及副本放置策略</title>
    <link href="http://yoursite.com/2018/03/30/HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2018/03/30/HDFS架构设计及副本放置策略/</id>
    <published>2018-03-29T16:00:00.000Z</published>
    <updated>2019-04-15T07:49:31.261Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS架构设计及副本放置策略<br><a id="more"></a><br>HDFS主要由3个组件构成，分别是<strong>NameNode、SecondaryNameNode和DataNode</strong>，HSFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。</p><h6 id="NameNode和DataNode架构图"><a href="#NameNode和DataNode架构图" class="headerlink" title="NameNode和DataNode架构图"></a>NameNode和DataNode架构图</h6><p><img src="/assets/blogImg/1.png" alt="1"><br>NameNode(名称节点)<br>存储：元信息的种类，包含:</p><ul><li>文件名称</li><li>文件目录结构</li><li>文件的属性[权限,创建时间,副本数]</li><li>文件对应哪些数据块–&gt;数据块对应哪些datanode节点</li><li>作用： </li><li>管理着文件系统命名空间</li><li>维护这文件系统树及树中的所有文件和目录</li><li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li></ul><p>DataNode(数据节点)<br>    存储：数据块、数据块校验、与NameNode通信<br>    作用：     </p><ul><li>读写文件的数据块</li><li>NameNode的指示来进行创建、删除、和复制等操作</li><li>通过心跳定期向NameNode发送所存储文件块列表信息</li><li>Scondary NameNode(第二名称节点)<br>  存储:    命名空间镜像文件fsimage+编辑日志editlog<br>  作用:    定期合并fsimage+editlog文件为新的fsimage推送给NamenNode<h6 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h6><img src="/assets/blogImg/2.png" alt="2"><br><strong>第一副本</strong>：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上<br><strong>第二副本</strong>：放置在与第一个副本不同的机架的节点上<br><strong>第三副本</strong>：与第二个副本相同机架的不同节点上<br>如果还有更多的副本：随机放在节点中</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS架构设计及副本放置策略&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>配置多台虚拟机之间的SSH信任</title>
    <link href="http://yoursite.com/2018/03/28/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84SSH%E4%BF%A1%E4%BB%BB/"/>
    <id>http://yoursite.com/2018/03/28/配置多台虚拟机之间的SSH信任/</id>
    <published>2018-03-27T16:00:00.000Z</published>
    <updated>2019-04-15T07:50:24.753Z</updated>
    
    <content type="html"><![CDATA[<h4 id="本机环境"><a href="#本机环境" class="headerlink" title="本机环境"></a>本机环境</h4><a id="more"></a> <p><img src="/assets/blogImg/640.png" alt="1"></p><p>3台机器执行命令ssh-keygen<br><img src="/assets/blogImg/641.png" alt="2"></p><p>选取第一台,生成authorized_keys文件<br><img src="/assets/blogImg/642.png" alt="3"></p><p>hadoop002 hadoop003传输id_rsa.pub文件到hadoop001<br><img src="/assets/blogImg/643.png" alt="4"><br><img src="/assets/blogImg/644.png" alt="5"></p><p>hadoop001机器 合并id_rsa.pub2、id_rsa.pub3到authorized_keys<br><img src="/assets/blogImg/645.png" alt="6"></p><p>设置每台机器的权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 -R ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>将authorized_keys分发到hadoop002、hadoop003机器<br><img src="/assets/blogImg/646.png" alt="7"></p><p><img src="/assets/blogImg/647.png" alt="8"></p><p>验证(每台机器上执行下面的命令，只输入yes，不输入密码，说明配置成功)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ssh root@hadoop002 date</span><br><span class="line">[root@hadoop002 ~]# ssh root@hadoop001 date</span><br><span class="line">[root@hadoop003 ~]# ssh root@hadoop001 date</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;本机环境&quot;&gt;&lt;a href=&quot;#本机环境&quot; class=&quot;headerlink&quot; title=&quot;本机环境&quot;&gt;&lt;/a&gt;本机环境&lt;/h4&gt;
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
</feed>
