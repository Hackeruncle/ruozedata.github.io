<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-25T14:51:30.678Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark不得不理解的重要概念——从源码角度看RDD</title>
    <link href="http://yoursite.com/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
    <id>http://yoursite.com/2018/05/20/Spark不得不理解的重要概念——从源码角度看RDD/</id>
    <published>2018-05-19T16:00:00.000Z</published>
    <updated>2019-04-25T14:51:30.678Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1.RDD是什么"></a>1.RDD是什么</h4><p>   Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合</p><h4 id="2-RDD五大特性"><a href="#2-RDD五大特性" class="headerlink" title="2.RDD五大特性"></a>2.RDD五大特性</h4><a id="more"></a> <ol><li><p>A list of partitions<br>每个rdd有多个分区<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>计算作用到每个分区<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rdd之间存在依赖（RDD的血缘关系）如：<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选，默认哈希的分区<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>源码来自github。</p><h4 id="3-如何创建RDD"><a href="#3-如何创建RDD" class="headerlink" title="3.如何创建RDD"></a>3.如何创建RDD</h4><p>创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfile（）</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p><strong>源码总结：<br>1）.取_2是因为数据为（key（偏移量），value（数据））</strong></p><h4 id="4-常见的transformation和action"><a href="#4-常见的transformation和action" class="headerlink" title="4.常见的transformation和action"></a>4.常见的transformation和action</h4><p>由于比较简单，大概说一下常用的用处，不做代码测试</p><p>transformation</p><ul><li>Map：对数据集的每一个元素进行操作</li><li>FlatMap：先对数据集进行扁平化处理，然后再Map</li><li>Filter：对数据进行过滤，为true则通过</li><li>destinct：去重操作</li></ul><p>action</p><ul><li>reduce：对数据进行聚集</li><li>reduceBykey：对key值相同的进行操作</li><li>collect：没有效果的action，但是很有用</li><li>saveAstextFile：数据存入文件系统</li><li>foreach：对每个元素进行func的操作</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;h4 id=&quot;1-RDD是什么&quot;&gt;&lt;a href=&quot;#1-RDD是什么&quot; class=&quot;headerlink&quot; title=&quot;1.RDD是什么&quot;&gt;&lt;/a&gt;1.RDD是什么&lt;/h4&gt;&lt;p&gt;   Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合&lt;/p&gt;
&lt;h4 id=&quot;2-RDD五大特性&quot;&gt;&lt;a href=&quot;#2-RDD五大特性&quot; class=&quot;headerlink&quot; title=&quot;2.RDD五大特性&quot;&gt;&lt;/a&gt;2.RDD五大特性&lt;/h4&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>美味不用等大数据面试题(201804月)</title>
    <link href="http://yoursite.com/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/"/>
    <id>http://yoursite.com/2018/05/20/美味不用等大数据面试题(201804月)/</id>
    <published>2018-05-19T16:00:00.000Z</published>
    <updated>2019-04-24T12:40:46.962Z</updated>
    
    <content type="html"><![CDATA[<h6 id="1-若泽大数据线下班，某某某的小伙伴现场面试题截图"><a href="#1-若泽大数据线下班，某某某的小伙伴现场面试题截图" class="headerlink" title="1.若泽大数据线下班，某某某的小伙伴现场面试题截图:"></a><strong>1.若泽大数据线下班，某某某的小伙伴现场面试题截图:</strong></h6><a id="more"></a> <p><img src="/assets/blogImg/520_1.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_2.png" alt="enter description here"></p><h6 id="2-分享另外1家的忘记名字公司的大数据面试题："><a href="#2-分享另外1家的忘记名字公司的大数据面试题：" class="headerlink" title="2.分享另外1家的忘记名字公司的大数据面试题："></a><strong>2.分享另外1家的忘记名字公司的大数据面试题：</strong></h6><p><img src="/assets/blogImg/520_3.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_4.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;1-若泽大数据线下班，某某某的小伙伴现场面试题截图&quot;&gt;&lt;a href=&quot;#1-若泽大数据线下班，某某某的小伙伴现场面试题截图&quot; class=&quot;headerlink&quot; title=&quot;1.若泽大数据线下班，某某某的小伙伴现场面试题截图:&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.若泽大数据线下班，某某某的小伙伴现场面试题截图:&lt;/strong&gt;&lt;/h6&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD、DataFrame和DataSet的区别</title>
    <link href="http://yoursite.com/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2018/05/19/Spark RDD、DataFrame和DataSet的区别/</id>
    <published>2018-05-18T16:00:00.000Z</published>
    <updated>2019-04-24T12:28:20.832Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br>在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！</font><h5 id="一-、共性"><a href="#一-、共性" class="headerlink" title="一 、共性"></a>一 、共性</h5><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p><p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</p><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>4、三者都有partition的概念。<br><a id="more"></a> </p><h5 id="二、RDD优缺点"><a href="#二、RDD优缺点" class="headerlink" title="二、RDD优缺点"></a>二、RDD优缺点</h5><p><strong>优点：</strong> </p><ul><li><p>1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</p></li><li><p>2、面向对象的编程风格</p></li><li><p>3、编译时类型安全，编译时就能检查出类型错误</p></li></ul><p><strong>缺点：</strong> </p><ul><li><p>1、序列化和反序列化的性能开销</p></li><li><p>2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p></li></ul><h5 id="三、DataFrame"><a href="#三、DataFrame" class="headerlink" title="三、DataFrame"></a>三、DataFrame</h5><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreach&#123;</span><br><span class="line">  x =&gt;</span><br><span class="line">    val v1=x.getAs[String](&quot;v1&quot;)</span><br><span class="line">    val v2=x.getAs[String](&quot;v2&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2、DataFrame引入了schema和off-heap</p><ul><li><p>schema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.</p></li><li><p>off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.</p></li><li><p>off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.</p></li></ul><p>3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表 </p><p>4、兼容Hive，支持Hql、UDF</p><p><strong>有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.</strong></p><h5 id="四、DataSet"><a href="#四、DataSet" class="headerlink" title="四、DataSet"></a>四、DataSet</h5><p>1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。</p><p>2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。</p><p>3、Dataset<row>等同于DataFrame（Spark 2.X）</row></p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！&lt;/font&gt;

&lt;h5 id=&quot;一-、共性&quot;&gt;&lt;a href=&quot;#一-、共性&quot; class=&quot;headerlink&quot; title=&quot;一 、共性&quot;&gt;&lt;/a&gt;一 、共性&lt;/h5&gt;&lt;p&gt;1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利&lt;/p&gt;
&lt;p&gt;2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。&lt;/p&gt;
&lt;p&gt;3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出&lt;/p&gt;
&lt;p&gt;4、三者都有partition的概念。&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决</title>
    <link href="http://yoursite.com/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
    <id>http://yoursite.com/2018/05/14/大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决/</id>
    <published>2018-05-13T16:00:00.000Z</published>
    <updated>2019-04-24T09:54:20.012Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一-数据源同步中间件："><a href="#一-数据源同步中间件：" class="headerlink" title="一.数据源同步中间件："></a>一.数据源同步中间件：</h5><p>Canal<br><a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">https://github.com/alibaba/canal</a><br><a href="https://github.com/Hackeruncle/syncClient" target="_blank" rel="noopener">https://github.com/Hackeruncle/syncClient</a></p><p>Maxwell<br><a href="https://github.com/zendesk/maxwell" target="_blank" rel="noopener">https://github.com/zendesk/maxwell</a><br><img src="/assets/blogImg/514_1.png" alt="maxwell"><br><a id="more"></a> </p><h5 id="二-架构使用"><a href="#二-架构使用" class="headerlink" title="二.架构使用"></a>二.架构使用</h5><p>MySQL —-  中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra  增量的<br>a.全量  bootstrap<br>b.增量  </p><h6 id="1-对比"><a href="#1-对比" class="headerlink" title="1.对比"></a>1.对比</h6><table><thead><tr><th></th><th></th><th>Canal(服务端)</th><th>Maxwell(服务端+客户端) </th></tr></thead><tbody><tr><td>语言</td><td>Java</td><td>Java</td><td></td></tr><tr><td>活跃度</td><td>活跃</td><td>活跃</td><td></td></tr><tr><td>HA</td><td>支持</td><td>定制  但是支持断点还原功能    </td></tr><tr><td>数据落地</td><td>定制</td><td>落地到kafka    </td></tr><tr><td>分区</td><td>支持</td><td>支持    </td></tr><tr><td>bootstrap(引导)</td><td>不支持</td><td>支持    </td></tr><tr><td>数据格式</td><td>格式自由</td><td>json(格式固定)    spark json–&gt;DF</td></tr><tr><td>文档</td><td>较详细</td><td>较详细</td><td></td></tr><tr><td>随机读</td><td>支持</td><td>支持</td><td></td></tr></tbody></table><p><strong>个人选择Maxwell</strong></p><p>a.服务端+客户端一体，轻量级的<br>b.支持断点还原功能+bootstrap+json<br>Can do SELECT * from table (bootstrapping) initial loads of a table.<br>supports automatic position recover on master promotion<br>flexible partitioning schemes for Kakfa - by database, table, primary key, or column<br>Maxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).</p><h6 id="2-官网解读"><a href="#2-官网解读" class="headerlink" title="2.官网解读"></a>2.官网解读</h6><p><a href="https://www.bilibili.com/video/av34778187?from=search&amp;seid=18393822973469412185" target="_blank" rel="noopener">B站视频</a></p><h6 id="3-部署"><a href="#3-部署" class="headerlink" title="3.部署"></a>3.部署</h6><p><strong>3.1 MySQL Install</strong><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a><br><a href="https://ke.qq.com/course/262452?tuin=11cffd50" target="_blank" rel="noopener">https://ke.qq.com/course/262452?tuin=11cffd50</a></p><p><strong>3.2 修改</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">binlog_format=row</span><br><span class="line"></span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line">3.3 创建Maxwell的db和用户</span><br><span class="line">mysql&gt; create database maxwell;</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></p><p><strong>3.4解压</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz</span><br></pre></td></tr></table></figure></p><p><strong>3.5测试STDOUT:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --user=&apos;maxwell&apos; \</span><br><span class="line">--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \</span><br><span class="line">--producer=stdout</span><br></pre></td></tr></table></figure></p><p>测试1：insert sql：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br></pre></td></tr></table></figure></p><p>maxwell输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;insert&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959044,</span><br><span class="line">    &quot;xid&quot;: 201,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试1：update sql:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update ruozedata set age=29 where id=999;</span><br></pre></td></tr></table></figure></p><p><strong>问题:  ROW，你觉得binlog更新几个字段？</strong></p><p>maxwell输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;update&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959208,</span><br><span class="line">    &quot;xid&quot;: 255,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 29,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;old&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h6 id="4-其他注意点和新特性"><a href="#4-其他注意点和新特性" class="headerlink" title="4.其他注意点和新特性"></a>4.其他注意点和新特性</h6><p><strong>4.1 kafka_version 版本</strong><br>Using kafka version: 0.11.0.1  0.10<br>jar:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 kafka-clients]# ll</span><br><span class="line">total 4000</span><br><span class="line">-rw-r--r--. 1 ruoze games  746207 May  8 06:34 kafka-clients-0.10.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  951041 May  8 06:35 kafka-clients-0.10.2.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games 1419544 May  8 06:35 kafka-clients-0.11.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  324016 May  8 06:34 kafka-clients-0.8.2.2.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  641408 May  8 06:34 kafka-clients-0.9.0.1.jar</span><br><span class="line">[root@hadoop000 kafka-clients]#</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一-数据源同步中间件：&quot;&gt;&lt;a href=&quot;#一-数据源同步中间件：&quot; class=&quot;headerlink&quot; title=&quot;一.数据源同步中间件：&quot;&gt;&lt;/a&gt;一.数据源同步中间件：&lt;/h5&gt;&lt;p&gt;Canal&lt;br&gt;&lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/alibaba/canal&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/Hackeruncle/syncClient&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Hackeruncle/syncClient&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maxwell&lt;br&gt;&lt;a href=&quot;https://github.com/zendesk/maxwell&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/zendesk/maxwell&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;/assets/blogImg/514_1.png&quot; alt=&quot;maxwell&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="其他组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/"/>
    
    
      <category term="maxwell" scheme="http://yoursite.com/tags/maxwell/"/>
    
  </entry>
  
  <entry>
    <title>Spark on YARN-Cluster和YARN-Client的区别</title>
    <link href="http://yoursite.com/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2018/05/12/Spark on YARN-Cluster和YARN-Client的区别/</id>
    <published>2018-05-11T16:00:00.000Z</published>
    <updated>2019-04-24T10:32:57.123Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一-YARN-Cluster和YARN-Client的区别"><a href="#一-YARN-Cluster和YARN-Client的区别" class="headerlink" title="一. YARN-Cluster和YARN-Client的区别"></a>一. YARN-Cluster和YARN-Client的区别</h5><p><img src="/assets/blogImg/512_1.png" alt><br>（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；<br>（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；<br>（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。<br><a id="more"></a> </p><h5 id="二-yarn-client-模式"><a href="#二-yarn-client-模式" class="headerlink" title="二. yarn client 模式"></a>二. yarn client 模式</h5><p><img src="/assets/blogImg/512_2.png" alt></p><p><font color="#FF4200">yarn-client  模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。<br></font></p><h5 id="三-yarn-cluster-模式"><a href="#三-yarn-cluster-模式" class="headerlink" title="三.yarn  cluster 模式"></a>三.yarn  cluster 模式</h5><p><img src="/assets/blogImg/512_3.png" alt></p><p><font color="#FF4200">yarn-cluster 模式的话， client 关闭是可以提交任务的 ，<br></font></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p><strong>1.spark-shell/spark-sql 只支持 yarn-client模式；<br>2.spark-submit对于两种模式都支持。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一-YARN-Cluster和YARN-Client的区别&quot;&gt;&lt;a href=&quot;#一-YARN-Cluster和YARN-Client的区别&quot; class=&quot;headerlink&quot; title=&quot;一. YARN-Cluster和YARN-Client的区别&quot;&gt;&lt;/a&gt;一. YARN-Cluster和YARN-Client的区别&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/512_1.png&quot; alt&gt;&lt;br&gt;（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；&lt;br&gt;（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；&lt;br&gt;（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="-spark - 架构" scheme="http://yoursite.com/tags/spark-%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>生产改造Spark1.6源代码，create table语法支持Oracle列表分区</title>
    <link href="http://yoursite.com/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/"/>
    <id>http://yoursite.com/2018/05/08/生产改造Spark1.6源代码，create table语法支持Oracle列表分区/</id>
    <published>2018-05-07T16:00:00.000Z</published>
    <updated>2019-04-24T10:36:40.482Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><h5 id="1-需求"><a href="#1-需求" class="headerlink" title="1.需求"></a>1.需求</h5><p>通过Spark SQL JDBC 方法，抽取Oracle表数据。</p><h5 id="2-问题"><a href="#2-问题" class="headerlink" title="2.问题"></a>2.问题</h5><p>大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。<br>参考 <a href="http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases" target="_blank" rel="noopener">http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases</a><br><a id="more"></a> </p><h5 id="3-Oracle的分区"><a href="#3-Oracle的分区" class="headerlink" title="3.Oracle的分区"></a>3.Oracle的分区</h5><h6 id="3-1列表分区"><a href="#3-1列表分区" class="headerlink" title="3.1列表分区:"></a>3.1列表分区:</h6><p>该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。<br>例一:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PROBLEM_TICKETS</span><br><span class="line">(</span><br><span class="line">PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),</span><br><span class="line">CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,</span><br><span class="line">STATUS VARCHAR2(20)</span><br><span class="line">)</span><br><span class="line">PARTITION BY LIST (STATUS)</span><br><span class="line">(</span><br><span class="line">PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,</span><br><span class="line">PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h6 id="3-2散列分区"><a href="#3-2散列分区" class="headerlink" title="3.2散列分区:"></a>3.2散列分区:</h6><p>这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。<br>例一:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE HASH_TABLE</span><br><span class="line">(</span><br><span class="line">COL NUMBER(8),</span><br><span class="line">INF VARCHAR2(100) </span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (COL)</span><br><span class="line">(</span><br><span class="line">PARTITION PART01 TABLESPACE HASH_TS01, </span><br><span class="line">PARTITION PART02 TABLESPACE HASH_TS02, </span><br><span class="line">PARTITION PART03 TABLESPACE HASH_TS03</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h5 id="4-改造"><a href="#4-改造" class="headerlink" title="4.改造"></a>4.改造</h5><p>蓝色代码是改造Spark源代码,加课程顾问领取PDF。</p><h6 id="1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。"><a href="#1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。" class="headerlink" title="1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。"></a>1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE TBLS_IN</span><br><span class="line">USING org.apache.spark.sql.jdbc OPTIONS (</span><br><span class="line">driver &quot;com.mysql.jdbc.Driver&quot;,</span><br><span class="line">url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,</span><br><span class="line">fetchSize &quot;1000&quot;,</span><br><span class="line">partitionColumn &quot;TBL_ID&quot;,</span><br><span class="line">numPartitions &quot;null&quot;,</span><br><span class="line">lowerBound &quot;null&quot;,</span><br><span class="line">upperBound &quot;null&quot;,</span><br><span class="line">user &quot;hive2user&quot;,</span><br><span class="line">password &quot;hive2user&quot;,</span><br><span class="line">partitionInRule &quot;1|15,16,18,19|20,21&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation"><a href="#2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation" class="headerlink" title="2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation"></a>2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">sqlContext: SQLContext,</span><br><span class="line">parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))</span><br><span class="line">val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)</span><br><span class="line">var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)</span><br><span class="line">var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)</span><br><span class="line"></span><br><span class="line">// add partition in rule</span><br><span class="line">val partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)</span><br><span class="line">// validind all the partition in rule </span><br><span class="line">if (partitionColumn != null</span><br><span class="line">&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)</span><br><span class="line">&amp;&amp; partitionInRule == null </span><br><span class="line">)&#123;</span><br><span class="line">   sys.error(&quot;Partitioning incompletely specified&quot;) </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val partitionInfo = </span><br><span class="line">if (partitionColumn == null) &#123; </span><br><span class="line">    null</span><br><span class="line">&#125; else &#123;</span><br><span class="line"></span><br><span class="line">val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123;</span><br><span class="line">val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot;</span><br><span class="line">upperBound = &quot;0&quot;</span><br><span class="line">inGroups &#125;</span><br><span class="line">else&#123;</span><br><span class="line">Array[String]() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JDBCPartitioningInfo( partitionColumn, </span><br><span class="line">lowerBound.toLong, </span><br><span class="line">upperBound.toLong, </span><br><span class="line">numPartitions.toInt, </span><br><span class="line">inPartitions)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val parts = JDBCRelation.columnPartition(partitionInfo)</span><br><span class="line">val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))</span><br><span class="line">// parameters is immutable</span><br><span class="line">if(numPartitions != null)&#123;</span><br><span class="line">properties.put(&quot;numPartitions&quot; , numPartitions) &#125;</span><br><span class="line">JDBCRelation(url, table, parts, properties)(sqlContext)</span><br><span class="line"></span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition"><a href="#3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition" class="headerlink" title="3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition"></a>3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;</span><br><span class="line">if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))</span><br><span class="line">val column = partitioning.column</span><br><span class="line">var i: Int = 0</span><br><span class="line">var ans = new ArrayBuffer[Partition]()</span><br><span class="line"></span><br><span class="line">// partition by long if(partitioning.inPartitions.length == 0)&#123;</span><br><span class="line"></span><br><span class="line">val numPartitions = partitioning.numPartitions</span><br><span class="line">if (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.</span><br><span class="line">// Here we get a little roundoff, but that&apos;s (hopefully) OK.</span><br><span class="line">val stride: Long = (partitioning.upperBound / numPartitions</span><br><span class="line"></span><br><span class="line">- partitioning.lowerBound / numPartitions)</span><br><span class="line">var currentValue: Long = partitioning.lowerBound</span><br><span class="line">while (i &lt; numPartitions) &#123;</span><br><span class="line">val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else null</span><br><span class="line">currentValue += stride</span><br><span class="line">val upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =</span><br><span class="line"></span><br><span class="line">if (upperBound == null) &#123; </span><br><span class="line">  lowerBound</span><br><span class="line"></span><br><span class="line">&#125; else if (lowerBound == null) &#123; </span><br><span class="line">  upperBound</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">  s&quot;$lowerBound AND $upperBound&quot; </span><br><span class="line">&#125;</span><br><span class="line">  ans += JDBCPartition(whereClause, i)</span><br><span class="line">   i= i+ 1 &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// partition by in </span><br><span class="line">else&#123;</span><br><span class="line">    while(i &lt; partitioning.inPartitions.length)&#123;</span><br><span class="line">           val inContent = partitioning.inPartitions(i)</span><br><span class="line">           val whereClause = s&quot;$column in ($inContent)&quot; </span><br><span class="line">           ans += JDBCPartition(whereClause, i)</span><br><span class="line">           i= i+ 1</span><br><span class="line">     &#125; </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ans.toArray </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="4-对外方法org-apache-spark-sql-SQLContext-方法jdbc"><a href="#4-对外方法org-apache-spark-sql-SQLContext-方法jdbc" class="headerlink" title="4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc"></a>4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">url: String,</span><br><span class="line">table: String,</span><br><span class="line">columnName: String,</span><br><span class="line">lowerBound: Long,</span><br><span class="line">upperBound: Long,</span><br><span class="line">numPartitions: Int,</span><br><span class="line">inPartitions: Array[String] = Array[String]()</span><br><span class="line"></span><br><span class="line">): DataFrame = &#123;</span><br><span class="line">read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;h5 id=&quot;1-需求&quot;&gt;&lt;a href=&quot;#1-需求&quot; class=&quot;headerlink&quot; title=&quot;1.需求&quot;&gt;&lt;/a&gt;1.需求&lt;/h5&gt;&lt;p&gt;通过Spark SQL JDBC 方法，抽取Oracle表数据。&lt;/p&gt;
&lt;h5 id=&quot;2-问题&quot;&gt;&lt;a href=&quot;#2-问题&quot; class=&quot;headerlink&quot; title=&quot;2.问题&quot;&gt;&lt;/a&gt;2.问题&lt;/h5&gt;&lt;p&gt;大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。&lt;br&gt;参考 &lt;a href=&quot;http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="源码阅读" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>生产中Hive静态和动态分区表，该怎样抉择呢？</title>
    <link href="http://yoursite.com/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/05/06/生产中Hive静态和动态分区表，该怎样抉择呢？/</id>
    <published>2018-05-05T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:24.149Z</updated>
    
    <content type="html"><![CDATA[<h6 id="一-需求"><a href="#一-需求" class="headerlink" title="一.需求"></a>一.需求</h6><p>按照不同部门作为分区，导数据到目标表</p><h6 id="二-使用静态分区表来完成"><a href="#二-使用静态分区表来完成" class="headerlink" title="二.使用静态分区表来完成"></a>二.使用静态分区表来完成</h6><p>71.创建静态分区表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table emp_static_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure></p><p>2.插入数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_static_partition partition(deptno=10)</span><br><span class="line">     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;</span><br></pre></td></tr></table></figure></p><a id="more"></a> <p>3.查询数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_static_partition;</span><br></pre></td></tr></table></figure></p><p><img src="/assets/blogImg/0506_1.png" alt></p><h6 id="三-使用动态分区表来完成"><a href="#三-使用动态分区表来完成" class="headerlink" title="三.使用动态分区表来完成"></a>三.使用动态分区表来完成</h6><p>1.创建动态分区表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table emp_dynamic_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure></p><font color="#FF4500">【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的</font><p>2.插入数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_dynamic_partition partition(deptno)     </span><br><span class="line">select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br></pre></td></tr></table></figure></p><font color="#FF4500">【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where</font><p>需要设置属性的值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；</span><br></pre></td></tr></table></figure></p><p>假如不设置，报错如下:<br><img src="/assets/blogImg/0506_2.png" alt><br>3.查询数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_dynamic_partition;</span><br></pre></td></tr></table></figure></p><p><img src="/assets/blogImg/0506_3.png" alt></p><p><font color="#FF4500">分区列为deptno，实现了动态分区</font></p><h6 id="四-总结"><a href="#四-总结" class="headerlink" title="四.总结"></a>四.总结</h6><p>在生产上我们更倾向是选择<strong>动态分区</strong>，<br>无需手工指定数据导入的具体分区，<br>而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。</p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;一-需求&quot;&gt;&lt;a href=&quot;#一-需求&quot; class=&quot;headerlink&quot; title=&quot;一.需求&quot;&gt;&lt;/a&gt;一.需求&lt;/h6&gt;&lt;p&gt;按照不同部门作为分区，导数据到目标表&lt;/p&gt;
&lt;h6 id=&quot;二-使用静态分区表来完成&quot;&gt;&lt;a href=&quot;#二-使用静态分区表来完成&quot; class=&quot;headerlink&quot; title=&quot;二.使用静态分区表来完成&quot;&gt;&lt;/a&gt;二.使用静态分区表来完成&lt;/h6&gt;&lt;p&gt;71.创建静态分区表：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;create table emp_static_partition(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;empno int, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ename string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;job string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mgr int, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hiredate string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sal double, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;comm double)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARTITIONED BY(deptno int)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;row format delimited fields terminated by &amp;apos;\t&amp;apos;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.插入数据：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hive&amp;gt;insert into table emp_static_partition partition(deptno=10)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用</title>
    <link href="http://yoursite.com/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/05/04/5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用/</id>
    <published>2018-05-03T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:29.426Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><p><img src="/assets/blogImg/504_1.png" alt><br><a id="more"></a> </p><h6 id="1-介绍："><a href="#1-介绍：" class="headerlink" title="1. 介绍："></a>1. 介绍：</h6><p>两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，<br>客户端可以在不启动CLI的情况下对Hive中的数据进行操作，<br>两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果<br>（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。</p><p>HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，<br>而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？<br>这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，<br>不能通过修改HiveServer的代码修正。</p><p>因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。<br>HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。</p><h6 id="2-配置参数"><a href="#2-配置参数" class="headerlink" title="2.配置参数"></a>2.配置参数</h6><p>Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：<br>参数 | 含义 |<br>-|-|<br>hive.server2.thrift.min.worker.threads|  最小工作线程数，默认为5。<br>hive.server2.thrift.max.worker.threads|  最小工作线程数，默认为500。<br>hive.server2.thrift.port|  TCP 的监听端口，默认为10000。<br>hive.server2.thrift.bind.host|  TCP绑定的主机，默认为localhost </p><p>配置监听端口和路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;192.168.48.130&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><h6 id="3-启动hiveserver2"><a href="#3-启动hiveserver2" class="headerlink" title="3. 启动hiveserver2"></a>3. 启动hiveserver2</h6><p>使用hadoop用户启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/</span><br><span class="line">[hadoop@hadoop001 bin]$ hiveserver2 </span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br></pre></td></tr></table></figure></p><h6 id="4-重新开个窗口，使用beeline方式连接"><a href="#4-重新开个窗口，使用beeline方式连接" class="headerlink" title="4. 重新开个窗口，使用beeline方式连接"></a>4. 重新开个窗口，使用beeline方式连接</h6><ul><li>-n 指定机器登陆的名字，当前机器的登陆用户名</li><li>-u 指定一个连接串</li><li>每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK</li><li>如果命令错误，hiveserver2那个窗口就会抛出异常</li></ul><p>使用hadoop用户启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoop</span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br><span class="line">scan complete in 4ms</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.7.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.7.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.7.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt;</span><br></pre></td></tr></table></figure></p><p>使用SQL<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show databases;</span><br><span class="line">INFO  : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</span><br><span class="line">INFO  : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">1 row selected</span><br></pre></td></tr></table></figure></p><h6 id="5-使用编写java代码方式连接"><a href="#5-使用编写java代码方式连接" class="headerlink" title="5.使用编写java代码方式连接"></a>5.使用编写java代码方式连接</h6><p><strong>5.1</strong>使用maven构建项目，pom.xml文件如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-train&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive-train&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;    </span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p><strong>5.2</strong>JdbcApp.java文件代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class JdbcApp &#123;</span><br><span class="line">     private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">         &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">             // TODO Auto-generated catch block</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">             System.exit(1);</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;);</span><br><span class="line">         Statement stmt = con.createStatement();</span><br><span class="line">         //select table:ename</span><br><span class="line">         String tableName = &quot;emp&quot;;</span><br><span class="line">         String sql = &quot;select ename from &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">          while(res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1));</span><br><span class="line">         &#125;</span><br><span class="line">         // describe table</span><br><span class="line">         sql = &quot;describe &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         res = stmt.executeQuery(sql);</span><br><span class="line">         while (res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;/assets/blogImg/504_1.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>2min快速了解，Hive内部表和外部表</title>
    <link href="http://yoursite.com/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
    <id>http://yoursite.com/2018/05/01/2min快速了解，Hive内部表和外部表/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:40.606Z</updated>
    
    <content type="html"><![CDATA[<p><font color="#FF4500"><br></font><br>在了解内部表和外部表区别前，<br>我们需要先了解一下<strong>Hive架构</strong> ：</p><p><img src="/assets/blogImg/501_1.png" alt="Hive架构"><br><a id="more"></a><br>大家可以简单看一下这个架构图，我介绍其中要点：<br>Hive的数据分为两种，<strong>一种为普通数据，一种为元数据。</strong></p><ol><li>元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。</li><li>Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。</li></ol><p>下面我们来介绍表的两种类型：内部表和外部表</p><ol><li><p>内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。</p></li><li><p>外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表<br>而内部表和外部表的主要区别就是 </p><ul><li>内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；</li><li>外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；</li></ul></li></ol><h6 id="1-准备数据-按tab键制表符作为字段分割符"><a href="#1-准备数据-按tab键制表符作为字段分割符" class="headerlink" title="1.准备数据:  按tab键制表符作为字段分割符"></a>1.准备数据:  按tab键制表符作为字段分割符</h6><pre><code>cat /tmp/ruozedata.txt1   jepson  32  1102   ruoze   22  1123   www.ruozedata.com   18  120</code></pre><h6 id="2-内部表测试："><a href="#2-内部表测试：" class="headerlink" title="2.内部表测试："></a>2.内部表测试：</h6><ol><li><p>在Hive里面创建一个表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table ruozedata(id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tele string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.759 seconds</span><br></pre></td></tr></table></figure></li><li><p>这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>内部表删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table ruozedata;</span><br></pre></td></tr></table></figure></li></ol><h6 id="3-外部表测试"><a href="#3-外部表测试" class="headerlink" title="3.外部表测试:"></a>3.外部表测试:</h6><ol><li>创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table exter_ruozedata(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tel string)</span><br><span class="line">    &gt; location &apos;/hive/external&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.098 seconds</span><br></pre></td></tr></table></figure></li></ol><p>创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径<br>（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）</p><ol start="2"><li><p>外部表导入数据和内部表一样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table exter_ruozedata;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;br&gt;在了解内部表和外部表区别前，&lt;br&gt;我们需要先了解一下&lt;strong&gt;Hive架构&lt;/strong&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blogImg/501_1.png&quot; alt=&quot;Hive架构&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="主题" scheme="http://yoursite.com/tags/%E4%B8%BB%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>谈谈我和大数据的情缘及入门</title>
    <link href="http://yoursite.com/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/05/01/谈谈我和大数据的情缘及入门/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:17.752Z</updated>
    
    <content type="html"><![CDATA[<p> &#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。</p><p> &#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。<br> <a id="more"></a><br> &#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。</p><p> &#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。</p><p>&#8195;<strong>后来这样的进度太慢了</strong>，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。</p><p>&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。</p><p> &#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。<strong>然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。</strong></p><p> &#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！</p><p><strong>以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。</strong><br><strong>1. 心态要端正。</strong><br>既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。<br>后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。</p><p><strong>2. 心目中要有计划。</strong><br>先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，<br>然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。</p><p><strong>3. 各种方式学习。</strong><br>QQ群，博客，上下班看技术文章，选择好的老师和课程培训，</p><p><font color="#FF4500"><br>(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)</font><br>可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。</p><p><strong>4. 项目经验。</strong><br>很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，<br>这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。<br>而面试，就看看其他人面试分享，学习他人。</p><p><strong>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &amp;#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。&lt;/p&gt;
&lt;p&gt; &amp;#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。&lt;br&gt;
    
    </summary>
    
      <category term="感想" scheme="http://yoursite.com/categories/%E6%84%9F%E6%83%B3/"/>
    
    
      <category term="人生感悟" scheme="http://yoursite.com/tags/%E4%BA%BA%E7%94%9F%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数(UDF)的部署使用，你会吗？</title>
    <link href="http://yoursite.com/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/27/Hive自定义函数(UDF)的部署使用，你会吗？/</id>
    <published>2018-04-26T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:11.545Z</updated>
    
    <content type="html"><![CDATA[<p>Hive自定义函数(UDF)的部署使用，你会吗，三种方式！<br><a id="more"></a> </p><font color="#FF4500"><br></font><h6 id="一-临时函数"><a href="#一-临时函数" class="headerlink" title="一.临时函数"></a>一.临时函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>添加jar包<br>hive&gt;add xxx.jar jar_filepath;</li><li>查看jar包<br>hive&gt;list jars;</li><li>创建临时函数<br>hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;</li></ol><h6 id="二-持久函数"><a href="#二-持久函数" class="headerlink" title="二.持久函数"></a>二.持久函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>上传到HDFS<br>$ hdfs dfs -put xxx.jar  hdfs:///path/to/xxx.jar</li><li>创建持久函数<br>hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;</li></ol><p><strong>注意点：</strong></p><ul><li><ol><li>此方法在show functions时是看不到的，但是可以使用</li></ol></li><li><ol start="2"><li>需要上传至hdfs</li></ol></li></ul><h6 id="三-持久函数，并注册"><a href="#三-持久函数，并注册" class="headerlink" title="三.持久函数，并注册"></a>三.持久函数，并注册</h6><p>环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>下载源码<br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a> </p></li><li><p>解压源码<br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>将HelloUDF.java文件增加到HIVE源码中<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>修改FunctionRegistry.java 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>重新编译<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>编译结果全部为：BUILD SUCCESS<br>文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>配置hive环境<br>配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：<br>7.1. 全部配置：参照之前文档  <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hive全网最详细的编译及部署</a></p><p>7.2. 将编译后带UDF函数的包复制到旧hive环境<br>   到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉<br>   命令：</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>最终启动hive</p></li><li><p>测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- 能查看到有 helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudf函数生效</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive自定义函数(UDF)的部署使用，你会吗，三种方式！&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数(UDF)的编程开发，你会吗？</title>
    <link href="http://yoursite.com/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/25/Hive自定义函数(UDF)的编程开发，你会吗？/</id>
    <published>2018-04-24T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:20.661Z</updated>
    
    <content type="html"><![CDATA[<p>本地开发环境：IntelliJ IDEA+Maven3.3.9<br><a id="more"></a> </p><h6 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h6><p>   打开IntelliJ IDEA<br>   File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h6><p> 在工程中找到pom.xml文件，添加hadoop、hive依赖<br> <img src="/assets/blogImg/425hive1.png" alt="Hive图1"></p><h6 id="3-创建类、并编写一个HelloUDF-java，代码如下："><a href="#3-创建类、并编写一个HelloUDF-java，代码如下：" class="headerlink" title="3. 创建类、并编写一个HelloUDF.java，代码如下："></a>3. 创建类、并编写一个HelloUDF.java，代码如下：</h6><p>  <img src="/assets/blogImg/425hive2.png" alt="Hive图2"></p><p><strong>首先一个UDF必须满足下面两个条件:</strong></p><ul><li><ol><li>一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）</li></ol></li><li><ol start="2"><li>一个UDF必须至少实现了evaluate()方法</li></ol></li></ul><h6 id="4-测试，右击运行run-‘HelloUDF-main-’"><a href="#4-测试，右击运行run-‘HelloUDF-main-’" class="headerlink" title="4. 测试，右击运行run ‘HelloUDF.main()’"></a>4. 测试，右击运行run ‘HelloUDF.main()’</h6><h6 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h6><p>在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包<br>执行成功后在日志中找：<br>     <font color="#FF4500">     [INFO] Building jar: (路径)/hive-1.0.jar  </font></p><p></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本地开发环境：IntelliJ IDEA+Maven3.3.9&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive DDL，你真的了解吗？</title>
    <link href="http://yoursite.com/2018/04/24/Hive%20DDL%EF%BC%8C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/24/Hive DDL，你真的了解吗？/</id>
    <published>2018-04-23T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:16.836Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，带你全面剖析Hive DDL！</p><font color="#FF4500"><br></font><p><img src="/assets/blogImg/hive424.png" alt="Hive架构图"><br><a id="more"></a> </p><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p><strong>Database</strong><br>Hive中包含了多个数据库，默认的数据库为default，对应于HDFS目录是/user/hadoop/hive/warehouse，可以通过hive.metastore.warehouse.dir参数进行配置（hive-site.xml中配置）</p><p><strong>Table</strong><br>Hive中的表又分为内部表和外部表 ,Hive 中的每张表对应于HDFS上的一个目录，HDFS目录为：/user/hadoop/hive/warehouse/[databasename.db]/table</p><p><strong>Partition</strong><br>分区，每张表中可以加入一个分区或者多个，方便查询，提高效率；并且HDFS上会有对应的分区目录：<br>/user/hadoop/hive/warehouse/[databasename.db]/table</p><h5 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h5><p><strong>Create Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure></p><p>IF NOT EXISTS：加上这句话代表判断数据库是否存在，不存在就会创建，存在就不会创建。<br>COMMENT：数据库的描述<br>LOCATION：创建数据库的地址，不加默认在/user/hive/warehouse/路径下<br>WITH DBPROPERTIES：数据库的属性</p><p><strong>Drop Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name </span><br><span class="line">[RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure></p><p>RESTRICT：默认是restrict，如果该数据库还有表存在则报错；<br>CASCADE：级联删除数据库(当数据库还有表时，级联删除表后在删除数据库)。</p><p><strong>Alter Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line"></span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>Use Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br></pre></td></tr></table></figure></p><p><strong>Show Databases</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE &apos;identifier_with_wildcards&apos;</span><br><span class="line">“ | ”：可以选择其中一种</span><br><span class="line"></span><br><span class="line">“[ ]”：可选项</span><br><span class="line"></span><br><span class="line">LIKE ‘identifier_with_wildcards’：模糊查询数据库</span><br></pre></td></tr></table></figure></p><p><strong>Describe Database</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE DATABASE [EXTENDED] db_name;</span><br><span class="line">DESCRIBE DATABASE db_name：查看数据库的描述信息和文件目录位置路径信息；</span><br><span class="line">EXTENDED：加上数据库键值对的属性信息。</span><br><span class="line">hive&gt; describe database default;</span><br><span class="line">OK</span><br><span class="line">default    Default Hive database    hdfs://hadoop1:9000/user/hive/warehouse    public    ROLE    </span><br><span class="line">Time taken: 0.065 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line"></span><br><span class="line">hive&gt; describe database extended hive2;</span><br><span class="line">OK</span><br><span class="line">hive2   it is my database       hdfs://hadoop1:9000/user/hive/warehouse/hive2.db        hadoop      USER    &#123;date=2018-08-08, creator=zhangsan&#125;</span><br><span class="line">Time taken: 0.135 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></p><p><strong>Create Table</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line"> ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line"> [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line"> | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"></span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br></pre></td></tr></table></figure></p><p><strong>data_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">: primitive_type</span><br><span class="line">| array_type</span><br><span class="line">| map_type</span><br><span class="line">| struct_type</span><br><span class="line">| union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>primitive_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> : TINYINT</span><br><span class="line"> | SMALLINT</span><br><span class="line"> | INT</span><br><span class="line"> | BIGINT</span><br><span class="line"> | BOOLEAN</span><br><span class="line">| FLOAT</span><br><span class="line"> | DOUBLE</span><br><span class="line"> | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line"> | STRING</span><br><span class="line"> | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line"> | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line"> | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line"> | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>array_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: ARRAY &lt; data_type &gt;</span><br></pre></td></tr></table></figure></p><p><strong>map_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: MAP &lt; primitive_type, data_type &gt;</span><br></pre></td></tr></table></figure></p><p><strong>struct_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br></pre></td></tr></table></figure></p><p><strong>union_type</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">: UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note:     Available in Hive 0.7.0 and later)</span><br></pre></td></tr></table></figure></p><p><strong>row_format</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure></p><p><strong>file_format:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure></p><p><strong>constraint_specification:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">TEMPORARY（临时表）</span><br><span class="line">Hive从0.14.0开始提供创建临时表的功能，表只对当前session有效，session退出后，表自动删除。</span><br><span class="line">语法：CREATE TEMPORARY TABLE …</span><br></pre></td></tr></table></figure></p><h6 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h6><ol><li>如果创建的临时表表名已存在，那么当前session引用到该表名时实际用的是临时表，只有drop或rename临时表名才能使用原始表</li><li>临时表限制：不支持分区字段和创建索引</li></ol><p>EXTERNAL（外部表）<br>Hive上有两种类型的表，一种是Managed Table(默认的)，另一种是External Table（加上EXTERNAL关键字）。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data（存储在HDFS上）和meta data（存储在MySQL），而External Table只会删meta data。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table external_table(</span><br><span class="line">  &gt; id int,</span><br><span class="line">&gt;  name string </span><br><span class="line">&gt; );</span><br></pre></td></tr></table></figure></p><p>PARTITIONED BY（分区表）<br>产生背景：如果一个表中数据很多，我们查询时就很慢，耗费大量时间，如果要查询其中部分数据该怎么办呢，这是我们引入分区的概念。</p><p>可以根据PARTITIONED BY创建分区表，一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下；</p><p>分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。</p><p>分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。</p><p>单分区：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE order_partition (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">    &gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (event_month string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>多分区：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE TABLE order_partition2 (</span><br><span class="line">&gt; order_number string,</span><br><span class="line">&gt; event_time string</span><br><span class="line">&gt; )</span><br><span class="line">&gt;  PARTITIONED BY (event_month string,every_day string);</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ hadoop fs -ls /user/hive/warehouse/hive.db</span><br><span class="line">18/01/08 05:07:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:04 /user/hive/warehouse/hive.db/order_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-08 05:05 /user/hive/warehouse/hive.db/order_partition2</span><br><span class="line">[hadoop@hadoop000 ~]$</span><br><span class="line">ROW FORMAT</span><br></pre></td></tr></table></figure><p>官网解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">: DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char [ESCAPED BY char]]       [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char]</span><br><span class="line">[NULL DEFINED AS char]   </span><br><span class="line">-- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure></p><p>DELIMITED：分隔符（可以自定义分隔符）；</p><p>FIELDS TERMINATED BY char:每个字段之间使用的分割；</p><p>例：-FIELDS TERMINATED BY ‘\n’ 字段之间的分隔符为\n;</p><p>COLLECTION ITEMS TERMINATED BY char:集合中元素与元素（array）之间使用的分隔符（collection单例集合的跟接口）；</p><p>MAP KEYS TERMINATED BY char：字段是K-V形式指定的分隔符；</p><p>LINES TERMINATED BY char：每条数据之间由换行符分割（默认[ \n ]）</p><p>一般情况下LINES TERMINATED BY char我们就使用默认的换行符\n，只需要指定FIELDS TERMINATED BY char。</p><p>创建demo1表，字段与字段之间使用\t分开，换行符使用默认\n：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo1(</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>创建demo2表，并指定其他字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table demo2 (</span><br><span class="line">&gt; id int,</span><br><span class="line">&gt; name string,</span><br><span class="line">&gt; hobbies ARRAY &lt;string&gt;,</span><br><span class="line">&gt; address MAP &lt;string, string&gt;</span><br><span class="line">&gt; )</span><br><span class="line">&gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">&gt; COLLECTION ITEMS TERMINATED BY &apos;-&apos;</span><br><span class="line">&gt; MAP KEYS TERMINATED BY &apos;:&apos;;</span><br><span class="line">OK</span><br><span class="line">STORED AS（存储格式）</span><br><span class="line">Create Table As Select</span><br></pre></td></tr></table></figure></p><p>创建表（拷贝表结构及数据，并且会运行MapReduce作业）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp (</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">salary double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br></pre></td></tr></table></figure></p><p>加载数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &quot;/home/hadoop/data/emp.txt&quot; OVERWRITE INTO TABLE emp;</span><br></pre></td></tr></table></figure></p><p>复制整张表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp2 as select * from emp;</span><br><span class="line">Query ID = hadoop_20180108043232_a3b15326-d885-40cd-89dd-e8fb1b8ff350</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1514116522188_0003, Tracking URL = http://hadoop1:8088/proxy/application_1514116522188_0003/</span><br><span class="line">Kill Command = /opt/software/hadoop/bin/hadoop job  -kill job_1514116522188_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2018-01-08 05:21:07,707 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2018-01-08 05:21:19,605 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 810 msec</span><br><span class="line">Ended Job = job_1514116522188_0003</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/.hive-staging_hive_2018-01-08_05-20-49_202_8556594144038797957-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp2</span><br><span class="line">Table hive.emp2 stats: [numFiles=1, numRows=14, totalSize=664, rawDataSize=650]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 3927 HDFS Write: 730 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 810 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 33.322 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp2</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.071 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>复制表中的一些字段<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table emp3 as select empno,ename from emp;</span><br></pre></td></tr></table></figure></p><p>LIKE<br>使用like创建表时，只会复制表的结构，不会复制表的数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table emp4 like emp;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.149 seconds</span><br><span class="line">hive&gt; select * from emp4;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.151 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>并没有查询到数据</p><p>desc formatted table_name<br>查询表的详细信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted emp;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>col_name                data_type               comment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">empno                   int                                         </span><br><span class="line">ename                   string                                      </span><br><span class="line">job                     string                                      </span><br><span class="line">mgr                     int                                         </span><br><span class="line">hiredate                string                                      </span><br><span class="line">salary                  double                                      </span><br><span class="line">comm                    double                                      </span><br><span class="line">deptno                  int</span><br></pre></td></tr></table></figure></p><p>Detailed Table Information<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Database:               hive                     </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Mon Jan 08 05:17:54 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Protect Mode:           None                     </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">COLUMN_STATS_ACCURATE    true                </span><br><span class="line">numFiles                1                   </span><br><span class="line">numRows                 0                   </span><br><span class="line">rawDataSize             0                   </span><br><span class="line">totalSize               668                 </span><br><span class="line">transient_lastDdlTime    1515359982</span><br></pre></td></tr></table></figure></p><p>Storage Information<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">field.delim             \t                  </span><br><span class="line">serialization.format    \t                  </span><br><span class="line">Time taken: 0.228 seconds, Fetched: 39 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>通过查询可以列出创建表时的所有信息，并且我们可以在mysql中查询出这些信息（元数据）select * from table_params;</p><p>查询数据库下的所有表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">emp</span><br><span class="line">emp1</span><br><span class="line">emp2</span><br><span class="line">emp3</span><br><span class="line">emp4</span><br><span class="line">order_partition</span><br><span class="line">order_partition2</span><br><span class="line">Time taken: 0.047 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p><p>查询创建表的语法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show create table emp;</span><br><span class="line">OK</span><br><span class="line">CREATE TABLE `emp`(</span><br><span class="line">  `empno` int, </span><br><span class="line">  `ename` string, </span><br><span class="line">  `job` string, </span><br><span class="line">  `mgr` int, </span><br><span class="line">  `hiredate` string, </span><br><span class="line">  `salary` double, </span><br><span class="line">  `comm` double, </span><br><span class="line">  `deptno` int)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;\t&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop1:9000/user/hive/warehouse/hive.db/emp&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;numRows&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;rawDataSize&apos;=&apos;0&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;668&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1515359982&apos;)</span><br><span class="line">Time taken: 0.192 seconds, Fetched: 24 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">Drop Table</span><br><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE];     -- (Note: PURGE available in Hive 0.14.0 and later)</span><br></pre></td></tr></table></figure></p><p>指定PURGE后，数据不会放到回收箱，会直接删除</p><p>DROP TABLE删除此表的元数据和数据。如果配置了垃圾箱（并且未指定PURGE），则实际将数据移至.Trash / Current目录。元数据完全丢失</p><p>删除EXTERNAL表时，表中的数据不会从文件系统中删除<br>Alter Table</p><p>重命名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table demo2 rename to new_demo2;</span><br><span class="line">OK</span><br><span class="line">Add Partitions</span><br><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &apos;location&apos;][, PARTITION partition_spec [LOCATION &apos;location&apos;], ...];</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line">  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)</span><br><span class="line">用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。分区名是字符串时加引号。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注：添加分区时可能出现FAILED: SemanticException table is not partitioned but partition spec exists错误。</span><br><span class="line">原因是，你在创建表时并没有添加分区，需要在创建表时创建分区，再添加分区。</span><br><span class="line"></span><br><span class="line">hive&gt;  create table dept(</span><br><span class="line">&gt;  deptno int,</span><br><span class="line">&gt; dname string,</span><br><span class="line">&gt; loc string</span><br><span class="line">&gt; )</span><br><span class="line">&gt; PARTITIONED BY (dt string)</span><br><span class="line">&gt;  ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.953 seconds </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath &apos;/home/hadoop/dept.txt&apos;into table dept partition (dt=&apos;2018-08-08&apos;);</span><br><span class="line">Loading data to table default.dept partition (dt=2018-08-08)</span><br><span class="line">Partition default.dept&#123;dt=2018-08-08&#125; stats: [numFiles=1, numRows=0, totalSize=84, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.147 seconds</span><br></pre></td></tr></table></figure></p><p>查询结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 0.481 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept ADD PARTITION (dt=&apos;2018-09-09&apos;);</span><br><span class="line">OK</span><br><span class="line">Drop Partitions</span><br><span class="line">ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER TABLE dept DROP PARTITION (dt=&apos;2018-09-09&apos;);</span><br></pre></td></tr></table></figure></p><p>查看分区语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept;</span><br><span class="line">OK</span><br><span class="line">dt=2018-08-08</span><br><span class="line">dt=2018-09-09</span><br><span class="line">Time taken: 0.385 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure></p><p>按分区查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from dept where dt=&apos;2018-08-08&apos;;</span><br><span class="line">OK</span><br><span class="line">10      ACCOUNTING      NEW YORK        2018-08-08</span><br><span class="line">20      RESEARCH        DALLAS  2018-08-08</span><br><span class="line">30      SALES   CHICAGO 2018-08-08</span><br><span class="line">40      OPERATIONS      BOSTON  2018-08-08</span><br><span class="line">Time taken: 2.323 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，带你全面剖析Hive DDL！&lt;/p&gt;
&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;/assets/blogImg/hive424.png&quot; alt=&quot;Hive架构图&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive生产上，压缩和存储结合使用案例</title>
    <link href="http://yoursite.com/2018/04/23/Hive%E7%94%9F%E4%BA%A7%E4%B8%8A%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2018/04/23/Hive生产上，压缩和存储结合使用案例/</id>
    <published>2018-04-22T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:04.965Z</updated>
    
    <content type="html"><![CDATA[<p>你们Hive生产上，压缩和存储，结合使用了吗？</p><p>案例：<br>原文件大小：19M<br><img src="/assets/blogImg/423_1.png" alt="enter description here"><br><a id="more"></a> </p><h6 id="1-ORC-Zlip结合"><a href="#1-ORC-Zlip结合" class="headerlink" title="1. ORC+Zlip结合"></a>1. ORC+Zlip结合</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc_zlib</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;ZLIB&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><font color="#FF4500"> 用ORC+Zlip之后的文件为2.8M<br><br></font><br> 用ORC+Zlip之后的文件为2.8M<br><img src="/assets/blogImg/423_2.png" alt="enter description here"><br><br><br>######  2. Parquet+gzip结合<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       set parquet.compression=gzip;</span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><br><br><font color="#FF4500"><br>用Parquet+gzip之后的文件为3.9M<br></font><p><img src="/assets/blogImg/423_3.png" alt="enter description here"></p><h6 id="3-Parquet-Lzo结合"><a href="#3-Parquet-Lzo结合" class="headerlink" title="3. Parquet+Lzo结合"></a>3. Parquet+Lzo结合</h6><p><strong>3.1 安装Lzo</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.06.tar.gz</span><br><span class="line">tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">cd lzo-2.06</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzo/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib/</span><br><span class="line">cp /usr/local/hadoop/lzo/lib/* /usr/lib64/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export PATH=/usr/local//hadoop/lzo/:$PATH</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include/</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>3.2 安装Lzop</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    wget http://www.lzop.org/download/lzop-1.03.tar.gz</span><br><span class="line">tar -zxvf lzop-1.03.tar.gz</span><br><span class="line">cd lzop-1.03</span><br><span class="line">./configure -enable-shared -prefix=/usr/local/hadoop/lzop</span><br><span class="line">make  &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p><strong>3.3 软连接</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/hadoop/lzop/bin/lzop /usr/bin/lzop</span><br></pre></td></tr></table></figure></p><p><strong>3.4 测试lzop</strong><br>        lzop xxx.log<br>    若生成xxx.log.lzo文件，则说明成功<br><strong>3.5 安装Hadoop-LZO</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   git或svn 下载https://github.com/twitter/hadoop-lzo</span><br><span class="line">cd hadoop-lzo</span><br><span class="line">mvn clean package -Dmaven.test.skip=true </span><br><span class="line">tar -cBf - -C target/native/Linux-amd64-64/lib . | tar -xBvf - -C /opt/software/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar /opt/software/hadoop/share/hadoop/common/</span><br></pre></td></tr></table></figure></p><p><strong>3.6 配置</strong><br>    在core-site.xml配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">     org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">           &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在mapred-site.xml中配置</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">           &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">           &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">           &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">在hadoop-env.sh中配置</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure></p><p><strong>3.7 测试</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.compress.output=true;  </span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line">SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line">create table page_views_parquet_lzo ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">TBLPROPERTIES(&quot;parquet.compression&quot;=&quot;lzo&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure></p><p><font color="#FF4500">用Parquet+Lzo(未建立索引)之后的文件为5.9M<br></font><br><img src="/assets/blogImg/423_4.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;你们Hive生产上，压缩和存储，结合使用了吗？&lt;/p&gt;
&lt;p&gt;案例：&lt;br&gt;原文件大小：19M&lt;br&gt;&lt;img src=&quot;/assets/blogImg/423_1.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
      <category term="案例" scheme="http://yoursite.com/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hive存储格式的生产应用</title>
    <link href="http://yoursite.com/2018/04/20/Hive%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%94%9F%E4%BA%A7%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2018/04/20/Hive存储格式的生产应用/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:24.992Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。</strong></p><p>原始大小: 19M</p><p><img src="/assets/blogImg/420_1.png" alt="enter description here"><br><a id="more"></a> </p><h5 id="1-TextFile-默认-文件大小为18-1M"><a href="#1-TextFile-默认-文件大小为18-1M" class="headerlink" title="1. TextFile(默认) 文件大小为18.1M"></a>1. TextFile(默认) 文件大小为18.1M</h5><p><img src="/assets/blogImg/420_2.png" alt="enter description here"></p><h5 id="2-SequenceFile"><a href="#2-SequenceFile" class="headerlink" title="2. SequenceFile"></a>2. SequenceFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_seq( </span><br><span class="line">track_time string, </span><br><span class="line">url string, </span><br><span class="line">session_id string, </span><br><span class="line">referer string, </span><br><span class="line">ip string, </span><br><span class="line">end_user_id string, </span><br><span class="line">city_id string </span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY “\t” </span><br><span class="line">STORED AS SEQUENCEFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_seq select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用SequenceFile存储后的文件为19.6M</strong><br><img src="/assets/blogImg/420_3.png" alt="enter description here"></p><h5 id="3-RcFile"><a href="#3-RcFile" class="headerlink" title="3. RcFile"></a>3. RcFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_rcfile(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS RCFILE;</span><br><span class="line"></span><br><span class="line">insert into table page_views_rcfile select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用RcFile存储后的文件为17.9M</strong><br><img src="/assets/blogImg/420_4.png" alt="enter description here"></p><h5 id="4-ORCFile"><a href="#4-ORCFile" class="headerlink" title="4. ORCFile"></a>4. ORCFile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   create table page_views_orc</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">STORED AS ORC </span><br><span class="line">TBLPROPERTIES(&quot;orc.compress&quot;=&quot;NONE&quot;)</span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure><p><strong>用ORCFile存储后的文件为7.7M</strong><br><img src="/assets/blogImg/420_5.png" alt="enter description here"></p><h5 id="5-Parquet"><a href="#5-Parquet" class="headerlink" title="5. Parquet"></a>5. Parquet</h5><pre><code>create table page_views_parquetROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;STORED AS PARQUET as select * from page_views;</code></pre><p><strong>用ORCFile存储后的文件为13.1M</strong><br><img src="/assets/blogImg/420_6.png" alt="enter description here"></p><p><strong>总结：磁盘空间占用大小比较</strong></p><font color="#FF4500">ORCFile(7.7M)&lt;parquet(13.1M)&lt;RcFile(17.9M)&lt;Textfile(18.1M)&lt;SequenceFile(19.6)</font>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;相同数据，分别以TextFile、SequenceFile、RcFile、ORC存储的比较。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;原始大小: 19M&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blogImg/420_1.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>大数据压缩，你们真的了解吗？</title>
    <link href="http://yoursite.com/2018/04/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%8C%E4%BD%A0%E4%BB%AC%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/18/大数据压缩，你们真的了解吗？/</id>
    <published>2018-04-17T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:41.342Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，带你们剖析大数据之压缩！<br><a id="more"></a> </p><h6 id="1-压缩的好处和坏处"><a href="#1-压缩的好处和坏处" class="headerlink" title="1. 压缩的好处和坏处"></a>1. 压缩的好处和坏处</h6><p> <strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h6 id="2-压缩格式"><a href="#2-压缩格式" class="headerlink" title="2. 压缩格式"></a>2. 压缩格式</h6><p><img src="/assets/blogImg/压缩1.png" alt="enter description here"><br>压缩比<br><img src="/assets/blogImg/压缩2.png" alt="enter description here"><br>压缩时间<br><img src="/assets/blogImg/yasuo3.png" alt="enter description here"></p><font color="#FF0000">可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</font><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>gzip</strong></td><td>压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td>不支持split </td></tr><tr><td><strong>lzo</strong></td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td><strong>snappy</strong></td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td><strong>bzip2</strong></td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h5><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h5 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a><strong>应用场景：</strong></h5><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，带你们剖析大数据之压缩！&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="压缩格式" scheme="http://yoursite.com/tags/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop常用命令大全</title>
    <link href="http://yoursite.com/2018/04/14/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
    <id>http://yoursite.com/2018/04/14/Hadoop常用命令大全/</id>
    <published>2018-04-13T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:35.108Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Hadoop常用命令大全<br><a id="more"></a> </p><h6 id="1-单独启动和关闭hadoop服务"><a href="#1-单独启动和关闭hadoop服务" class="headerlink" title="1. 单独启动和关闭hadoop服务"></a><strong>1. 单独启动和关闭hadoop服务</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>启动名称节点</strong></td><td style="text-align:center">hadoop-daemon.sh start namenode</td></tr><tr><td><strong>启动数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh start datanode slave</td></tr><tr><td><strong>启动secondarynamenode</strong></td><td style="text-align:center">hadoop-daemon.sh start secondarynamenode</td></tr><tr><td><strong>启动resourcemanager</strong></td><td style="text-align:center">yarn-daemon.sh start resourcemanager</td></tr><tr><td><strong>启动nodemanager</strong></td><td style="text-align:center">bin/yarn-daemons.sh start nodemanager</td></tr><tr><td><strong>停止数据节点</strong></td><td style="text-align:center">hadoop-daemons.sh stop datanode</td></tr></tbody></table><h6 id="2-常用的命令"><a href="#2-常用的命令" class="headerlink" title="2. 常用的命令"></a><strong>2. 常用的命令</strong></h6><table><thead><tr><th>功能</th><th style="text-align:center">命令</th></tr></thead><tbody><tr><td><strong>创建目录</strong></td><td style="text-align:center">hdfs dfs -mkdir /input</td></tr><tr><td><strong>查看</strong></td><td style="text-align:center">hdfs dfs  -ls</td></tr><tr><td><strong>递归查看</strong></td><td style="text-align:center">hdfs dfs ls -R</td></tr><tr><td><strong>上传</strong></td><td style="text-align:center">hdfs dfs -put </td></tr><tr><td><strong>下载</strong></td><td style="text-align:center">hdfs dfs -get </td></tr><tr><td><strong>删除</strong></td><td style="text-align:center">hdfs dfs -rm</td></tr><tr><td><strong>从本地剪切粘贴到hdfs</strong></td><td style="text-align:center">hdfs fs -moveFromLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>从hdfs剪切粘贴到本地</strong></td><td style="text-align:center">hdfs fs -moveToLocal /input/xx.txt /input/xx.txt</td></tr><tr><td><strong>追加一个文件到另一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -appedToFile ./hello.txt /input/hello.txt</td></tr><tr><td><strong>查看文件内容</strong></td><td style="text-align:center">hdfs fs -cat /input/hello.txt</td></tr><tr><td><strong>显示一个文件到末尾</strong></td><td style="text-align:center">hdfs fs -tail /input/hello.txt</td></tr><tr><td><strong>以字符串的形式打印文件的内容</strong></td><td style="text-align:center">hdfs fs -text /input/hello.txt</td></tr><tr><td><strong>修改文件权限</strong></td><td style="text-align:center">hdfs fs -chmod 666 /input/hello.txt</td></tr><tr><td><strong>修改文件所属</strong></td><td style="text-align:center">hdfs fs -chown ruoze.ruoze  /input/hello.txt</td></tr><tr><td><strong>从本地文件系统拷贝到hdfs里</strong></td><td style="text-align:center">hdfs fs -copyFromLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs拷贝到本地</strong></td><td style="text-align:center">hdfs fs -copyToLocal /input/hello.txt /input/</td></tr><tr><td><strong>从hdfs到一个路径拷贝到另一个路径</strong></td><td style="text-align:center">hdfs fs -cp /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>从hdfs到一个路径移动到另一个路径</strong></td><td style="text-align:center">hdfs fs -mv /input/xx.txt /output/xx.txt</td></tr><tr><td><strong>统计文件系统的可用空间信息</strong></td><td style="text-align:center">hdfs fs -df -h /</td></tr><tr><td><strong>统计文件夹的大小信息</strong></td><td style="text-align:center">hdfs fs -du -s -h /</td></tr><tr><td><strong>统计一个指定目录下的文件节点数量</strong></td><td style="text-align:center">hadoop  fs -count /aaa</td></tr><tr><td><strong>设置hdfs的文件副本数量</strong></td><td style="text-align:center">hadoop fs -setrep 3 /input/xx.txt</td></tr></tbody></table><h5 id="总结：一定要学会查看命令帮助"><a href="#总结：一定要学会查看命令帮助" class="headerlink" title="总结：一定要学会查看命令帮助"></a>总结：一定要学会查看命令帮助</h5><p><strong>1.hadoop命令直接回车查看命令帮助<br>2.hdfs命令、hdfs dfs命令直接回车查看命令帮助<br>3.hadoop fs 等价 hdfs dfs命令，和Linux的命令差不多。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Hadoop常用命令大全&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.2.0 全网最详细的源码编译</title>
    <link href="http://yoursite.com/2018/04/14/Spark2.2.0%20%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2018/04/14/Spark2.2.0 全网最详细的源码编译/</id>
    <published>2018-04-13T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:49.268Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Spark2.2.0 全网最详细的源码编译<br><a id="more"></a> </p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><hr><p>JDK： Spark 2.2.0及以上版本只支持JDK1.8 </p><hr><p>Maven：3.3.9<br>设置maven环境变量时，需设置maven内存：<br>export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”</p><hr><p>Scala：2.11.8</p><hr><p>Git</p><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>下载spark的tar包，并解压<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 source]$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</span><br><span class="line">[hadoop@hadoop000 source]$ tar -xzvf spark-2.2.0.tgz</span><br></pre></td></tr></table></figure></p><p>编辑dev/make-distribution.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi dev/make-distribution.sh</span><br><span class="line">注释以下内容：</span><br><span class="line">#VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v &quot;INFO&quot; | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v &quot;INFO&quot;\</span><br><span class="line">#    | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use &quot;set -o pipefail&quot;</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure></p><p>添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION=2.2.0</span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure></p><p>编辑pom.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ vi pom.xml</span><br><span class="line">添加在repositorys内</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">      &lt;id&gt;clouders&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;clouders Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br></pre></td></tr></table></figure></p><p>安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 spark-2.2.0]$ ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn</span><br></pre></td></tr></table></figure></p><p>稍微等待几小时，网络较好的话，非常快。<br>也可以参考J哥博客：<br>基于CentOS6.4环境编译Spark-2.1.0源码  <a href="http://blog.itpub.net/30089851/viewspace-2140779/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2140779/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Spark2.2.0 全网最详细的源码编译&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>为什么我们生产上要选择Spark On Yarn模式？</title>
    <link href="http://yoursite.com/2018/04/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%A6%81%E9%80%89%E6%8B%A9Spark%20On%20Yarn%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/13/为什么我们生产上要选择Spark On Yarn？/</id>
    <published>2018-04-12T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:46.141Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，为什么我们生产上要选择Spark On Yarn？<br><a id="more"></a><br>开发上我们选择local[2]模式<br>生产上跑任务Job，我们选择Spark On Yarn模式 ，</p><p>将Spark Application部署到yarn中，有如下优点：</p><p>1.部署Application和服务更加方便</p><ul><li>只需要yarn服务，包括Spark，Storm在内的多种应用程序不要要自带服务，它们经由客户端提交后，由yarn提供的分布式缓存机制分发到各个计算节点上。</li></ul><p>2.资源隔离机制</p><ul><li>yarn只负责资源的管理和调度，完全由用户和自己决定在yarn集群上运行哪种服务和Applicatioin，所以在yarn上有可能同时运行多个同类的服务和Application。Yarn利用Cgroups实现资源的隔离，用户在开发新的服务或者Application时，不用担心资源隔离方面的问题。</li></ul><p>3.资源弹性管理</p><ul><li>Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理。</li></ul><p>Spark On Yarn有两种模式，一种是cluster模式，一种是client模式。</p><p><strong>运行client模式：</strong></p><ul><li><p>“./spark-shell –master yarn”</p></li><li><p>“./spark-shell –master yarn-client”</p></li><li><p>“./spark-shell –master yarn –deploy-mode client”</p></li></ul><p><strong>运行的是cluster模式</strong></p><ul><li><p>“./spark-shell –master yarn-cluster”</p></li><li><p>“./spark-shell –master yarn –deploy-mode cluster”</p></li></ul><p><strong>client和cluster模式的主要区别：<br>a. client的driver是运行在客户端进程中<br>b. cluster的driver是运行在Application Master之中</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，为什么我们生产上要选择Spark On Yarn？&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive全网最详细的编译及部署</title>
    <link href="http://yoursite.com/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2018/04/11/Hive全网最详细的编译及部署/</id>
    <published>2018-04-10T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:51.666Z</updated>
    
    <content type="html"><![CDATA[<p>若泽大数据，Hive全网最详细的编译及部署<br><a id="more"></a> </p><h6 id="一、需要安装的软件"><a href="#一、需要安装的软件" class="headerlink" title="一、需要安装的软件"></a>一、需要安装的软件</h6><ul><li><p>相关环境：</p><ul><li><p>jdk-7u80 </p><ul><li>hadoop-2.6.0-cdh5.7.1 不支持jdk1.8，因此此处也延续jdk1.7</li></ul></li><li><p>apache-maven-3.3.9</p></li><li><p>mysql5.1</p></li><li><p>hadoop伪分布集群已启动</p></li></ul></li></ul><h6 id="二、安装jdk"><a href="#二、安装jdk" class="headerlink" title="二、安装jdk"></a>二、安装jdk</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir  /usr/java &amp;&amp; cd  /usr/java/    </span><br><span class="line"></span><br><span class="line">tar -zxvf  /tmp/server-jre-7u80-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line">chown -R root:root  /usr/java/jdk1.7.0_80/ </span><br><span class="line"></span><br><span class="line">echo &apos;export JAVA_HOME=/usr/java/jdk1.7.0_80&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="三、安装maven"><a href="#三、安装maven" class="headerlink" title="三、安装maven"></a>三、安装maven</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">unzip /tmp/apache-maven-3.3.9-bin.zip</span><br><span class="line"></span><br><span class="line">chown root: /usr/local/apache-maven-3.3.9 -R</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_HOME=/usr/local/apache-maven-3.3.9&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h6 id="四、安装mysql"><a href="#四、安装mysql" class="headerlink" title="四、安装mysql"></a>四、安装mysql</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server mysql</span><br><span class="line"></span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line"></span><br><span class="line">chkconfig mysqld on</span><br><span class="line"></span><br><span class="line">mysqladmin -u root password 123456</span><br><span class="line"></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;v123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;127.0.0.1&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">update user set password=password(&apos;123456&apos;) where user=&apos;root&apos;;</span><br><span class="line"></span><br><span class="line">delete from user where not (user=&apos;root&apos;) ;</span><br><span class="line"></span><br><span class="line">delete from user where user=&apos;root&apos; and password=&apos;&apos;; </span><br><span class="line"></span><br><span class="line">drop database test;</span><br><span class="line"></span><br><span class="line">DROP USER &apos;&apos;@&apos;%&apos;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h6 id="五、下载hive源码包："><a href="#五、下载hive源码包：" class="headerlink" title="五、下载hive源码包："></a>五、下载hive源码包：</h6><p>输入：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>根据cdh版本选择对应hive软件包：<br>hive-1.1.0-cdh5.7.1-src.tar.gz<br>解压后使用maven命令编译成安装包</p><h6 id="六、编译"><a href="#六、编译" class="headerlink" title="六、编译:"></a>六、编译:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line"></span><br><span class="line">tar -xf hive-1.1.0-cdh5.7.1-src.tar.gz</span><br><span class="line"></span><br><span class="line">cd /tmp/hive-1.1.0-cdh5.7.1</span><br><span class="line"></span><br><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br><span class="line"></span><br><span class="line"># 编译生成的包在以下位置：</span><br><span class="line"></span><br><span class="line"># packaging/target/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br></pre></td></tr></table></figure><h6 id="七、安装编译生成的Hive包，然后测试"><a href="#七、安装编译生成的Hive包，然后测试" class="headerlink" title="七、安装编译生成的Hive包，然后测试"></a>七、安装编译生成的Hive包，然后测试</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line"></span><br><span class="line">tar -xf /tmp/apache-hive-1.1.0-cdh5.7.1-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-1.1.0-cdh5.7.1-bin hive</span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop apache-hive-1.1.0-cdh5.7.1-bin </span><br><span class="line"></span><br><span class="line">chown -R hadoop:hadoop hive </span><br><span class="line"></span><br><span class="line">echo &apos;export HIVE_HOME=/usr/local/hive&apos;&gt;&gt;/etc/profile</span><br><span class="line"></span><br><span class="line">echo &apos;export PATH=$HIVE_HOME/bin:$PATH&apos;&gt;&gt;/etc/profile</span><br></pre></td></tr></table></figure><h6 id="八、更改环境变量"><a href="#八、更改环境变量" class="headerlink" title="八、更改环境变量"></a>八、更改环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">cd /usr/local/hive</span><br><span class="line"></span><br><span class="line">cd conf</span><br></pre></td></tr></table></figure><p>1、hive-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh&amp;&amp;vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure></p><p>2、hive-site.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt; </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;jdbc:mysql://localhost:3306/vincent_hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">        &lt;value&gt;vincent&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><h6 id="九、拷贝mysql驱动包到-HIVE-HOME-lib"><a href="#九、拷贝mysql驱动包到-HIVE-HOME-lib" class="headerlink" title="九、拷贝mysql驱动包到$HIVE_HOME/lib"></a>九、拷贝mysql驱动包到$HIVE_HOME/lib</h6><p>上方的hive-site.xml使用了java的mysql驱动包<br>需要将这个包上传到hive的lib目录之下<br>解压 mysql-connector-java-5.1.45.zip 对应的文件到目录即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">unzip mysql-connector-java-5.1.45.zip</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure></p><p>未拷贝有相关报错：</p><p>The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. </p><p>Please check your CLASSPATH specification, </p><p>and the name of the driver.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若泽大数据，Hive全网最详细的编译及部署&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="环境搭建" scheme="http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
</feed>
