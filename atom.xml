<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>若泽大数据 www.ruozedata.com</title>
  
  <subtitle>ruozedata</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-26T08:32:14.033Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ruozedata</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spark2.4.2详细介绍</title>
    <link href="http://yoursite.com/2019/04/23/spark2.4.2%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/04/23/spark2.4.2详细介绍/</id>
    <published>2019-04-22T16:00:00.000Z</published>
    <updated>2019-04-26T08:32:14.033Z</updated>
    
    <content type="html"><![CDATA[<p>Spark发布了最新的版本spark-2.4.2<br>根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的</p><h4 id="版本介绍"><a href="#版本介绍" class="headerlink" title="版本介绍"></a>版本介绍</h4><p><img src="/assets/blogImg/spark2.4.2_1.jpg" alt="enter description here"><br>Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。<font color="#FF4500"> <strong>我们强烈建议所有2.4用户升级到此稳定版本。</strong></font><br><a id="more"></a> </p><h4 id="显著的变化"><a href="#显著的变化" class="headerlink" title="显著的变化"></a>显著的变化</h4><p><img src="/assets/blogImg/spark2.4.2_2.jpg" alt="enter description here"></p><ul><li>SPARK-27419：在spark2.4中将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败。 因为该值将转换为0，心跳将始终超时，并最终终止执行程序。</li><li>还原SPARK-25250：可能导致作业永久挂起，在2.4.2中还原。</li></ul><h4 id="详细更改"><a href="#详细更改" class="headerlink" title="详细更改"></a>详细更改</h4><p><img src="/assets/blogImg/spark2.4.2_3.jpg" alt="enter description here"></p><h6 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26961" target="_blank" rel="noopener">[ SPARK-26961 ]</a></td><td>在Spark Driver中发现Java死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-26998" target="_blank" rel="noopener">[ SPARK-26998 ]</a></td><td>在Standalone模式下执行’ps -ef’程序进程,输出spark.ssl.keyStorePassword的明文</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27216" target="_blank" rel="noopener">[ SPARK-27216 ]</a></td><td>将RoaringBitmap升级到0.7.45以修复Kryo不安全的ser / dser问题</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27244" target="_blank" rel="noopener">[ SPARK-27244 ]</a></td><td>使用选项logConf = true时密码将以conf的明文形式记录</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27267" target="_blank" rel="noopener">[ SPARK-27267 ]</a></td><td>用Snappy 1.1.7.1解压、压缩空序列化数据时失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27275" target="_blank" rel="noopener">[ SPARK-27275 ]</a></td><td>EncryptedMessage.transferTo中的潜在损坏</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27301" target="_blank" rel="noopener">[ SPARK-27301 ]</a></td><td>DStreamCheckpointData因文件系统已缓存而无法清理</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27338" target="_blank" rel="noopener">[ SPARK-27338 ]</a></td><td>TaskMemoryManager和UnsafeExternalSorter $ SpillableIterator之间的死锁</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27351" target="_blank" rel="noopener">[ SPARK-27351 ]</a></td><td>在仅使用空值列的AggregateEstimation之后的错误outputRows估计</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27390" target="_blank" rel="noopener">[ SPARK-27390 ]</a></td><td>修复包名称不匹配</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27394" target="_blank" rel="noopener">[ SPARK-27394 ]</a></td><td>当没有任务开始或结束时，UI 的陈旧性可能持续数分钟或数小时</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27403" target="_blank" rel="noopener">[ SPARK-27403 ]</a></td><td>修复updateTableStats以使用新统计信息或无更新表统计信息</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27406" target="_blank" rel="noopener">[ SPARK-27406 ]</a></td><td>当两台机器具有不同的Oops大小时，UnsafeArrayData序列化会中断</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27419" target="_blank" rel="noopener">[ SPARK-27419 ]</a></td><td>将spark.executor.heartbeatInterval设置为小于1秒的值时，它将始终失败</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27453" target="_blank" rel="noopener">[ SPARK-27453 ]</a></td><td>DSV1静默删除DataFrameWriter.partitionBy</td></tr></tbody></table><h6 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27346" target="_blank" rel="noopener">[ SPARK-27346 ]</a></td><td>松开在ExpressionInfo的’examples’字段中换行断言条件</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27358" target="_blank" rel="noopener">[ SPARK-27358 ]</a></td><td>将jquery更新为1.12.x以获取安全修复程序</td></tr><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27479" target="_blank" rel="noopener">[ SPARK-27479 ]</a></td><td>隐藏“org.apache.spark.util.kvstore”的API文档</td></tr></tbody></table><h6 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h6><table><thead><tr><th>issues</th><th>内容摘要</th></tr></thead><tbody><tr><td><a href="https://issues.apache.org/jira/browse/SPARK-27382" target="_blank" rel="noopener">[ SPARK-27382 ]</a></td><td>在HiveExternalCatalogVersionsSuite中更新Spark 2.4.x测试</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark发布了最新的版本spark-2.4.2&lt;br&gt;根据官网介绍，此版本对于使用spark2.4的用户来说帮助是巨大的&lt;/p&gt;
&lt;h4 id=&quot;版本介绍&quot;&gt;&lt;a href=&quot;#版本介绍&quot; class=&quot;headerlink&quot; title=&quot;版本介绍&quot;&gt;&lt;/a&gt;版本介绍&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/spark2.4.2_1.jpg&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;Spark2.4.2是一个包含稳定性修复的维护版本。 此版本基于Spark2.4维护分支。&lt;font color=&quot;#FF4500&quot;&gt; &lt;strong&gt;我们强烈建议所有2.4用户升级到此稳定版本。&lt;/strong&gt;&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我司Kafka+Flink+MySQL生产完整案例代码</title>
    <link href="http://yoursite.com/2018/12/20/%E6%88%91%E5%8F%B8Kafka+Flink+MySQL%E7%94%9F%E4%BA%A7%E5%AE%8C%E6%95%B4%E6%A1%88%E4%BE%8B%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2018/12/20/我司Kafka+Flink+MySQL生产完整案例代码/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-05-03T01:24:05.187Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><h6 id="1-版本信息："><a href="#1-版本信息：" class="headerlink" title="1.版本信息："></a>1.版本信息：</h6><p>Flink Version:1.6.2<br>Kafka Version:0.9.0.0<br>MySQL Version:5.6.21</p><h6 id="2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER"><a href="#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER" class="headerlink" title="2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]"></a>2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.74.103.143    2018-12-20 18:12:00  &quot;GET /class/130.html HTTP/1.1&quot;     404 https://search.yahoo.com/search?p=Flink实战</span><br></pre></td></tr></table></figure><a id="more"></a> <h6 id="3-工程pom-xml"><a href="#3-工程pom-xml" class="headerlink" title="3.工程pom.xml"></a>3.工程pom.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">&lt;flink.version&gt;1.6.2&lt;/flink.version&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!--Flink-Kafka --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;5.1.39&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>4.sConf类 定义与MySQL连接的JDBC的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.conf;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:11</span><br><span class="line"> */</span><br><span class="line">public class sConf &#123;</span><br><span class="line">    public static final String USERNAME = &quot;root&quot;;</span><br><span class="line">    public static final String PASSWORD = &quot;www.ruozedata.com&quot;;</span><br><span class="line">    public static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;;</span><br><span class="line">    public static final String URL = &quot;jdbc:mysql://localhost:3306/soul&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h6 id="5-MySQLSlink类"><a href="#5-MySQLSlink类" class="headerlink" title="5.MySQLSlink类"></a>5.MySQLSlink类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.conf.sConf;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">/**</span><br><span class="line"> * @author 若泽数据soulChun</span><br><span class="line"> * @create 2018-12-20-15:09</span><br><span class="line"> */</span><br><span class="line">public class MySQLSink extends RichSinkFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt; &#123;</span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    private Connection connection;</span><br><span class="line">    private PreparedStatement preparedStatement;</span><br><span class="line">    public void invoke(Tuple5&lt;String, String, String, String, String&gt; value) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            if (connection == null) &#123;</span><br><span class="line">                Class.forName(sConf.DRIVERNAME);</span><br><span class="line">                connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">            &#125;</span><br><span class="line">            String sql = &quot;insert into log_info (ip,time,courseid,status_code,referer) values (?,?,?,?,?)&quot;;</span><br><span class="line">            preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1, value.f0);</span><br><span class="line">            preparedStatement.setString(2, value.f1);</span><br><span class="line">            preparedStatement.setString(3, value.f2);</span><br><span class="line">            preparedStatement.setString(4, value.f3);</span><br><span class="line">            preparedStatement.setString(5, value.f4);</span><br><span class="line">            System.out.println(&quot;Start insert&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public void open(Configuration parms) throws Exception &#123;</span><br><span class="line">        Class.forName(sConf.DRIVERNAME);</span><br><span class="line">        connection = DriverManager.getConnection(sConf.URL, sConf.USERNAME, sConf.PASSWORD);</span><br><span class="line">    &#125;</span><br><span class="line">    public void close() throws Exception &#123;</span><br><span class="line">        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="6-数据清洗日期工具类"><a href="#6-数据清洗日期工具类" class="headerlink" title="6.数据清洗日期工具类"></a>6.数据清洗日期工具类</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.utils;</span><br><span class="line">import org.apache.commons.lang3.time.FastDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-18:44</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line">    private static FastDateFormat SOURCE_FORMAT = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">    private static FastDateFormat TARGET_FORMAT = FastDateFormat.getInstance(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line">    public static Long  getTime(String  time) throws Exception&#123;</span><br><span class="line">        return SOURCE_FORMAT.parse(time).getTime();</span><br><span class="line">    &#125;</span><br><span class="line">    public static String parseMinute(String time) throws  Exception&#123;</span><br><span class="line">        return TARGET_FORMAT.format(new Date(getTime(time)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    //测试一下</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        String time = &quot;2018-12-19 18:55:00&quot;;</span><br><span class="line">        System.out.println(parseMinute(time));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="7-MySQL建表"><a href="#7-MySQL建表" class="headerlink" title="7.MySQL建表"></a>7.MySQL建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table log_info(</span><br><span class="line">ID INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">IP VARCHAR(50),</span><br><span class="line">TIME VARCHAR(50),</span><br><span class="line">CourseID VARCHAR(10),</span><br><span class="line">Status_Code VARCHAR(10),</span><br><span class="line">Referer VARCHAR(100),</span><br><span class="line">PRIMARY KEY ( ID )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h6 id="8-主程序："><a href="#8-主程序：" class="headerlink" title="8.主程序："></a>8.主程序：</h6><p>主要是将time的格式转成yyyyMMddHHmmss,</p><p>还有取URL中的课程ID，将不是/class开头的过滤掉。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.soul.kafka;</span><br><span class="line">import com.soul.utils.DateUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">/**</span><br><span class="line"> * @author soulChun</span><br><span class="line"> * @create 2018-12-19-17:23</span><br><span class="line"> */</span><br><span class="line">public class FlinkCleanKafka &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(5000);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);//zookeeper的节点的IP或者hostName，多个使用逗号进行分隔</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);//flink consumer flink的消费者的group.id</span><br><span class="line">        FlinkKafkaConsumer09&lt;String&gt; myConsumer = new FlinkKafkaConsumer09&lt;String&gt;(&quot;imooc_topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(myConsumer);</span><br><span class="line">//        stream.print().setParallelism(2);</span><br><span class="line">        DataStream CleanData = stream.map(new MapFunction&lt;String, Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple5&lt;String, String, String, String, String&gt; map(String value) throws Exception &#123;</span><br><span class="line">                String[] data = value.split(&quot;\\\t&quot;);</span><br><span class="line">                String CourseID = null;</span><br><span class="line">                String url = data[2].split(&quot;\\ &quot;)[2];</span><br><span class="line">                if (url.startsWith(&quot;/class&quot;)) &#123;</span><br><span class="line">                    String CourseHTML = url.split(&quot;\\/&quot;)[2];</span><br><span class="line">                    CourseID = CourseHTML.substring(0, CourseHTML.lastIndexOf(&quot;.&quot;));</span><br><span class="line">//                    System.out.println(CourseID);</span><br><span class="line">                &#125;</span><br><span class="line">                return Tuple5.of(data[0], DateUtils.parseMinute(data[1]), CourseID, data[3], data[4]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(new FilterFunction&lt;Tuple5&lt;String, String, String, String, String&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public boolean filter(Tuple5&lt;String, String, String, String, String&gt; value) throws Exception &#123;</span><br><span class="line">                return value.f2 != null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        CleanData.addSink(new MySQLSink());</span><br><span class="line">        env.execute(&quot;Flink kafka&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h6 id="9-启动主程序，查看MySQL表数据在递增"><a href="#9-启动主程序，查看MySQL表数据在递增" class="headerlink" title="9.启动主程序，查看MySQL表数据在递增"></a>9.启动主程序，查看MySQL表数据在递增</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select count(*) from log_info;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|    15137 |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Kafka过来的消息是我模拟的，一分钟产生100条。</p><p>以上是我司生产项目代码的抽取出来的案例代码V1。稍后还有WaterMark之类会做分享。</p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;h6 id=&quot;1-版本信息：&quot;&gt;&lt;a href=&quot;#1-版本信息：&quot; class=&quot;headerlink&quot; title=&quot;1.版本信息：&quot;&gt;&lt;/a&gt;1.版本信息：&lt;/h6&gt;&lt;p&gt;Flink Version:1.6.2&lt;br&gt;Kafka Version:0.9.0.0&lt;br&gt;MySQL Version:5.6.21&lt;/p&gt;
&lt;h6 id=&quot;2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot;&gt;&lt;a href=&quot;#2-Kafka-消息样例及格式：-IP-TIME-URL-STATU-CODE-REFERER&quot; class=&quot;headerlink&quot; title=&quot;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&quot;&gt;&lt;/a&gt;2.Kafka 消息样例及格式：[IP TIME URL STATU_CODE REFERER]&lt;/h6&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1.74.103.143    2018-12-20 18:12:00  &amp;quot;GET /class/130.html HTTP/1.1&amp;quot;     404 https://search.yahoo.com/search?p=Flink实战&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)</title>
    <link href="http://yoursite.com/2018/11/10/%E6%9C%80%E5%85%A8%E7%9A%84Flink%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B(KafkaSource+SinkToMySQL)/"/>
    <id>http://yoursite.com/2018/11/10/最全的Flink部署及开发案例(KafkaSource+SinkToMySQL)/</id>
    <published>2018-11-09T16:00:00.000Z</published>
    <updated>2019-05-03T11:57:01.078Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-下载Flink安装包"><a href="#1-下载Flink安装包" class="headerlink" title="1.下载Flink安装包"></a>1.下载Flink安装包</h5><p>flink下载地址</p><p><a href="https://archive.apache.org/dist/flink/flink-1.5.0/" target="_blank" rel="noopener">https://archive.apache.org/dist/flink/flink-1.5.0/</a></p><p>因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可</p><p>上传至机器的/opt目录下<br><a id="more"></a> </p><h5 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h5><p>tar -zxf flink-1.5.0-bin-scala_2.11.tgz -C ../opt/</p><h5 id="3-配置master节点"><a href="#3-配置master节点" class="headerlink" title="3.配置master节点"></a>3.配置master节点</h5><p>选择一个 master节点(JobManager)然后在conf/flink-conf.yaml中设置jobmanager.rpc.address 配置项为该节点的IP 或者主机名。确保所有节点有有一样的jobmanager.rpc.address 配置。</p><p>jobmanager.rpc.address: node1</p><p>(配置端口如果被占用也要改 如默认8080已经被spark占用，改成了8088)</p><p>rest.port: 8088</p><p>本次安装 master节点为node1，因为单机，slave节点也为node1</p><h5 id="4-配置slaves"><a href="#4-配置slaves" class="headerlink" title="4.配置slaves"></a>4.配置slaves</h5><p>将所有的 worker 节点 （TaskManager）的IP 或者主机名（一行一个）填入conf/slaves 文件中。</p><h5 id="5-启动flink集群"><a href="#5-启动flink集群" class="headerlink" title="5.启动flink集群"></a>5.启动flink集群</h5><p>bin/start-cluster.sh</p><p>打开 <a href="http://node1:8088" target="_blank" rel="noopener">http://node1:8088</a> 查看web页面<br><img src="/assets/blogImg/1110_1.png" alt="enter description here"><br>Task Managers代表当前的flink只有一个节点，每个task还有两个slots</p><h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6.测试"></a>6.测试</h5><p><strong>依赖</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;com.rz.flinkdemo&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Flink-programe&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">    &lt;flink.version&gt;1.5.0&lt;/flink.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.5.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></p><h5 id="7-Socket测试代码"><a href="#7-Socket测试代码" class="headerlink" title="7.Socket测试代码"></a>7.Socket测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class SocketWindowWordCount &#123;    public static void main(String[] args) throws Exception &#123;        // the port to connect to</span><br><span class="line">        final int port;        final String hostName;        try &#123;            final ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(&quot;port&quot;);</span><br><span class="line">            hostName = params.get(&quot;hostname&quot;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            System.err.println(&quot;No port or hostname specified. Please run &apos;SocketWindowWordCount --port &lt;port&gt; --hostname &lt;hostname&gt;&apos;&quot;);            return;</span><br><span class="line">        &#125;        // get the execution environment</span><br><span class="line">        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // get input data by connecting to the socket</span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port, &quot;\n&quot;);        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123;                    public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123;                        for (String word : value.split(&quot;\\s&quot;)) &#123;</span><br><span class="line">                            out.collect(new WordWithCount(word, 1L));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(&quot;word&quot;)</span><br><span class="line">                .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">                .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123;                    public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123;                        return new WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(&quot;Socket Window WordCount&quot;);</span><br><span class="line">    &#125;    // Data type for words with count</span><br><span class="line">    public static class WordWithCount &#123;        public String word;        public long count;        public WordWithCount() &#123;&#125;        public WordWithCount(String word, long count) &#123;            this.word = word;            this.count = count;</span><br><span class="line">        &#125;        @Override</span><br><span class="line">        public String toString() &#123;            return word + &quot; : &quot; + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包mvn clean install (如果打包过程中报错java.lang.OutOfMemoryError)</p><p>在命令行set MAVEN_OPTS= -Xms128m -Xmx512m</p><p>继续执行mvn clean install</p><p>生成FlinkTest.jar<br><img src="/assets/blogImg/1110_2.png" alt="enter description here"><br>找到打成的jar，并upload，开始上传<br><img src="/assets/blogImg/1110_3.png" alt="enter description here"><br>运行参数介绍<br><img src="/assets/blogImg/1110_4.png" alt="enter description here"><br><img src="/assets/blogImg/1110_5.png" alt="enter description here"><br><img src="/assets/blogImg/1110_6.png" alt="enter description here"><br>提交结束之后去overview界面看，可以看到，可用的slots变成了一个，因为我们的socket程序占用了一个，正在running的job变成了一个</p><p>发送数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 flink-1.5.0]# nc -l 8099</span><br><span class="line">aaa bbb</span><br><span class="line">aaa ccc</span><br><span class="line">aaa bbb</span><br><span class="line">bbb ccc</span><br></pre></td></tr></table></figure></p><p><img src="/assets/blogImg/1110_7.png" alt="enter description here"><br>点开running的job，你可以看见接收的字节数等信息</p><p>到log目录下可以清楚的看见输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost log]# tail -f flink-root-taskexecutor-2-localhost.out</span><br><span class="line">aaa : 1</span><br><span class="line">ccc : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br><span class="line">bbb : 1</span><br><span class="line">ccc : 1</span><br></pre></td></tr></table></figure></p><p>除了可以在界面提交，还可以将jar上传的linux中进行提交任务</p><p>运行flink上传的jar<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.rz.flinkdemo.SocketWindowWordCount jars/FlinkTest.jar --port 8099 --hostname node1</span><br></pre></td></tr></table></figure></p><p>其他步骤一致。</p><h5 id="8-使用kafka作为source"><a href="#8-使用kafka作为source" class="headerlink" title="8.使用kafka作为source"></a>8.使用kafka作为source</h5><p>加上依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.0&lt;/version&gt;&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class KakfaSource010 &#123;    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;node1:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);        //DataStream&lt;String&gt; test = env.addSource(new FlinkKafkaConsumer010&lt;String&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));</span><br><span class="line">        //可以通过正则表达式来匹配合适的topic</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaSource = new FlinkKafkaConsumer010&lt;&gt;(java.util.regex.Pattern.compile(&quot;test-[0-9]&quot;), new SimpleStringSchema(), properties);        //配置从最新的地方开始消费</span><br><span class="line">        kafkaSource.setStartFromLatest();        //使用addsource，将kafka的输入转变为datastream</span><br><span class="line">        DataStream&lt;String&gt; consume = env.addSource(wordfre);</span><br><span class="line"></span><br><span class="line">        ...        //process  and   sink</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;KakfaSource010&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="9-使用mysql作为sink"><a href="#9-使用mysql作为sink" class="headerlink" title="9.使用mysql作为sink"></a>9.使用mysql作为sink</h5><p>flink本身并没有提供datastream输出到mysql，需要我们自己去实现</p><p>首先，导入依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.1.30&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p><p>自定义sink，首先想到的是extends SinkFunction，集成flink自带的sinkfunction，再当中实现方法，实现如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink implements</span><br><span class="line">        SinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private static final long serialVersionUID = 1L;    private Connection connection;    private PreparedStatement preparedStatement;</span><br><span class="line">    String username = &quot;mysql.user&quot;;</span><br><span class="line">    String password = &quot;mysql.password&quot;;</span><br><span class="line">    String drivername = &quot;mysql.driver&quot;;</span><br><span class="line">    String dburl = &quot;mysql.url&quot;;    @Override</span><br><span class="line">    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;</span><br><span class="line">        Class.forName(drivername);</span><br><span class="line">        connection = DriverManager.getConnection(dburl, username, password);</span><br><span class="line">        String sql = &quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, value.f0);</span><br><span class="line">        preparedStatement.setString(2, value.f1);</span><br><span class="line">        preparedStatement.executeUpdate();        if (preparedStatement != null) &#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if (connection != null) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样实现有个问题，每一条数据，都要打开mysql连接，再关闭，比较耗时，这个可以使用flink中比较好的Rich方式来实现，代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public class MysqlSink extends RichSinkFunction&lt;Tuple2&lt;String,String&gt;&gt; &#123;    private Connection connection = null;    private PreparedStatement preparedStatement = null;    private String userName = null;    private String password = null;    private String driverName = null;    private String DBUrl = null;    public MysqlSink() &#123;</span><br><span class="line">        userName = &quot;mysql.username&quot;;</span><br><span class="line">        password = &quot;mysql.password&quot;;</span><br><span class="line">        driverName = &quot;mysql.driverName&quot;;</span><br><span class="line">        DBUrl = &quot;mysql.DBUrl&quot;;</span><br><span class="line">    &#125;    public void invoke(Tuple2&lt;String,String&gt; value) throws Exception &#123;        if(connection==null)&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">            connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">        &#125;</span><br><span class="line">        String sql =&quot;insert into table(name,nickname) values(?,?)&quot;;</span><br><span class="line">        preparedStatement = connection.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">        preparedStatement.setString(1,value.f0);</span><br><span class="line">        preparedStatement.setString(2,value.f1);</span><br><span class="line"></span><br><span class="line">        preparedStatement.executeUpdate();//返回成功的话就是一个，否则就是0</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">        Class.forName(driverName);</span><br><span class="line">        connection = DriverManager.getConnection(DBUrl, userName, password);</span><br><span class="line">    &#125;    @Override</span><br><span class="line">    public void close() throws Exception &#123;        if(preparedStatement!=null)&#123;</span><br><span class="line">            preparedStatement.close();</span><br><span class="line">        &#125;        if(connection!=null)&#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Rich方式的优点在于，有个open和close方法，在初始化的时候建立一次连接，之后一直使用这个连接即可，缩短建立和关闭连接的时间，也可以使用连接池实现，这里只是提供这样一种思路。</p><p>使用这个mysqlsink也非常简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//直接addsink，即可输出到自定义的mysql中，也可以将mysql的字段等写成可配置的，更加方便和通用proceDataStream.addSink(new MysqlSink());</span><br></pre></td></tr></table></figure></p><h5 id="10-总结"><a href="#10-总结" class="headerlink" title="10.总结"></a>10.总结</h5><p>本次的笔记做了简单的部署、测试、kafkademo，以及自定义实现mysqlsink的一些内容，其中比较重要的是Rich的使用，希望大家能有所收获。</p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;1-下载Flink安装包&quot;&gt;&lt;a href=&quot;#1-下载Flink安装包&quot; class=&quot;headerlink&quot; title=&quot;1.下载Flink安装包&quot;&gt;&lt;/a&gt;1.下载Flink安装包&lt;/h5&gt;&lt;p&gt;flink下载地址&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/flink/flink-1.5.0/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://archive.apache.org/dist/flink/flink-1.5.0/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因为例子不需要hadoop，下载flink-1.5.0-bin-scala_2.11.tgz即可&lt;/p&gt;
&lt;p&gt;上传至机器的/opt目录下&lt;br&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>若泽数据带你随时了解业界面试题，随时跳高薪</title>
    <link href="http://yoursite.com/2018/05/25/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE%E5%B8%A6%E4%BD%A0%E9%9A%8F%E6%97%B6%E4%BA%86%E8%A7%A3%E4%B8%9A%E7%95%8C%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%9A%8F%E6%97%B6%E8%B7%B3%E9%AB%98%E8%96%AA/"/>
    <id>http://yoursite.com/2018/05/25/若泽数据带你随时了解业界面试题，随时跳高薪/</id>
    <published>2018-05-24T16:00:00.000Z</published>
    <updated>2019-05-05T12:39:41.675Z</updated>
    
    <content type="html"><![CDATA[<h4 id="链家-一面，二面"><a href="#链家-一面，二面" class="headerlink" title="链家(一面，二面)"></a>链家(一面，二面)</h4><p>0.自我介绍</p><p>1.封装继承多态概念</p><p>2.mvc设计思想</p><p>3.线程池,看过源码吗<br><a id="more"></a><br>4.ssh框架中分别对应mvc中那一层</p><p>5.shell命令（查询一个文件有多少行。 chown 修改文件权限， 只记得那么多了 ）</p><p>6.spring ioc aop 原理</p><p>7.单利模式</p><p>8.SQL题，想不起来了。。</p><p>9.jvm 运行时数据区域</p><p>10.spring mvc知道吗。。</p><p>11.工厂模式</p><p>12.mr 计算流程</p><p>13.hive查询语句（表1：时间 食堂消费 表二：各个时间段 用户 每个食堂消费  查询用户在每个时间出现在那个食堂统计消费记录 ，大概是这样的。。）</p><p>14.git的使用</p><p>15.hadoop的理解</p><p>16.hive内部表和外部表的区别</p><p>17.hive存储格式和压缩格式</p><p>18.对spark了解吗？ 当时高级班还没学。。</p><p>19.hive于关系型数据库的区别</p><p>20.各种排序 手写堆排序,说说原理</p><p>21.链表问题，浏览器访问记录，前进后退形成链表，新加一个记录，多出一个分支，删除以前的分支。设计结构，如果这个结构写在函数中怎么维护。</p><p>22中间也穿插了项目。</p><p>无论是已经找到工作的还是正在工作的，我的觉的面试题都可以给您们带来一些启发。可以了解大数据行业需要什么样的人才，什么技能，对应去补充自己的不足之处，为下一个高薪工作做准备。</p><p>若泽大数据后面会随时更新学员面试题，让大家了解大数据行业的发展趋势，旨在帮助正在艰辛打拼的您指出一条区直的未来之路！（少走弯路噢噢。。）</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;链家-一面，二面&quot;&gt;&lt;a href=&quot;#链家-一面，二面&quot; class=&quot;headerlink&quot; title=&quot;链家(一面，二面)&quot;&gt;&lt;/a&gt;链家(一面，二面)&lt;/h4&gt;&lt;p&gt;0.自我介绍&lt;/p&gt;
&lt;p&gt;1.封装继承多态概念&lt;/p&gt;
&lt;p&gt;2.mvc设计思想&lt;/p&gt;
&lt;p&gt;3.线程池,看过源码吗&lt;br&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>一次跳槽经历（阿里/美团/头条/网易/有赞...)</title>
    <link href="http://yoursite.com/2018/05/24/%E6%9C%89%E8%B5%9E...)/"/>
    <id>http://yoursite.com/2018/05/24/有赞...)/</id>
    <published>2018-05-23T16:00:00.000Z</published>
    <updated>2019-04-26T13:21:04.634Z</updated>
    
    <content type="html"><![CDATA[<h6 id="为啥跳槽"><a href="#为啥跳槽" class="headerlink" title="为啥跳槽"></a>为啥跳槽</h6><p>每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。<br><img src="./assets/blogImg/tiaocao524.png" alt="enter description here"><br><a id="more"></a> </p><h6 id="面试过程"><a href="#面试过程" class="headerlink" title="面试过程"></a>面试过程</h6><p>（先打个广告，有兴趣加入阿里的欢迎发简历至 <a href="mailto:zhangzb2007@gmail.com" target="_blank" rel="noopener">zhangzb2007@gmail.com</a>，或简书上给我发信息）<br>面的是Java岗，总共面了7家公司，通过了6家。按自己的信心提升度我把面试过程分为上半场和下半场。</p><h6 id="上半场"><a href="#上半场" class="headerlink" title="上半场"></a>上半场</h6><ul><li><p>曹操专车<br>这是吉利集团下属子公司，已经是一家独角兽。一面中规中矩，没啥特别的。二面好像是个主管，隔了好几天，基本没问技术问题，反而是问职业规划，对加班有啥看法，有点措手不及，感觉回答的不好。但是过几天还是收到HR的现场面试通知。现场是技术面加HR面，技术面被问了几个问题有点懵逼：a. zookeeper的watcher乐观锁怎么实现 b. 一个项目的整个流程 c. 说出一个空间换时间的场景 d. centos7的内存分配方式和6有啥不同 f. 你对公司有什么价值。HR跟我说节后（那会再过两天就是清明）会给我消息，结果过了半个月突然接到他们的电话，说我通过了，给我讲了他们的薪资方案，没太大吸引力，再加上这种莫名其妙的时间等待，直接拒了。</p></li><li><p>美亚柏科<br>估计很多人没听说过这家公司，这是一家厦门本土公司，做政府安防项目的，在厦门也还是小有名气。但是面试完直接颠覆了我对这家公司的认知。进门最显眼的地方是党活动室，在等面试官的一小段时间里有好几拨人到里面参观。面试前做了一份笔试题，基本都是web/数据库方面的。第一面简单问了几个redis的问题之后面试官介绍了他们的项目，他们都是做C和C++的，想找一个人搭一套大数据集群，处理他们每天几百G的数据，然后服务器全部是windows！二面是另一个部门的，印象中就问了kafka为什么性能这么好，然后就开始问买房了没有，结婚了没有，他对我现在的公司比较了解，又扯了挺久。三面应该是个部门老大了，没有问技术问题，也是问买房了没，结婚没，问各种生活问题，有点像人口普查。我有点好奇，问他们为啥这么关心这些问题，他直接说他们更强调员工的稳定性，项目比较简单，能力不用要求太高，不要太差就行。汗，直接拒了。</p></li><li><p>有赞<br>绝对推荐的一家公司，效率超高。中午找了一个网友帮忙内推，晚上就开始一面，第二天早上二面，第三天HR就约现场面试时间，快的超乎想象。现场面也是先一个技术面，最后才HR面。面试的整体难度中等。现在就记得几个问题：G1和CMS的区别，G1有啥劣势；Kafka的整体架构；Netty的一次请求过程；自旋锁/偏向锁/轻量级锁（这个问题在头条的面试里也出现了一次）、hbase线上问题排查（刚好遇到过NUMA架构下的一个问题，借此把hbase的内核介绍了下）。<br>这里不得不说下有赞的人，真的很赞。终面的面试官是一个研发团队的负责人，全程一直微笑，中间电话响了一次，一直跟我道歉。面完之后还提供了团队的三个研发方向让我自己选择。后面看他的朋友圈状态，他那天高烧，面完我就去打点滴了，但是整个过程完全看不出来。帮我内推的网友是在微信群里找到的，知道我过了之后主动找我，让我过去杭州有啥问题随时找他。虽然最终没有去，但还是可以明显感受到他们的热情。</p></li><li><p>字节跳动(今日头条)<br>HR美眉打电话过来说是字节跳动公司，想约下视频面试时间。那会是有点懵的，我只知道今日头条和抖音。后面想到北京的号码才想起来。头条可以说是这次所有面试里流程最规范的，收到简历后有邮件通知，预约面试时间后邮件短信通知，面试完后不超过一天通知面试结果，每次面试有面试反馈。还有一个比较特别的，大部分公司的电话或者视频面试基本是下班后，头条都是上班时间，还不给约下班时间（难道他们不加班？）。<br>一面面试官刚上来就说他们是做go的，问我有没有兴趣，他自己也是Java转的。我说没问题，他先问了一些Java基础问题，然后有一道编程题，求一棵树两个节点的最近的公共父节点。思路基本是对的，但是有些细节有问题，面试官人很好，边看边跟我讨论，我边改进，前前后后估计用来快半小时。然后又继续问问题，HTTP 301 302有啥区别？设计一个短链接算法；md5长度是多少？整个面试过程一个多小时，自我感觉不是很好，我以为这次应该挂了，结果晚上收到面试通过的通知。<br>二面是在一个上午进行的，我以为zoom视频系统会自动连上（一面就是自动连上），就在那边等，过了5分钟还是不行，我就联系HR，原来要改id，终于连上后面试官的表情不是很好看，有点不耐烦的样子，不懂是不是因为我耽误了几分钟，这种表情延续了整个面试过程，全程有点压抑。问的问题大部分忘了，只记得问了一个线程安全的问题，ThreadLocal如果引用一个static变量是不是线程安全的？问着问着突然说今天面试到此为止，一看时间才过去二十几分钟。第二天就收到面试没过的通知，感觉自己二面答的比一面好多了，实在想不通。</p></li></ul><h6 id="下半场"><a href="#下半场" class="headerlink" title="下半场"></a>下半场</h6><p>一直感觉自己太水了，代码量不大，三年半的IT经验还有一年去做了产品，都不敢投大厂。上半场的技术面基本过了之后自信心大大提升，开始挑战更高难度的。</p><ul><li><p>美团<br>这个是厦门美团，他们在这边做了一个叫榛果民宿的APP，办公地点在JFC高档写字楼，休息区可以面朝大海，环境是很不错，面试就有点虐心了。<br>两点半进去。<br>一面。我的简历大部分是大数据相关的，他不是很了解，问了一些基础问题和netty的写流程，还问了一个redis数据结构的实现，结构他问了里面字符串是怎么实现的，有什么优势。一直感觉这个太简单，没好好看，只记得有标记长度，可以直接取。然后就来两道编程题。第一题是求一棵树所有左叶子节点的和，比较简单，一个深度优先就可以搞定。第二题是给定一个值K，一个数列，求数列中两个值a和b，使得a+b=k。我想到了一个使用数组下标的方法（感觉是在哪里有见过，不然估计是想不出来），这种可是达到O(n)的复杂度；他又加了个限制条件，不能使用更多内存，我想到了快排+遍历，他问有没有更优的，实在想不出来，他提了一个可以两端逼近，感觉很巧妙。<br>二面。面试官高高瘦瘦的，我对这种人的印象都是肯定很牛逼，可能是源于大学时代那些大牛都长这样。先让我讲下kafka的结构，然后怎么防止订单重复提交，然后开始围绕缓存同步问题展开了长达半小时的讨论：先写数据库，再写缓存有什么问题？先写缓存再写数据库有什么问题？写库成功缓存更新失败怎么办？缓存更新成功写库失败怎么办？他和我一起在一张纸上各种画，感觉不是面试，而是在设计方案。<br>三面。这是后端团队负责人了，很和蔼，一直笑呵呵。问了我一些微服务的问题，我提到了istio，介绍了设计理念，感觉他有点意外。然后他问java8的新特性，问我知不知道lambda表达式怎么来的，我从lambda演算说到lisp说到scala，感觉他更意外。此处有点吹牛了。我问了一些团队的问题，项目未来规划等，感觉榛果还是挺不错的。<br>四面。这个应该是榛果厦门的负责人了，技术问题问的不多，更多是一些职业规划，对业务的看法等。面试结束的时候他先出去，我收拾下东西，出去的时候发现他在电梯旁帮我开电梯，对待面试者的这种态度实在让人很有好感。<br>出来的时候已经是六点半。</p></li><li><p>网易<br>面的是网易云音乐，平时经常用，感觉如果可以参与研发应该是种挺美妙的感觉。<br>一面。下午打过来的，问我有没有空，我说有，他说你不用上班吗？有态度真的可以为所欲为（苦笑）。然后问了为什么离职，聊了会房价，问了几个netty的问题，gc的问题，最后问下对业务的看法。<br>然后约了个二面的时间，结果时间到了没人联系我，第二天打电话跟我道歉重新约了时间，不得不说态度还是很好的。二面问的反而很基础，没太多特别的。让我提问的时候我把美团二面里的缓存问题拿出来问他，很耐心的给我解答了好几分钟，人很好。</p></li><li><p>阿里<br>这个其实不是最后面试的，但是是最后结束的，不得不说阿里人真的好忙，周三跟我预约时间，然后已经排到下一周的周一。总体上感觉阿里的面试风格是喜欢在某个点上不断深入，直到你说不知道。<br>一面。自我介绍，然后介绍现在的项目架构，第一部分就是日志上传和接收，然后就如何保证日志上传的幂等性开始不断深入，先让我设计一个方案，然后问有没有什么改进的，然后如何在保证幂等的前提下提高性能，中间穿插分布式锁、redis、mq、数据库锁等各种问题。这个问题讨论了差不多半小时。然后就问我有没有什么要了解的，花了十几分钟介绍他们现在做的事情、技术栈、未来的一些计划，非常耐心。<br>二面。也是从介绍项目开始，然后抓住一个点，结合秒杀的场景深入，如何实现分布式锁、如何保证幂等性、分布式事务的解决方案。问我分布式锁的缺点，我说性能会出现瓶颈，他问怎么解决，我想了比较久，他提示说发散下思维，我最后想了个简单的方案，直接不使用分布式锁，他好像挺满意。感觉他们更看重思考的过程，而不是具体方案。还问了一致性hash如何保证负载均衡，kafka和rocketmq各自的优缺点，dubbo的一个请求过程、序列化方式，序列化框架、PB的缺点、如何从数据库大批量导入数据到hbase。<br>三面。是HR和主管的联合视频面试。这种面试还第一次遇到，有点紧张。主管先面，也是让我先介绍项目，问我有没有用过mq，如何保证消息幂等性。我就把kafka0.11版本的幂等性方案说了下，就没再问技术问题了。后面又问了为啥离职，对业务的看法之类的。然后就交给HR，只问了几个问题，然后就结束了，全程不到半小时。<br>不懂是不是跟面试的部门有关，阿里对幂等性这个问题很执着，三次都问到，而且还是从不同角度。</p></li></ul><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>从面试的难易程度看阿里 &gt; 美团 &gt; 头条 &gt; 有赞 &gt; 网易 &gt; 曹操专车 &gt; 美亚柏科。<strong>整个过程的体会是基础真的很重要，基础好了很多问题即使没遇到过也可以举一反三。</strong> 另外对一样技术一定要懂原理，而不仅仅是怎么使用，尤其是缺点，对选型很关键，可以很好的用来回答为什么不选xxx。另外对一些比较新的技术有所了解也是一个加分项。</p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;为啥跳槽&quot;&gt;&lt;a href=&quot;#为啥跳槽&quot; class=&quot;headerlink&quot; title=&quot;为啥跳槽&quot;&gt;&lt;/a&gt;为啥跳槽&lt;/h6&gt;&lt;p&gt;每次说因为生活成本的时候面试官都会很惊奇，难道有我们这里贵？好想直接给出下面这张图，厦门的房价真的好贵好贵好贵。。。&lt;br&gt;&lt;img src=&quot;./assets/blogImg/tiaocao524.png&quot; alt=&quot;enter description here&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Hive中自定义UDAF函数生产小案例</title>
    <link href="http://yoursite.com/2018/05/23/Hive%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%87%BD%E6%95%B0%E7%94%9F%E4%BA%A7%E5%B0%8F%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2018/05/23/Hive中自定义UDAF函数生产小案例/</id>
    <published>2018-05-22T16:00:00.000Z</published>
    <updated>2019-04-26T13:11:34.049Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、UDAF-回顾"><a href="#一、UDAF-回顾" class="headerlink" title="一、UDAF 回顾"></a>一、UDAF 回顾</h4><ul><li>1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。</li><li>2.Hive有两种UDAF：简单和通用<br>简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。<br>通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。</li><li>3.一个计算函数必须实现的5个方法的具体含义如下：<br>init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。<br>iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。<br>terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。<br>merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。<br>terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。<h4 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h4>使用UDAF简单方式实现统计区域产品用户访问排名<a id="more"></a> <h4 id="三、自定义UDAF函数代码实现"><a href="#三、自定义UDAF函数代码实现" class="headerlink" title="三、自定义UDAF函数代码实现"></a>三、自定义UDAF函数代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">package hive.org.ruozedata;</span><br><span class="line">import java.util.*;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span><br><span class="line">import org.apache.log4j.Logger;</span><br><span class="line">public class UserClickUDAF extends UDAF &#123;</span><br><span class="line">    // 日志对象初始化</span><br><span class="line">    public static Logger logger = Logger.getLogger(UserClickUDAF.class);</span><br><span class="line">    // 静态类实现UDAFEvaluator</span><br><span class="line">    public static class Evaluator implements UDAFEvaluator &#123;</span><br><span class="line">        // 设置成员变量，存储每个统计范围内的总记录数</span><br><span class="line">        private static Map&lt;String, String&gt; courseScoreMap;</span><br><span class="line">        private static Map&lt;String, String&gt; city_info;</span><br><span class="line">        private static Map&lt;String, String&gt; product_info;</span><br><span class="line">        private static Map&lt;String, String&gt; user_click;</span><br><span class="line">        //初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span><br><span class="line">        public Evaluator() &#123;</span><br><span class="line">            init();</span><br><span class="line">        &#125;</span><br><span class="line">        // 初始化函数间传递的中间变量</span><br><span class="line">        public void init() &#123;</span><br><span class="line">            courseScoreMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">            city_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">            product_info = new HashMap&lt;String, String&gt;();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">        //map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出</span><br><span class="line">        public boolean iterate(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (pcid == null || pcname == null || pccount == null) &#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">            if (pccount.equals(&quot;-1&quot;)) &#123;</span><br><span class="line">                // 城市表</span><br><span class="line">                city_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else if (pccount.equals(&quot;-2&quot;)) &#123;</span><br><span class="line">                // 产品表</span><br><span class="line">                product_info.put(pcid, pcname);</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // 处理用户点击关联</span><br><span class="line">                unionCity_Prod_UserClic1(pcid, pcname, pccount);</span><br><span class="line">           &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 处理用户点击关联</span><br><span class="line">        private void unionCity_Prod_UserClic1(String pcid, String pcname, String pccount) &#123;</span><br><span class="line">            if (product_info.containsKey(pcid)) &#123;</span><br><span class="line">                if (city_info.containsKey(pcname)) &#123;</span><br><span class="line">                    String city_name = city_info.get(pcname);</span><br><span class="line">                    String prod_name = product_info.get(pcid);</span><br><span class="line">                    String cp_name = city_name + prod_name;</span><br><span class="line">                    // 如果之前已经Put过Key值为区域信息，则把记录相加处理</span><br><span class="line">                    if (courseScoreMap.containsKey(cp_name)) &#123;</span><br><span class="line">                        int pcrn = 0;</span><br><span class="line">                        String strTemp = courseScoreMap.get(cp_name);</span><br><span class="line">                        String courseScoreMap_pn </span><br><span class="line">                         = strTemp.substring(strTemp.lastIndexOf(&quot;\t&quot;.toString())).trim();</span><br><span class="line">                        pcrn = Integer.parseInt(pccount) + Integer.parseInt(courseScoreMap_pn);</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ Integer.toString(pcrn));</span><br><span class="line">                    &#125;</span><br><span class="line">                    else &#123;</span><br><span class="line">                        courseScoreMap.put(cp_name, city_name + &quot;\t&quot; + prod_name + &quot;\t&quot;+ pccount);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput</span><br><span class="line">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span><br><span class="line">         */</span><br><span class="line">        public Map&lt;String, String&gt; terminatePartial() &#123;</span><br><span class="line">            return courseScoreMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span><br><span class="line">        public boolean merge(Map&lt;String, String&gt; mapOutput) &#123;</span><br><span class="line">            this.courseScoreMap.putAll(mapOutput);</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        // 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span><br><span class="line">        public String terminate() &#123;</span><br><span class="line">            return courseScoreMap.toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="四、创建hive中的临时函数"><a href="#四、创建hive中的临时函数" class="headerlink" title="四、创建hive中的临时函数"></a>四、创建hive中的临时函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION user_click;</span><br><span class="line">add jar /data/hive_udf-1.0.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION user_click AS &apos;hive.org.ruozedata.UserClickUDAF&apos;;</span><br></pre></td></tr></table></figure><h4 id="五、调用自定义UDAF函数处理数据"><a href="#五、调用自定义UDAF函数处理数据" class="headerlink" title="五、调用自定义UDAF函数处理数据"></a>五、调用自定义UDAF函数处理数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/works/tmp1&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">select regexp_replace(substring(rs, instr(rs, &apos;=&apos;)+1), &apos;&#125;&apos;, &apos;&apos;) from (</span><br><span class="line">  select explode(split(user_click(pcid, pcname, type),&apos;,&apos;)) as rs from (</span><br><span class="line">    select * from (</span><br><span class="line">      select &apos;-2&apos; as type, product_id as pcid, product_name as pcname from product_info</span><br><span class="line">      union all</span><br><span class="line">      select &apos;-1&apos; as type, city_id as pcid,area as pcname from city_info</span><br><span class="line">      union all</span><br><span class="line">      select count(1) as type,</span><br><span class="line">             product_id as pcid,</span><br><span class="line">             city_id as pcname</span><br><span class="line">        from user_click</span><br><span class="line">       where action_time=&apos;2016-05-05&apos;</span><br><span class="line">      group by product_id,city_id</span><br><span class="line">    ) a</span><br><span class="line">  order by type) b</span><br><span class="line">) c ;</span><br></pre></td></tr></table></figure><h4 id="六、创建Hive临时外部表"><a href="#六、创建Hive临时外部表" class="headerlink" title="六、创建Hive临时外部表"></a>六、创建Hive临时外部表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table tmp1(</span><br><span class="line">city_name string,</span><br><span class="line">product_name string,</span><br><span class="line">rn string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">location &apos;/works/tmp1&apos;;</span><br></pre></td></tr></table></figure><h4 id="七、统计最终区域前3产品排名"><a href="#七、统计最终区域前3产品排名" class="headerlink" title="七、统计最终区域前3产品排名"></a>七、统计最终区域前3产品排名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">select city_name,</span><br><span class="line">       product_name,</span><br><span class="line">       floor(sum(rn)) visit_num,</span><br><span class="line">       row_number()over(partition by city_name order by sum(rn) desc) rn,</span><br><span class="line">       &apos;2016-05-05&apos; action_time</span><br><span class="line">  from tmp1 </span><br><span class="line"> group by city_name,product_name</span><br><span class="line">) a where rn &lt;=3 ;</span><br></pre></td></tr></table></figure><h4 id="八、最终结果"><a href="#八、最终结果" class="headerlink" title="八、最终结果"></a>八、最终结果</h4><p><img src="/assets/blogImg/hive523.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;一、UDAF-回顾&quot;&gt;&lt;a href=&quot;#一、UDAF-回顾&quot; class=&quot;headerlink&quot; title=&quot;一、UDAF 回顾&quot;&gt;&lt;/a&gt;一、UDAF 回顾&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;1.定义：UDAF(User Defined Aggregation Funcation ) 用户自定义聚类方法，和group by联合使用，接受多个输入数据行，并产生一个输出数据行。&lt;/li&gt;
&lt;li&gt;2.Hive有两种UDAF：简单和通用&lt;br&gt;简单：利用抽象类UDAF和UDAFEvaluator，使用Java反射导致性能损失，且有些特性不能使用，如可变长度参数列表 。&lt;br&gt;通用：利用接口GenericUDAFResolver2（或抽象类AbstractGenericUDAFResolver）和抽象类GenericUDAFEvaluator，可以使用所有功能，但比较复杂，不直观。&lt;/li&gt;
&lt;li&gt;3.一个计算函数必须实现的5个方法的具体含义如下：&lt;br&gt;init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。&lt;br&gt;iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。&lt;br&gt;terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。&lt;br&gt;merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。&lt;br&gt;terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。&lt;h4 id=&quot;二、需求&quot;&gt;&lt;a href=&quot;#二、需求&quot; class=&quot;headerlink&quot; title=&quot;二、需求&quot;&gt;&lt;/a&gt;二、需求&lt;/h4&gt;使用UDAF简单方式实现统计区域产品用户访问排名
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark 基本概念</title>
    <link href="http://yoursite.com/2018/05/21/Spark%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/05/21/Spark 基本概念/</id>
    <published>2018-05-20T16:00:00.000Z</published>
    <updated>2019-04-29T12:59:01.981Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><p><strong>基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）</strong><br><a id="more"></a> </p><h5 id="spark名词解释"><a href="#spark名词解释" class="headerlink" title="spark名词解释"></a>spark名词解释</h5><ul><li><p>Application jar：应用程序jar包<br>包含了用户的 Spark 程序的一个 jar 包. 在某些情况下用户可能想要创建一个囊括了应用及其依赖的 “胖” jar 包. 但实际上, 用户的 jar 不应该包括 Hadoop 或是 Spark 的库, 这些库会在运行时被进行加载；</p></li><li><p>Driver Program：<br>这个进程运行应用程序的 main 方法并且新建 SparkContext ；</p></li><li><p>Cluster Manager：集群管理者<br>在集群上获取资源的外部服务 (例如:standalone,Mesos,Yarn)；（–master）</p></li><li><p>Deploy mode：部署模式<br>告诉你在哪里启动driver program. 在 “cluster” 模式下, 框架在集群内部运行 driver. 在 “client” 模式下, 提交者在集群外部运行 driver.；</p></li><li><p>Worker Node：工作节点<br>集群中任何可以运行应用代码的节点；（yarn上就是node manager）</p></li><li><p>Executor：<br>在一个工作节点上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。每个应用都有各自独立的 executors；</p></li><li><p>Task：任务<br>被送到某个 executor 上执行的工作单元；</p></li><li><p>Job：<br>包含很多并行计算的task。一个 action 就会产生一个job；</p></li><li><p>Stage：<br>一个 Job 会被拆分成多个task的集合，每个task集合被称为 stage，stage之间是相互依赖的(就像 Mapreduce 分 map和 reduce stages一样)，可以在Driver 的日志上看到。</p></li></ul><h5 id="spark工作流程"><a href="#spark工作流程" class="headerlink" title="spark工作流程"></a>spark工作流程</h5><p>1个action会触发1个job，1个job包含n个stage，每个stage包含n个task，n个task会送到n个executor上执行，一个Application是由一个driver 程序和n个 executor组成。提交的时候，通过Cluster Manager和Deploy mode控制。</p><p>spark应用程序在集群上运行一组独立的进程，通过SparkContext协调的在main方法里面。<br>如果运行在一个集群之上，SparkContext能够连接各种的集群管理者，去获取到作业所需要的资源。一旦连接成功，spark在集群节点之上运行executor进程，来给你的应用程序运行计算和存储数据。它会发送你的应用程序代码到executors上。最后，SparkContext发送tasks到executors上去运行</p><ul><li>1、每个Application都有自己独立的executor进程，这些进程在运行周期内都是常驻的以多线程的方式运行tasks。好处是每个进程无论是在调度还是执行都是相互独立的。所以，这就意味着数据不能跨应用程序进行共享，除非写到外部存储系统（Alluxio）。</li><li>2、spark并不关心底层的集群管理。</li><li>3、driver 程序会监听并且接收外面的一些executor请求，在整个生命周期里面。所以，driver 程序应该能被Worker Node通过网络访问。</li><li>4、因为driver 在集群上调度Tasks，driver 就应该靠近Worker Node。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;


&lt;p&gt;&lt;strong&gt;基于 Spark 构建的用户程序，包含了 一个driver 程序和集群上的 executors；（起了一个作业，就是一个Application）&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark History Server Web UI配置</title>
    <link href="http://yoursite.com/2018/05/21/Spark%20History%20Server%20Web%20UI%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2018/05/21/Spark History Server Web UI配置/</id>
    <published>2018-05-20T16:00:00.000Z</published>
    <updated>2019-04-26T12:55:10.070Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-进入spark目录和配置文件"><a href="#1-进入spark目录和配置文件" class="headerlink" title="1.进入spark目录和配置文件"></a>1.进入spark目录和配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd /opt/app/spark/conf</span><br><span class="line">[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><a id="more"></a> <h5 id="2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上"><a href="#2-创建spark-history的存储日志路径为hdfs上-当然也可以在linux文件系统上" class="headerlink" title="2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)"></a>2.创建spark-history的存储日志路径为hdfs上(当然也可以在linux文件系统上)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /Found 3 items</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:43 /spark</span><br><span class="line">drwxrwx---   - root root          0 2017-02-14 12:58 /tmp</span><br><span class="line">drwxr-xr-x   - root root          0 2017-02-14 12:58 /user</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -ls /sparkFound 1 items</span><br><span class="line">drwxrwxrwx   - root root          0 2017-02-15 21:44 /spark/checkpointdata</span><br><span class="line">[root@hadoop000 conf]# hdfs dfs -mkdir /spark/historylog</span><br></pre></td></tr></table></figure><p>在HDFS中创建一个目录，用于保存Spark运行日志信息。Spark History Server从此目录中读取日志信息 </p><h5 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 conf]# vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.eventLog.dir             hdfs://nameservice1/spark/historylog</span><br><span class="line">spark.yarn.historyServer.address 172.16.101.55:18080</span><br></pre></td></tr></table></figure><p>spark.eventLog.dir保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建<br>spark.yarn.historyServer.address : Spark history server的地址(不加http://).<br>这个地址会在Spark应用程序完成后提交给YARN RM，然后可以在RM UI上点击链接跳转到history server UI上.</p><h5 id="4-添加SPARK-HISTORY-OPTS参数"><a href="#4-添加SPARK-HISTORY-OPTS参数" class="headerlink" title="4.添加SPARK_HISTORY_OPTS参数"></a>4.添加SPARK_HISTORY_OPTS参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 conf]# vi spark-env.sh</span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bashexport SCALA_HOME=/root/learnproject/app/scalaexport JAVA_HOME=/usr/java/jdk1.8.0_111export SPARK_MASTER_IP=172.16.101.55export SPARK_WORKER_MEMORY=1gexport SPARK_PID_DIR=/root/learnproject/app/pidexport HADOOP_CONF_DIR=/root/learnproject/app/hadoop/etc/hadoopexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog \</span><br><span class="line">-Dspark.history.ui.port=18080 \</span><br><span class="line">-Dspark.history.retainedApplications=20&quot;</span><br></pre></td></tr></table></figure><h5 id="5-启动服务和查看"><a href="#5-启动服务和查看" class="headerlink" title="5.启动服务和查看"></a>5.启动服务和查看</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 spark]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /root/learnproject/app/spark/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-sht-sgmhadoopnn-01.out[root@hadoop01  ~]# jps28905 HistoryServer30407 ProdServerStart30373 ResourceManager30957 NameNode16949 Jps30280 DFSZKFailoverController31445 JobHistoryServer</span><br><span class="line">[root@hadoop01  ~]# ps -ef|grep sparkroot     17283 16928  0 21:42 pts/2    00:00:00 grep spark</span><br><span class="line">root     28905     1  0 Feb16 ?        00:09:11 /usr/java/jdk1.8.0_111/bin/java -cp /root/learnproject/app/spark/conf/:/root/learnproject/app/spark/jars/*:/root/learnproject/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://mycluster/spark/historylog -Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=20 -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@hadoop01  ~]# netstat -nlp|grep 28905</span><br><span class="line">tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      28905/java</span><br></pre></td></tr></table></figure><p>以上配置是针对使用自己编译的Spark部署到集群中一到两台机器上作为提交作业客户端的，如果你是CDH集群中集成的Spark那么可以在管理界面直接查看！</p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;1-进入spark目录和配置文件&quot;&gt;&lt;a href=&quot;#1-进入spark目录和配置文件&quot; class=&quot;headerlink&quot; title=&quot;1.进入spark目录和配置文件&quot;&gt;&lt;/a&gt;1.进入spark目录和配置文件&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@hadoop000 ~]# cd /opt/app/spark/conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@hadoop000 conf]# cp spark-defaults.conf.template spark-defaults.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark不得不理解的重要概念——从源码角度看RDD</title>
    <link href="http://yoursite.com/2018/05/20/Spark%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%90%86%E8%A7%A3%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E2%80%94%E2%80%94%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%9C%8BRDD/"/>
    <id>http://yoursite.com/2018/05/20/Spark不得不理解的重要概念——从源码角度看RDD/</id>
    <published>2018-05-19T16:00:00.000Z</published>
    <updated>2019-04-25T14:51:30.678Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1.RDD是什么"></a>1.RDD是什么</h4><p>   Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合</p><h4 id="2-RDD五大特性"><a href="#2-RDD五大特性" class="headerlink" title="2.RDD五大特性"></a>2.RDD五大特性</h4><a id="more"></a> <ol><li><p>A list of partitions<br>每个rdd有多个分区<br>protected def getPartitions: Array[Partition]</p></li><li><p>A function for computing each split<br>计算作用到每个分区<br>def compute(split: Partition, context: TaskContext): Iterator[T]</p></li><li><p>A list of dependencies on other RDDs<br>rdd之间存在依赖（RDD的血缘关系）如：<br>RDDA=&gt;RDDB=&gt;RDDC=&gt;RDDD<br>protected def getDependencies: Seq[Dependency[_]] = deps</p></li><li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选，默认哈希的分区<br>@transient val partitioner: Option[Partitioner] = None</p></li><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)<br>计算每个分区的最优执行位置，尽量实现数据本地化，减少IO（这往往是理想状态）<br>protected def getPreferredLocations(split: Partition): Seq[String] = Nil</p></li></ol><p>源码来自github。</p><h4 id="3-如何创建RDD"><a href="#3-如何创建RDD" class="headerlink" title="3.如何创建RDD"></a>3.如何创建RDD</h4><p>创建RDD有两种方式 parallelize() 和textfile()，其中parallelize可接收集合类，主要作为测试用。textfile可读取文件系统，是常用的一种方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parallelize()</span><br><span class="line">    def parallelize[T: ClassTag](    </span><br><span class="line">        seq: Seq[T],   </span><br><span class="line">        numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">        assertNotStopped()</span><br><span class="line">        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">textfile（）</span><br><span class="line">  def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">      assertNotStopped()</span><br><span class="line">      hadoopFile(path, classOf[TextInputFormat], </span><br><span class="line">                       classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p><strong>源码总结：<br>1）.取_2是因为数据为（key（偏移量），value（数据））</strong></p><h4 id="4-常见的transformation和action"><a href="#4-常见的transformation和action" class="headerlink" title="4.常见的transformation和action"></a>4.常见的transformation和action</h4><p>由于比较简单，大概说一下常用的用处，不做代码测试</p><p>transformation</p><ul><li>Map：对数据集的每一个元素进行操作</li><li>FlatMap：先对数据集进行扁平化处理，然后再Map</li><li>Filter：对数据进行过滤，为true则通过</li><li>destinct：去重操作</li></ul><p>action</p><ul><li>reduce：对数据进行聚集</li><li>reduceBykey：对key值相同的进行操作</li><li>collect：没有效果的action，但是很有用</li><li>saveAstextFile：数据存入文件系统</li><li>foreach：对每个元素进行func的操作</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;h4 id=&quot;1-RDD是什么&quot;&gt;&lt;a href=&quot;#1-RDD是什么&quot; class=&quot;headerlink&quot; title=&quot;1.RDD是什么&quot;&gt;&lt;/a&gt;1.RDD是什么&lt;/h4&gt;&lt;p&gt;   Resilient Distributed Dataset（弹性分布式数据集），是一个能够并行操作不可变的分区元素的集合&lt;/p&gt;
&lt;h4 id=&quot;2-RDD五大特性&quot;&gt;&lt;a href=&quot;#2-RDD五大特性&quot; class=&quot;headerlink&quot; title=&quot;2.RDD五大特性&quot;&gt;&lt;/a&gt;2.RDD五大特性&lt;/h4&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>美味不用等大数据面试题(201804月)</title>
    <link href="http://yoursite.com/2018/05/20/%E7%BE%8E%E5%91%B3%E4%B8%8D%E7%94%A8%E7%AD%89%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98(201804%E6%9C%88)/"/>
    <id>http://yoursite.com/2018/05/20/美味不用等大数据面试题(201804月)/</id>
    <published>2018-05-19T16:00:00.000Z</published>
    <updated>2019-04-24T12:40:46.962Z</updated>
    
    <content type="html"><![CDATA[<h6 id="1-若泽大数据线下班，某某某的小伙伴现场面试题截图"><a href="#1-若泽大数据线下班，某某某的小伙伴现场面试题截图" class="headerlink" title="1.若泽大数据线下班，某某某的小伙伴现场面试题截图:"></a><strong>1.若泽大数据线下班，某某某的小伙伴现场面试题截图:</strong></h6><a id="more"></a> <p><img src="/assets/blogImg/520_1.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_2.png" alt="enter description here"></p><h6 id="2-分享另外1家的忘记名字公司的大数据面试题："><a href="#2-分享另外1家的忘记名字公司的大数据面试题：" class="headerlink" title="2.分享另外1家的忘记名字公司的大数据面试题："></a><strong>2.分享另外1家的忘记名字公司的大数据面试题：</strong></h6><p><img src="/assets/blogImg/520_3.png" alt="enter description here"></p><p><img src="/assets/blogImg/520_4.png" alt="enter description here"></p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;1-若泽大数据线下班，某某某的小伙伴现场面试题截图&quot;&gt;&lt;a href=&quot;#1-若泽大数据线下班，某某某的小伙伴现场面试题截图&quot; class=&quot;headerlink&quot; title=&quot;1.若泽大数据线下班，某某某的小伙伴现场面试题截图:&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.若泽大数据线下班，某某某的小伙伴现场面试题截图:&lt;/strong&gt;&lt;/h6&gt;
    
    </summary>
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="大数据面试题" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD、DataFrame和DataSet的区别</title>
    <link href="http://yoursite.com/2018/05/19/Spark%20RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2018/05/19/Spark RDD、DataFrame和DataSet的区别/</id>
    <published>2018-05-18T16:00:00.000Z</published>
    <updated>2019-04-24T12:28:20.832Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br>在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！</font><h5 id="一-、共性"><a href="#一-、共性" class="headerlink" title="一 、共性"></a>一 、共性</h5><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</p><p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</p><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>4、三者都有partition的概念。<br><a id="more"></a> </p><h5 id="二、RDD优缺点"><a href="#二、RDD优缺点" class="headerlink" title="二、RDD优缺点"></a>二、RDD优缺点</h5><p><strong>优点：</strong> </p><ul><li><p>1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</p></li><li><p>2、面向对象的编程风格</p></li><li><p>3、编译时类型安全，编译时就能检查出类型错误</p></li></ul><p><strong>缺点：</strong> </p><ul><li><p>1、序列化和反序列化的性能开销</p></li><li><p>2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p></li></ul><h5 id="三、DataFrame"><a href="#三、DataFrame" class="headerlink" title="三、DataFrame"></a>三、DataFrame</h5><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.foreach&#123;</span><br><span class="line">  x =&gt;</span><br><span class="line">    val v1=x.getAs[String](&quot;v1&quot;)</span><br><span class="line">    val v2=x.getAs[String](&quot;v2&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2、DataFrame引入了schema和off-heap</p><ul><li><p>schema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.</p></li><li><p>off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.</p></li><li><p>off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.</p></li></ul><p>3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表 </p><p>4、兼容Hive，支持Hql、UDF</p><p><strong>有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.</strong></p><h5 id="四、DataSet"><a href="#四、DataSet" class="headerlink" title="四、DataSet"></a>四、DataSet</h5><p>1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。</p><p>2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。</p><p>3、Dataset<row>等同于DataFrame（Spark 2.X）</row></p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;在Spark中，RDD、DataFrame、Dataset是最常用的数据类型，今天谈谈他们的区别！&lt;/font&gt;

&lt;h5 id=&quot;一-、共性&quot;&gt;&lt;a href=&quot;#一-、共性&quot; class=&quot;headerlink&quot; title=&quot;一 、共性&quot;&gt;&lt;/a&gt;一 、共性&lt;/h5&gt;&lt;p&gt;1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利&lt;/p&gt;
&lt;p&gt;2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。&lt;/p&gt;
&lt;p&gt;3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出&lt;/p&gt;
&lt;p&gt;4、三者都有partition的概念。&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决</title>
    <link href="http://yoursite.com/2018/05/14/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%BA%90%E5%90%8C%E6%AD%A5%E4%B8%AD%E9%97%B4%E4%BB%B6--%E7%94%9F%E4%BA%A7%E4%B8%8ACanal%E4%B8%8EMaxwell%E9%A2%A0%E5%B3%B0%E5%AF%B9%E5%86%B3/"/>
    <id>http://yoursite.com/2018/05/14/大数据之实时数据源同步中间件--生产上Canal与Maxwell颠峰对决/</id>
    <published>2018-05-13T16:00:00.000Z</published>
    <updated>2019-04-24T09:54:20.012Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一-数据源同步中间件："><a href="#一-数据源同步中间件：" class="headerlink" title="一.数据源同步中间件："></a>一.数据源同步中间件：</h5><p>Canal<br><a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">https://github.com/alibaba/canal</a><br><a href="https://github.com/Hackeruncle/syncClient" target="_blank" rel="noopener">https://github.com/Hackeruncle/syncClient</a></p><p>Maxwell<br><a href="https://github.com/zendesk/maxwell" target="_blank" rel="noopener">https://github.com/zendesk/maxwell</a><br><img src="/assets/blogImg/514_1.png" alt="maxwell"><br><a id="more"></a> </p><h5 id="二-架构使用"><a href="#二-架构使用" class="headerlink" title="二.架构使用"></a>二.架构使用</h5><p>MySQL —-  中间件 mcp —&gt;KAFKA—&gt;?—&gt;存储HBASE/KUDU/Cassandra  增量的<br>a.全量  bootstrap<br>b.增量  </p><h6 id="1-对比"><a href="#1-对比" class="headerlink" title="1.对比"></a>1.对比</h6><table><thead><tr><th></th><th></th><th>Canal(服务端)</th><th>Maxwell(服务端+客户端) </th></tr></thead><tbody><tr><td>语言</td><td>Java</td><td>Java</td><td></td></tr><tr><td>活跃度</td><td>活跃</td><td>活跃</td><td></td></tr><tr><td>HA</td><td>支持</td><td>定制  但是支持断点还原功能    </td></tr><tr><td>数据落地</td><td>定制</td><td>落地到kafka    </td></tr><tr><td>分区</td><td>支持</td><td>支持    </td></tr><tr><td>bootstrap(引导)</td><td>不支持</td><td>支持    </td></tr><tr><td>数据格式</td><td>格式自由</td><td>json(格式固定)    spark json–&gt;DF</td></tr><tr><td>文档</td><td>较详细</td><td>较详细</td><td></td></tr><tr><td>随机读</td><td>支持</td><td>支持</td><td></td></tr></tbody></table><p><strong>个人选择Maxwell</strong></p><p>a.服务端+客户端一体，轻量级的<br>b.支持断点还原功能+bootstrap+json<br>Can do SELECT * from table (bootstrapping) initial loads of a table.<br>supports automatic position recover on master promotion<br>flexible partitioning schemes for Kakfa - by database, table, primary key, or column<br>Maxwell pulls all this off by acting as a full mysql replica, including a SQL parser for create/alter/drop statements (nope, there was no other way).</p><h6 id="2-官网解读"><a href="#2-官网解读" class="headerlink" title="2.官网解读"></a>2.官网解读</h6><p><a href="https://www.bilibili.com/video/av34778187?from=search&amp;seid=18393822973469412185" target="_blank" rel="noopener">B站视频</a></p><h6 id="3-部署"><a href="#3-部署" class="headerlink" title="3.部署"></a>3.部署</h6><p><strong>3.1 MySQL Install</strong><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a><br><a href="https://ke.qq.com/course/262452?tuin=11cffd50" target="_blank" rel="noopener">https://ke.qq.com/course/262452?tuin=11cffd50</a></p><p><strong>3.2 修改</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">binlog_format=row</span><br><span class="line"></span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line">3.3 创建Maxwell的db和用户</span><br><span class="line">mysql&gt; create database maxwell;</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL on maxwell.* to &apos;maxwell&apos;@&apos;%&apos; identified by &apos;ruozedata&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to &apos;maxwell&apos;@&apos;%&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></p><p><strong>3.4解压</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 software]# tar -xzvf maxwell-1.14.4.tar.gz</span><br></pre></td></tr></table></figure></p><p><strong>3.5测试STDOUT:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --user=&apos;maxwell&apos; \</span><br><span class="line">--password=&apos;ruozedata&apos; --host=&apos;127.0.0.1&apos; \</span><br><span class="line">--producer=stdout</span><br></pre></td></tr></table></figure></p><p>测试1：insert sql：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into ruozedata(id,name,age,address) values(999,&apos;jepson&apos;,18,&apos;www.ruozedata.com&apos;);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br></pre></td></tr></table></figure></p><p>maxwell输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;insert&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959044,</span><br><span class="line">    &quot;xid&quot;: 201,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试1：update sql:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update ruozedata set age=29 where id=999;</span><br></pre></td></tr></table></figure></p><p><strong>问题:  ROW，你觉得binlog更新几个字段？</strong></p><p>maxwell输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;database&quot;: &quot;ruozedb&quot;,</span><br><span class="line">    &quot;table&quot;: &quot;ruozedata&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;update&quot;,</span><br><span class="line">    &quot;ts&quot;: 1525959208,</span><br><span class="line">    &quot;xid&quot;: 255,</span><br><span class="line">    &quot;commit&quot;: true,</span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;id&quot;: 999,</span><br><span class="line">        &quot;name&quot;: &quot;jepson&quot;,</span><br><span class="line">        &quot;age&quot;: 29,</span><br><span class="line">        &quot;address&quot;: &quot;www.ruozedata.com&quot;,</span><br><span class="line">        &quot;createtime&quot;: &quot;2018-05-10 13:30:44&quot;,</span><br><span class="line">        &quot;creuser&quot;: null,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:33:28&quot;,</span><br><span class="line">        &quot;updateuser&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;old&quot;: &#123;</span><br><span class="line">        &quot;age&quot;: 18,</span><br><span class="line">        &quot;updatetime&quot;: &quot;2018-05-10 13:30:44&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h6 id="4-其他注意点和新特性"><a href="#4-其他注意点和新特性" class="headerlink" title="4.其他注意点和新特性"></a>4.其他注意点和新特性</h6><p><strong>4.1 kafka_version 版本</strong><br>Using kafka version: 0.11.0.1  0.10<br>jar:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop000 kafka-clients]# ll</span><br><span class="line">total 4000</span><br><span class="line">-rw-r--r--. 1 ruoze games  746207 May  8 06:34 kafka-clients-0.10.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  951041 May  8 06:35 kafka-clients-0.10.2.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games 1419544 May  8 06:35 kafka-clients-0.11.0.1.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  324016 May  8 06:34 kafka-clients-0.8.2.2.jar</span><br><span class="line">-rw-r--r--. 1 ruoze games  641408 May  8 06:34 kafka-clients-0.9.0.1.jar</span><br><span class="line">[root@hadoop000 kafka-clients]#</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一-数据源同步中间件：&quot;&gt;&lt;a href=&quot;#一-数据源同步中间件：&quot; class=&quot;headerlink&quot; title=&quot;一.数据源同步中间件：&quot;&gt;&lt;/a&gt;一.数据源同步中间件：&lt;/h5&gt;&lt;p&gt;Canal&lt;br&gt;&lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/alibaba/canal&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/Hackeruncle/syncClient&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Hackeruncle/syncClient&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maxwell&lt;br&gt;&lt;a href=&quot;https://github.com/zendesk/maxwell&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/zendesk/maxwell&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;/assets/blogImg/514_1.png&quot; alt=&quot;maxwell&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="其他组件" scheme="http://yoursite.com/categories/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/"/>
    
    
      <category term="maxwell" scheme="http://yoursite.com/tags/maxwell/"/>
    
  </entry>
  
  <entry>
    <title>Spark on YARN-Cluster和YARN-Client的区别</title>
    <link href="http://yoursite.com/2018/05/12/Spark%20on%20YARN-Cluster%E5%92%8CYARN-Client%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2018/05/12/Spark on YARN-Cluster和YARN-Client的区别/</id>
    <published>2018-05-11T16:00:00.000Z</published>
    <updated>2019-05-05T12:33:17.212Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一-YARN-Cluster和YARN-Client的区别"><a href="#一-YARN-Cluster和YARN-Client的区别" class="headerlink" title="一. YARN-Cluster和YARN-Client的区别"></a>一. YARN-Cluster和YARN-Client的区别</h5><p><img src="/assets/blogImg/512_1.png" alt><br>（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；<br>（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；<br>（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。<br><a id="more"></a> </p><h5 id="二-yarn-client-模式"><a href="#二-yarn-client-模式" class="headerlink" title="二. yarn client 模式"></a>二. yarn client 模式</h5><p><img src="/assets/blogImg/512_2.png" alt></p><p><font color="#FF4200">yarn-client  模式的话 ，把 客户端关掉的话 ，是不能提交任务的 。<br></font></p><h5 id="三-yarn-cluster-模式"><a href="#三-yarn-cluster-模式" class="headerlink" title="三.yarn  cluster 模式"></a>三.yarn  cluster 模式</h5><p><img src="/assets/blogImg/512_3.png" alt></p><p><font color="#FF4200">yarn-cluster 模式的话， client 关闭是可以提交任务的 ，<br></font></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p><strong>1.spark-shell/spark-sql 只支持 yarn-client模式；<br>2.spark-submit对于两种模式都支持。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一-YARN-Cluster和YARN-Client的区别&quot;&gt;&lt;a href=&quot;#一-YARN-Cluster和YARN-Client的区别&quot; class=&quot;headerlink&quot; title=&quot;一. YARN-Cluster和YARN-Client的区别&quot;&gt;&lt;/a&gt;一. YARN-Cluster和YARN-Client的区别&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;/assets/blogImg/512_1.png&quot; alt&gt;&lt;br&gt;（1）SparkContext初始化不同，这也导致了Driver所在位置的不同，YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在driver所在的机器上；&lt;br&gt;（2）而Driver会和Executors进行通信，这也导致了Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以；&lt;br&gt;（3）最后再来说应用场景，Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>生产改造Spark1.6源代码，create table语法支持Oracle列表分区</title>
    <link href="http://yoursite.com/2018/05/08/%E7%94%9F%E4%BA%A7%E6%94%B9%E9%80%A0Spark1.6%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8Ccreate%20table%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81Oracle%E5%88%97%E8%A1%A8%E5%88%86%E5%8C%BA/"/>
    <id>http://yoursite.com/2018/05/08/生产改造Spark1.6源代码，create table语法支持Oracle列表分区/</id>
    <published>2018-05-07T16:00:00.000Z</published>
    <updated>2019-04-24T10:36:40.482Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><h5 id="1-需求"><a href="#1-需求" class="headerlink" title="1.需求"></a>1.需求</h5><p>通过Spark SQL JDBC 方法，抽取Oracle表数据。</p><h5 id="2-问题"><a href="#2-问题" class="headerlink" title="2.问题"></a>2.问题</h5><p>大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。<br>参考 <a href="http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases" target="_blank" rel="noopener">http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases</a><br><a id="more"></a> </p><h5 id="3-Oracle的分区"><a href="#3-Oracle的分区" class="headerlink" title="3.Oracle的分区"></a>3.Oracle的分区</h5><h6 id="3-1列表分区"><a href="#3-1列表分区" class="headerlink" title="3.1列表分区:"></a>3.1列表分区:</h6><p>该分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。<br>例一:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PROBLEM_TICKETS</span><br><span class="line">(</span><br><span class="line">PROBLEM_ID NUMBER(7) NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR2(2000),</span><br><span class="line">CUSTOMER_ID NUMBER(7) NOT NULL, DATE_ENTERED DATE NOT NULL,</span><br><span class="line">STATUS VARCHAR2(20)</span><br><span class="line">)</span><br><span class="line">PARTITION BY LIST (STATUS)</span><br><span class="line">(</span><br><span class="line">PARTITION PROB_ACTIVE VALUES (&apos;ACTIVE&apos;) TABLESPACE PROB_TS01,</span><br><span class="line">PARTITION PROB_INACTIVE VALUES (&apos;INACTIVE&apos;) TABLESPACE PROB_TS02</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h6 id="3-2散列分区"><a href="#3-2散列分区" class="headerlink" title="3.2散列分区:"></a>3.2散列分区:</h6><p>这类分区是在列值上使用散列算法，以确定将行放入哪个分区中。当列的值没有合适的条件时，建议使用散列分区。 散列分区为通过指定分区编号来均匀分布数据的一种分区类型，因为通过在I/O设备上进行散列分区，使得这些分区大小一致。<br>例一:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE HASH_TABLE</span><br><span class="line">(</span><br><span class="line">COL NUMBER(8),</span><br><span class="line">INF VARCHAR2(100) </span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (COL)</span><br><span class="line">(</span><br><span class="line">PARTITION PART01 TABLESPACE HASH_TS01, </span><br><span class="line">PARTITION PART02 TABLESPACE HASH_TS02, </span><br><span class="line">PARTITION PART03 TABLESPACE HASH_TS03</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h5 id="4-改造"><a href="#4-改造" class="headerlink" title="4.改造"></a>4.改造</h5><p>蓝色代码是改造Spark源代码,加课程顾问领取PDF。</p><h6 id="1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。"><a href="#1-Spark-SQL-JDBC的建表脚本中需要加入列表分区配置项。" class="headerlink" title="1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。"></a>1) Spark SQL JDBC的建表脚本中需要加入列表分区配置项。</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE TBLS_IN</span><br><span class="line">USING org.apache.spark.sql.jdbc OPTIONS (</span><br><span class="line">driver &quot;com.mysql.jdbc.Driver&quot;,</span><br><span class="line">url &quot;jdbc:mysql://spark1:3306/hivemetastore&quot;, dbtable &quot;TBLS&quot;,</span><br><span class="line">fetchSize &quot;1000&quot;,</span><br><span class="line">partitionColumn &quot;TBL_ID&quot;,</span><br><span class="line">numPartitions &quot;null&quot;,</span><br><span class="line">lowerBound &quot;null&quot;,</span><br><span class="line">upperBound &quot;null&quot;,</span><br><span class="line">user &quot;hive2user&quot;,</span><br><span class="line">password &quot;hive2user&quot;,</span><br><span class="line">partitionInRule &quot;1|15,16,18,19|20,21&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation"><a href="#2-程序入口org-apache-spark-sql-execution-datasources-jdbc-DefaultSource，方法createRelation" class="headerlink" title="2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation"></a>2)程序入口org.apache.spark.sql.execution.datasources.jdbc.DefaultSource，方法createRelation</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">sqlContext: SQLContext,</span><br><span class="line">parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">val url = parameters.getOrElse(&quot;url&quot;, sys.error(&quot;Option &apos;url&apos; not specified&quot;))</span><br><span class="line">val table = parameters.getOrElse(&quot;dbtable&quot;, sys.error(&quot;Option &apos;dbtable&apos; not specified&quot;)) val partitionColumn = parameters.getOrElse(&quot;partitionColumn&quot;, null)</span><br><span class="line">var lowerBound = parameters.getOrElse(&quot;lowerBound&quot;, null)</span><br><span class="line">var upperBound = parameters.getOrElse(&quot;upperBound&quot;, null) var numPartitions = parameters.getOrElse(&quot;numPartitions&quot;, null)</span><br><span class="line"></span><br><span class="line">// add partition in rule</span><br><span class="line">val partitionInRule = parameters.getOrElse(&quot;partitionInRule&quot;, null)</span><br><span class="line">// validind all the partition in rule </span><br><span class="line">if (partitionColumn != null</span><br><span class="line">&amp;&amp; (lowerBound == null || upperBound == null || numPartitions == null)</span><br><span class="line">&amp;&amp; partitionInRule == null </span><br><span class="line">)&#123;</span><br><span class="line">   sys.error(&quot;Partitioning incompletely specified&quot;) </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val partitionInfo = </span><br><span class="line">if (partitionColumn == null) &#123; </span><br><span class="line">    null</span><br><span class="line">&#125; else &#123;</span><br><span class="line"></span><br><span class="line">val inPartitions = if(&quot;null&quot;.equals(numPartitions))&#123;</span><br><span class="line">val inGroups = partitionInRule.split(&quot;\\|&quot;) numPartitions = inGroups.length.toString lowerBound = &quot;0&quot;</span><br><span class="line">upperBound = &quot;0&quot;</span><br><span class="line">inGroups &#125;</span><br><span class="line">else&#123;</span><br><span class="line">Array[String]() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JDBCPartitioningInfo( partitionColumn, </span><br><span class="line">lowerBound.toLong, </span><br><span class="line">upperBound.toLong, </span><br><span class="line">numPartitions.toInt, </span><br><span class="line">inPartitions)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val parts = JDBCRelation.columnPartition(partitionInfo)</span><br><span class="line">val properties = new Properties() // Additional properties that we will pass to getConnection parameters.foreach(kv =&gt; properties.setProperty(kv._1, kv._2))</span><br><span class="line">// parameters is immutable</span><br><span class="line">if(numPartitions != null)&#123;</span><br><span class="line">properties.put(&quot;numPartitions&quot; , numPartitions) &#125;</span><br><span class="line">JDBCRelation(url, table, parts, properties)(sqlContext)</span><br><span class="line"></span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition"><a href="#3-org-apache-spark-sql-execution-datasources-jdbc-JDBCRelation，方法columnPartition" class="headerlink" title="3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition"></a>3)org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation，方法columnPartition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def columnPartition(partitioning: JDBCPartitioningInfo): Array[Partition] = &#123;</span><br><span class="line">if (partitioning == null) return Array[Partition](JDBCPartition(null, 0))</span><br><span class="line">val column = partitioning.column</span><br><span class="line">var i: Int = 0</span><br><span class="line">var ans = new ArrayBuffer[Partition]()</span><br><span class="line"></span><br><span class="line">// partition by long if(partitioning.inPartitions.length == 0)&#123;</span><br><span class="line"></span><br><span class="line">val numPartitions = partitioning.numPartitions</span><br><span class="line">if (numPartitions == 1) return Array[Partition](JDBCPartition(null, 0)) // Overflow and silliness can happen if you subtract then divide.</span><br><span class="line">// Here we get a little roundoff, but that&apos;s (hopefully) OK.</span><br><span class="line">val stride: Long = (partitioning.upperBound / numPartitions</span><br><span class="line"></span><br><span class="line">- partitioning.lowerBound / numPartitions)</span><br><span class="line">var currentValue: Long = partitioning.lowerBound</span><br><span class="line">while (i &lt; numPartitions) &#123;</span><br><span class="line">val lowerBound = if (i != 0) s&quot;$column &gt;= $currentValue&quot; else null</span><br><span class="line">currentValue += stride</span><br><span class="line">val upperBound = if (i != numPartitions - 1) s&quot;$column &lt; $currentValue&quot; else null val whereClause =</span><br><span class="line"></span><br><span class="line">if (upperBound == null) &#123; </span><br><span class="line">  lowerBound</span><br><span class="line"></span><br><span class="line">&#125; else if (lowerBound == null) &#123; </span><br><span class="line">  upperBound</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">  s&quot;$lowerBound AND $upperBound&quot; </span><br><span class="line">&#125;</span><br><span class="line">  ans += JDBCPartition(whereClause, i)</span><br><span class="line">   i= i+ 1 &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// partition by in </span><br><span class="line">else&#123;</span><br><span class="line">    while(i &lt; partitioning.inPartitions.length)&#123;</span><br><span class="line">           val inContent = partitioning.inPartitions(i)</span><br><span class="line">           val whereClause = s&quot;$column in ($inContent)&quot; </span><br><span class="line">           ans += JDBCPartition(whereClause, i)</span><br><span class="line">           i= i+ 1</span><br><span class="line">     &#125; </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ans.toArray </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="4-对外方法org-apache-spark-sql-SQLContext-方法jdbc"><a href="#4-对外方法org-apache-spark-sql-SQLContext-方法jdbc" class="headerlink" title="4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc"></a>4)对外方法org.apache.spark.sql.SQLContext , 方法jdbc</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">url: String,</span><br><span class="line">table: String,</span><br><span class="line">columnName: String,</span><br><span class="line">lowerBound: Long,</span><br><span class="line">upperBound: Long,</span><br><span class="line">numPartitions: Int,</span><br><span class="line">inPartitions: Array[String] = Array[String]()</span><br><span class="line"></span><br><span class="line">): DataFrame = &#123;</span><br><span class="line">read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, inPartitions ,new Properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;h5 id=&quot;1-需求&quot;&gt;&lt;a href=&quot;#1-需求&quot; class=&quot;headerlink&quot; title=&quot;1.需求&quot;&gt;&lt;/a&gt;1.需求&lt;/h5&gt;&lt;p&gt;通过Spark SQL JDBC 方法，抽取Oracle表数据。&lt;/p&gt;
&lt;h5 id=&quot;2-问题&quot;&gt;&lt;a href=&quot;#2-问题&quot; class=&quot;headerlink&quot; title=&quot;2.问题&quot;&gt;&lt;/a&gt;2.问题&lt;/h5&gt;&lt;p&gt;大数据开发人员反映，使用效果上列表分区优于散列分区。但Spark SQL JDBC方法只支持数字类型分区，而业务表的列表分区字段是个字符串。目前Oracle表使用列表分区，对省级代码分 区。&lt;br&gt;参考 &lt;a href=&quot;http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://spark.apache.org/docs/1.6.2/sql-programming-guide.html#jdbc-to-other-databases&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="源码阅读" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>生产中Hive静态和动态分区表，该怎样抉择呢？</title>
    <link href="http://yoursite.com/2018/05/06/%E7%94%9F%E4%BA%A7%E4%B8%ADHive%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%8C%E8%AF%A5%E6%80%8E%E6%A0%B7%E6%8A%89%E6%8B%A9%E5%91%A2%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/05/06/生产中Hive静态和动态分区表，该怎样抉择呢？/</id>
    <published>2018-05-05T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:24.149Z</updated>
    
    <content type="html"><![CDATA[<h6 id="一-需求"><a href="#一-需求" class="headerlink" title="一.需求"></a>一.需求</h6><p>按照不同部门作为分区，导数据到目标表</p><h6 id="二-使用静态分区表来完成"><a href="#二-使用静态分区表来完成" class="headerlink" title="二.使用静态分区表来完成"></a>二.使用静态分区表来完成</h6><p>71.创建静态分区表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table emp_static_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure></p><p>2.插入数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_static_partition partition(deptno=10)</span><br><span class="line">     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;</span><br></pre></td></tr></table></figure></p><a id="more"></a> <p>3.查询数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_static_partition;</span><br></pre></td></tr></table></figure></p><p><img src="/assets/blogImg/0506_1.png" alt></p><h6 id="三-使用动态分区表来完成"><a href="#三-使用动态分区表来完成" class="headerlink" title="三.使用动态分区表来完成"></a>三.使用动态分区表来完成</h6><p>1.创建动态分区表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table emp_dynamic_partition(</span><br><span class="line">empno int, </span><br><span class="line">ename string, </span><br><span class="line">job string, </span><br><span class="line">mgr int, </span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double)</span><br><span class="line">PARTITIONED BY(deptno int)row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure></p><font color="#FF4500">【注意】动态分区表与静态分区表的创建，在语法上是没有任何区别的</font><p>2.插入数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;insert into table emp_dynamic_partition partition(deptno)     </span><br><span class="line">select empno , ename , job , mgr , hiredate , sal , comm, deptno from emp;</span><br></pre></td></tr></table></figure></p><font color="#FF4500">【注意】分区的字段名称，写在最后，有几个就写几个 与静态分区相比，不需要where</font><p>需要设置属性的值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nonstrict；</span><br></pre></td></tr></table></figure></p><p>假如不设置，报错如下:<br><img src="/assets/blogImg/0506_2.png" alt><br>3.查询数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from emp_dynamic_partition;</span><br></pre></td></tr></table></figure></p><p><img src="/assets/blogImg/0506_3.png" alt></p><p><font color="#FF4500">分区列为deptno，实现了动态分区</font></p><h6 id="四-总结"><a href="#四-总结" class="headerlink" title="四.总结"></a>四.总结</h6><p>在生产上我们更倾向是选择<strong>动态分区</strong>，<br>无需手工指定数据导入的具体分区，<br>而是由select的字段(字段写在最后，有几个写几个)自行决定导出到哪一个分区中， 并自动创建相应的分区，使用上更加方便快捷 ，在生产工作中用的非常多多。</p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;一-需求&quot;&gt;&lt;a href=&quot;#一-需求&quot; class=&quot;headerlink&quot; title=&quot;一.需求&quot;&gt;&lt;/a&gt;一.需求&lt;/h6&gt;&lt;p&gt;按照不同部门作为分区，导数据到目标表&lt;/p&gt;
&lt;h6 id=&quot;二-使用静态分区表来完成&quot;&gt;&lt;a href=&quot;#二-使用静态分区表来完成&quot; class=&quot;headerlink&quot; title=&quot;二.使用静态分区表来完成&quot;&gt;&lt;/a&gt;二.使用静态分区表来完成&lt;/h6&gt;&lt;p&gt;71.创建静态分区表：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;create table emp_static_partition(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;empno int, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ename string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;job string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mgr int, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hiredate string, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sal double, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;comm double)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARTITIONED BY(deptno int)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;row format delimited fields terminated by &amp;apos;\t&amp;apos;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.插入数据：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hive&amp;gt;insert into table emp_static_partition partition(deptno=10)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     select empno , ename , job , mgr , hiredate , sal , comm from emp where deptno=10;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用</title>
    <link href="http://yoursite.com/2018/05/04/5min%E6%8E%8C%E6%8F%A1%EF%BC%8CHive%E7%9A%84HiveServer2%20%E5%92%8CJDBC%E5%AE%A2%E6%88%B7%E7%AB%AF&amp;%E4%BB%A3%E7%A0%81%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/05/04/5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用/</id>
    <published>2018-05-03T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:29.426Z</updated>
    
    <content type="html"><![CDATA[<font color="#FF4500"><br></font><p><img src="/assets/blogImg/504_1.png" alt><br><a id="more"></a> </p><h6 id="1-介绍："><a href="#1-介绍：" class="headerlink" title="1. 介绍："></a>1. 介绍：</h6><p>两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，<br>客户端可以在不启动CLI的情况下对Hive中的数据进行操作，<br>两者都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果<br>（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下HiveServer。</p><p>HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，<br>而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？<br>这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，<br>不能通过修改HiveServer的代码修正。</p><p>因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。<br>HiveServer2支持多客户端的并发和认证，为开放API客户端如采用jdbc、odbc、beeline的方式进行连接。</p><h6 id="2-配置参数"><a href="#2-配置参数" class="headerlink" title="2.配置参数"></a>2.配置参数</h6><p>Hiveserver2允许在配置文件hive-site.xml中进行配置管理，具体的参数为：<br>参数 | 含义 |<br>-|-|<br>hive.server2.thrift.min.worker.threads|  最小工作线程数，默认为5。<br>hive.server2.thrift.max.worker.threads|  最小工作线程数，默认为500。<br>hive.server2.thrift.port|  TCP 的监听端口，默认为10000。<br>hive.server2.thrift.bind.host|  TCP绑定的主机，默认为localhost </p><p>配置监听端口和路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;192.168.48.130&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p><h6 id="3-启动hiveserver2"><a href="#3-启动hiveserver2" class="headerlink" title="3. 启动hiveserver2"></a>3. 启动hiveserver2</h6><p>使用hadoop用户启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd /opt/software/hive/bin/</span><br><span class="line">[hadoop@hadoop001 bin]$ hiveserver2 </span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br></pre></td></tr></table></figure></p><h6 id="4-重新开个窗口，使用beeline方式连接"><a href="#4-重新开个窗口，使用beeline方式连接" class="headerlink" title="4. 重新开个窗口，使用beeline方式连接"></a>4. 重新开个窗口，使用beeline方式连接</h6><ul><li>-n 指定机器登陆的名字，当前机器的登陆用户名</li><li>-u 指定一个连接串</li><li>每成功运行一个命令，hiveserver2启动的那个窗口，只要在启动beeline的窗口中执行成功一条命令，另外个窗口随即打印一个OK</li><li>如果命令错误，hiveserver2那个窗口就会抛出异常</li></ul><p>使用hadoop用户启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 bin]$ ./beeline -u jdbc:hive2://localhost:10000/default -n hadoop</span><br><span class="line">which: no hbase in (/opt/software/hive/bin:/opt/software/hadoop/sbin:/opt/software/hadoop/bin:/opt/software/apache-maven-3.3.9/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin)</span><br><span class="line">scan complete in 4ms</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.7.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.7.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.7.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt;</span><br></pre></td></tr></table></figure></p><p>使用SQL<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show databases;</span><br><span class="line">INFO  : Compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.478 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</span><br><span class="line">INFO  : Completed executing command(queryId=hadoop_20180114082525_e8541a4a-e849-4017-9dab-ad5162fa74c1); Time taken: 0.135 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">1 row selected</span><br></pre></td></tr></table></figure></p><h6 id="5-使用编写java代码方式连接"><a href="#5-使用编写java代码方式连接" class="headerlink" title="5.使用编写java代码方式连接"></a>5.使用编写java代码方式连接</h6><p><strong>5.1</strong>使用maven构建项目，pom.xml文件如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.zhaotao.bigdata&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-train&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive-train&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;    </span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p><strong>5.2</strong>JdbcApp.java文件代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class JdbcApp &#123;</span><br><span class="line">     private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">         &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">             // TODO Auto-generated catch block</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">             System.exit(1);</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         Connection con = DriverManager.getConnection(&quot;jdbc:hive2://192.168.137.200:10000/default&quot;, &quot;root&quot;, &quot;&quot;);</span><br><span class="line">         Statement stmt = con.createStatement();</span><br><span class="line">         //select table:ename</span><br><span class="line">         String tableName = &quot;emp&quot;;</span><br><span class="line">         String sql = &quot;select ename from &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">          while(res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1));</span><br><span class="line">         &#125;</span><br><span class="line">         // describe table</span><br><span class="line">         sql = &quot;describe &quot; + tableName;</span><br><span class="line">         System.out.println(&quot;Running: &quot; + sql);</span><br><span class="line">         res = stmt.executeQuery(sql);</span><br><span class="line">         while (res.next()) &#123;</span><br><span class="line">             System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;/assets/blogImg/504_1.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>谈谈我和大数据的情缘及入门</title>
    <link href="http://yoursite.com/2018/05/01/%E8%B0%88%E8%B0%88%E6%88%91%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%83%85%E7%BC%98%E5%8F%8A%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/05/01/谈谈我和大数据的情缘及入门/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2019-04-24T10:33:17.752Z</updated>
    
    <content type="html"><![CDATA[<p> &#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。</p><p> &#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。<br> <a id="more"></a><br> &#8195;后来看了很多博文，都是Apache Hadoop版本搭建，于是我先试试用Apache Hadoop搭建部署单节点和集群，然后配置HA，最后我发现自己比较喜欢这种方式，因为我能了解其配置参数，配置文件和常规命令等等，再回头去对比CDH安装HDFS服务，真是太爽了，因为Apache Hadoop版本有真正体验感，这时我就迅速调整方向 : 先Apache版本，再CDH。</p><p> &#8195;由于公司项目环境，推进自己实在太慢，于是我在网上看各种相关视频教程；加n种群，在群里潜水，看水友们提的问题自己会不会，不会就去查资料，会就帮助他们一起研究学习进步。</p><p>&#8195;<strong>后来这样的进度太慢了</strong>，因为很多群都是打广告，潜水，没有真正的技术讨论氛围，于是我迅速调整方向，自己建个QQ群，慢慢招兵买马，和管理员们一起去管理，在过去的两年里我也学到了很多知识和认识和我一样前进的小伙伴们，现在也有很多已成为friends。</p><p>&#8195;每当夜晚，我就会深深思考仅凭公司项目,网上免费课程视频，QQ群等，还是不够的，于是我开始咨询培训机构的课程，在这里提醒各位小伙伴们，报班一定要擦亮眼睛，选择老师很重要，真心很重要，许多培训机构的老师都是Java转的，讲的是全是基础，根本没有企业项目实战经验；还有不要跟风，一定看仔细看清楚课程是否符合当前的你。</p><p> &#8195;这时还是远远不够的，于是我开始每天上下班地铁上看技术博客，积极分享。<strong>然后再申请博客，写博文，写总结，坚持每次做完一次实验就将博文，梳理好，写好，这样久而久之，知识点就慢慢夯实积累了。</strong></p><p> &#8195;再着后面就开始受邀几大培训机构做公开课，再一次将知识点梳理了，也认识了新的小伙伴们，我们有着相同的方向和目标，我们尽情的讨论着大数据的知识点，慢慢朝着我们心目中的目标而努力着！</p><p><strong>以上基本就是我和大数据的情缘，下面我来谈谈我对大数据入门的感悟。</strong><br><strong>1. 心态要端正。</strong><br>既然想要从事这行，那么一定要下定决心，当然付出是肯定大大的，不光光是毛爷爷，而更多的付出是自己的那一份坚持，凡事贵在坚持，真真体现在这里。<br>后来我将我老婆从化工实验室分析员转行，做Python爬虫和数据分析，当然这个主要还是靠她的那份坚持。</p><p><strong>2. 心目中要有计划。</strong><br>先学习Linux和Shell，再学习数据库和SQL，再学习Java和Scala，<br>然后学习Apache Haoop、Hive、Kafka、Spark，朝大数据研发或开发而努力着。</p><p><strong>3. 各种方式学习。</strong><br>QQ群，博客，上下班看技术文章，选择好的老师和课程培训，</p><p><font color="#FF4500"><br>(擦亮眼睛，很多视频，很多大数据老师都是瞎扯的，最终总结一句话，不在企业上班的教大数据都是耍流氓的。)</font><br>可以加速自己前进的马拉松里程，其实一般都要看大家怎么衡量培训这个事的，time和money的抉择，以及快速jump后的高薪。</p><p><strong>4. 项目经验。</strong><br>很多小白都没有项目经验也没有面试经验和技巧，屡屡面试以失败告终，<br>这时大家可以找你们熟悉的小伙伴们的，让他给你培训他的项目，这样就有了，当然可以直接互联网搜索一个就行，不过一般很难有完整的。<br>而面试，就看看其他人面试分享，学习他人。</p><p><strong>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。<br>最后，总结一句话，坚持才是最重要的。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &amp;#8195;当年我是做C#+Java软件开发，然后考取OCP来了上海，立志要做一名DBA。只记得当年试用期刚过时，阴差阳错轮到我负责公司的大数据平台这块，刚开始很痛苦，一个陌生的行业，一个讨论的小伙伴都没有，一份现成资料都没有，心情焦虑。后来我调整心态，从DB转移到对大数据的研究，决定啃下这块硬骨头，把它嚼碎，把它消化吸收。&lt;/p&gt;
&lt;p&gt; &amp;#8195;由于当时公司都是CDH环境，刚开始安装卡了很久都过不去，后面选择在线安装，很慢，有时需要1天。后来安装HDFS ,YARN,HIVE组件，不过对它们不理解，不明白，有时很困惑。这样的过程大概持续三个月了。&lt;br&gt;
    
    </summary>
    
      <category term="感想" scheme="http://yoursite.com/categories/%E6%84%9F%E6%83%B3/"/>
    
    
      <category term="人生感悟" scheme="http://yoursite.com/tags/%E4%BA%BA%E7%94%9F%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>2min快速了解，Hive内部表和外部表</title>
    <link href="http://yoursite.com/2018/05/01/2min%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3%EF%BC%8CHive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
    <id>http://yoursite.com/2018/05/01/2min快速了解，Hive内部表和外部表/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2019-05-05T12:32:44.219Z</updated>
    
    <content type="html"><![CDATA[<p><font color="#FF4500"><br></font><br>在了解内部表和外部表区别前，<br>我们需要先了解一下<strong>Hive架构</strong> ：</p><p><img src="/assets/blogImg/501_1.png" alt="Hive架构"><br><a id="more"></a><br>大家可以简单看一下这个架构图，我介绍其中要点：<br>Hive的数据分为两种，<strong>一种为普通数据，一种为元数据。</strong></p><ol><li>元数据存储着表的基本信息，增删改查记录，类似于Hadoop架构中的namespace。普通数据就是表中的详细数据。</li><li>Hive的元数据默认存储在derby中，但大多数情况下存储在MySQL中。普通数据如架构图所示存储在hdfs中。</li></ol><p>下面我们来介绍表的两种类型：内部表和外部表</p><ol><li><p>内部表（MANAGED）：hive在hdfs中存在默认的存储路径，即default数据库。之后创建的数据库及表，如果没有指定路径应都在/user/hive/warehouse下，所以在该路径下的表为内部表。</p></li><li><p>外部表（EXTERNAL）：指定了/user/hive/warehouse以外路径所创建的表<br>而内部表和外部表的主要区别就是 </p><ul><li>内部表：当删除内部表时，MySQL的元数据和HDFS上的普通数据都会删除 ；</li><li>外部表：当删除外部表时，MySQL的元数据会被删除，HDFS上的数据不会被删除；</li></ul></li></ol><h6 id="1-准备数据-按tab键制表符作为字段分割符"><a href="#1-准备数据-按tab键制表符作为字段分割符" class="headerlink" title="1.准备数据:  按tab键制表符作为字段分割符"></a>1.准备数据:  按tab键制表符作为字段分割符</h6><pre><code>cat /tmp/ruozedata.txt1   jepson  32  1102   ruoze   22  1123   www.ruozedata.com   18  120</code></pre><h6 id="2-内部表测试："><a href="#2-内部表测试：" class="headerlink" title="2.内部表测试："></a>2.内部表测试：</h6><ol><li><p>在Hive里面创建一个表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table ruozedata(id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tele string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.759 seconds</span><br></pre></td></tr></table></figure></li><li><p>这样我们就在Hive里面创建了一张普通的表，现在给这个表导入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>内部表删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table ruozedata;</span><br></pre></td></tr></table></figure></li></ol><h6 id="3-外部表测试"><a href="#3-外部表测试" class="headerlink" title="3.外部表测试:"></a>3.外部表测试:</h6><ol><li>创建外部表多了external关键字说明以及hdfs上location ‘/hive/external’<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create external table exter_ruozedata(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string,</span><br><span class="line">    &gt; age int,</span><br><span class="line">    &gt; tel string)</span><br><span class="line">    &gt; location &apos;/hive/external&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.098 seconds</span><br></pre></td></tr></table></figure></li></ol><p>创建外部表，需要在创建表的时候加上external关键字，同时指定外部表存放数据的路径<br>（当然，你也可以不指定外部表的存放路径，这样Hive将 在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）</p><ol start="2"><li><p>外部表导入数据和内部表一样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/ruozedata.txt&apos; into table exter_ruozedata;</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table exter_ruozedata;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font color=&quot;#FF4500&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;br&gt;在了解内部表和外部表区别前，&lt;br&gt;我们需要先了解一下&lt;strong&gt;Hive架构&lt;/strong&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blogImg/501_1.png&quot; alt=&quot;Hive架构&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数(UDF)的部署使用，你会吗？</title>
    <link href="http://yoursite.com/2018/04/27/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/27/Hive自定义函数(UDF)的部署使用，你会吗？/</id>
    <published>2018-04-26T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:11.545Z</updated>
    
    <content type="html"><![CDATA[<p>Hive自定义函数(UDF)的部署使用，你会吗，三种方式！<br><a id="more"></a> </p><font color="#FF4500"><br></font><h6 id="一-临时函数"><a href="#一-临时函数" class="headerlink" title="一.临时函数"></a>一.临时函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>添加jar包<br>hive&gt;add xxx.jar jar_filepath;</li><li>查看jar包<br>hive&gt;list jars;</li><li>创建临时函数<br>hive&gt;create temporary function my_lower as ‘com.example.hive.udf.Lower’;</li></ol><h6 id="二-持久函数"><a href="#二-持久函数" class="headerlink" title="二.持久函数"></a>二.持久函数</h6><ol><li>idea编写udf</li><li>打包<br>Maven Projects —-&gt;Lifecycle —-&gt;package —-&gt; 右击 Run Maven Build</li><li>rz上传至服务器</li><li>上传到HDFS<br>$ hdfs dfs -put xxx.jar  hdfs:///path/to/xxx.jar</li><li>创建持久函数<br>hive&gt;CREATE FUNCTION myfunc AS ‘myclass’ USING JAR ‘hdfs:///path/to/xxx.jar’;</li></ol><p><strong>注意点：</strong></p><ul><li><ol><li>此方法在show functions时是看不到的，但是可以使用</li></ol></li><li><ol start="2"><li>需要上传至hdfs</li></ol></li></ul><h6 id="三-持久函数，并注册"><a href="#三-持久函数，并注册" class="headerlink" title="三.持久函数，并注册"></a>三.持久函数，并注册</h6><p>环境介绍：CentOS7+hive-1.1.0-cdh5.7.0+Maven3.3.9</p><ol><li><p>下载源码<br>hive-1.1.0-cdh5.7.0-src.tar.gz<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0-src.tar.gz</a> </p></li><li><p>解压源码<br>tar -zxvf hive-1.1.0-cdh5.7.0-src.tar.gz -C /home/hadoop/<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0</p></li><li><p>将HelloUDF.java文件增加到HIVE源码中<br>cp HelloUDF.java /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/udf/</p></li><li><p>修改FunctionRegistry.java 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/ql/src/java/org/apache/hadoop/hive/ql/exec/</span><br><span class="line">vi FunctionRegistry.java</span><br><span class="line">在import中增加：import org.apache.hadoop.hive.ql.udf.HelloUDF;</span><br><span class="line">在文件头部 static 块中添加：system.registerUDF(&quot;helloUDF&quot;, HelloUDF.class, false);</span><br></pre></td></tr></table></figure></li><li><p>重新编译<br>cd /home/hadoop/hive-1.1.0-cdh5.7.0<br>mvn clean package -DskipTests -Phadoop-2 -Pdist</p></li><li><p>编译结果全部为：BUILD SUCCESS<br>文件所在目录：/home/hadoop/hive-1.1.0-cdh5.7.0/hive-1.1.0-cdh5.7.0/packaging/target</p></li><li><p>配置hive环境<br>配置hive环境时，可以全新配置或将编译后带UDF函数的包复制到旧hive环境中：<br>7.1. 全部配置：参照之前文档  <a href="https://ruozedata.github.io/2018/04/11/Hive%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">Hive全网最详细的编译及部署</a></p><p>7.2. 将编译后带UDF函数的包复制到旧hive环境<br>   到/home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib下，找到hive-exec-1.1.0-cdh5.7.0.jar包，并将旧环境中对照的包替换掉<br>   命令：</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br><span class="line">mv hive-exec-1.1.0-cdh5.7.0.jar hive-exec-1.1.0-cdh5.7.0.jar_bak</span><br><span class="line">cd /home/hadoop/hive-1.1.0-cdh5.7.0/packaging/target/apache-hive-1.1.0-cdh5.7.0-bin/apache-hive-1.1.0-cdh5.7.0-bin/lib</span><br><span class="line">cp hive-exec-1.1.0-cdh5.7.0.jar /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib</span><br></pre></td></tr></table></figure><p>最终启动hive</p></li><li><p>测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive (default)&gt; show functions ;   -- 能查看到有 helloudf</span><br><span class="line">hive(default)&gt;select deptno,dname,helloudf(dname) from dept;   -- helloudf函数生效</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive自定义函数(UDF)的部署使用，你会吗，三种方式！&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数(UDF)的编程开发，你会吗？</title>
    <link href="http://yoursite.com/2018/04/25/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0(UDF)%E7%9A%84%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%A0%E4%BC%9A%E5%90%97%EF%BC%9F/"/>
    <id>http://yoursite.com/2018/04/25/Hive自定义函数(UDF)的编程开发，你会吗？/</id>
    <published>2018-04-24T16:00:00.000Z</published>
    <updated>2019-04-24T10:34:20.661Z</updated>
    
    <content type="html"><![CDATA[<p>本地开发环境：IntelliJ IDEA+Maven3.3.9<br><a id="more"></a> </p><h6 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h6><p>   打开IntelliJ IDEA<br>   File–&gt;New–&gt;Project…–&gt;Maven选择Create from archetye–&gt;org.apache.maven.archety:maven-archetype-quitkstart</p><h6 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h6><p> 在工程中找到pom.xml文件，添加hadoop、hive依赖<br> <img src="/assets/blogImg/425hive1.png" alt="Hive图1"></p><h6 id="3-创建类、并编写一个HelloUDF-java，代码如下："><a href="#3-创建类、并编写一个HelloUDF-java，代码如下：" class="headerlink" title="3. 创建类、并编写一个HelloUDF.java，代码如下："></a>3. 创建类、并编写一个HelloUDF.java，代码如下：</h6><p>  <img src="/assets/blogImg/425hive2.png" alt="Hive图2"></p><p><strong>首先一个UDF必须满足下面两个条件:</strong></p><ul><li><ol><li>一个UDF必须是org.apache.hadoop.hive.ql.exec.UDF的子类（换句话说就是我们一般都是去继承这个类）</li></ol></li><li><ol start="2"><li>一个UDF必须至少实现了evaluate()方法</li></ol></li></ul><h6 id="4-测试，右击运行run-‘HelloUDF-main-’"><a href="#4-测试，右击运行run-‘HelloUDF-main-’" class="headerlink" title="4. 测试，右击运行run ‘HelloUDF.main()’"></a>4. 测试，右击运行run ‘HelloUDF.main()’</h6><h6 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h6><p>在IDEA菜单中选择view–&gt;Tool Windows–&gt;Maven Projects，然后在Maven Projects窗口中选择【工程名】–&gt;Lifecycle–&gt;package，在package中右键选择Run Maven Build开始打包<br>执行成功后在日志中找：<br>     <font color="#FF4500">     [INFO] Building jar: (路径)/hive-1.0.jar  </font></p><p></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本地开发环境：IntelliJ IDEA+Maven3.3.9&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
</feed>
